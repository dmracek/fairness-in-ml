{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-291e4cd19d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"muted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"talk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# HIDE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import keras as ke\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "create_gif = False\n",
    "\n",
    "print(f\"sklearn: {sk.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"keras: {ke.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards fairness in machine learning with adversarial networks \n",
    "\n",
    "## Introduction\n",
    "\n",
    "From credit ratings to housing allocation, machine learning models are increasingly used to automate 'everyday' decision making processes. With the growing impact on society, more and more concerns are being voiced about the loss of transparency, accountability and fairness of the algorithms making the decisions. We as data scientists need to step-up our game and look for ways to mitigate emergent discrimination in our models. We need to make sure that our predictions do not disproportionately hurt people with certain sensitive characteristics (e.g., gender, ethnicity).\n",
    "\n",
    "\n",
    "Luckily, [last year's NIPS conference](https://blog.godatadriven.com/gdd-nips-2017) showed that the field is actively investigating how to bring fairness to predictive models. The number of papers published on the topic is rapidly increasing, a signal that fairness is finally being taken seriously. This point is also nicely made in the cartoon below, which was taken from the excellent [CS 294: Fairness in Machine Learning](https://fairmlclass.github.io/) course taught at UC Berkley.\n",
    "\n",
    "<center><br><img src=\"images/fairness_plot.svg\" alt=\"Fairness\" width=\"500\"/><br></center>\n",
    "\n",
    "Some approaches focus on interpretability and transparency by allowing deeper interrogation of complex, black box models. Other approaches, make trained models more robust and fair in their predictions by taking the route of constraining and changing the optimization objective. We will consider the latter approach and show how adversarial networks can bring fairness to our predictive models. \n",
    "\n",
    "In this blog post, we will train a model for making income level predictions, analyse the fairness of its predictions and then show how adversarial training can be used to make it fair. The used approach is based on the 2017 NIPS paper [\"Learning to Pivot with Adversarial Networks\"](https://papers.nips.cc/paper/6699-learning-to-pivot-with-adversarial-networks) by Louppe et al. \n",
    "\n",
    "*Note that most of the code has been omitted, you can find the Jupyter notebook with all the code [here](https://github.com/equialgo/fairness-in-ml/blob/master/fairness-in-ml.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making income predictions\n",
    "\n",
    "Let's start by training a basic classifier that can predict whether or not a person's income is larger than 50K dollar a year. Too make these income level predictions we turn to the [adult UCI](https://archive.ics.uci.edu/ml/datasets/Adult) dataset, which is also referred to as \"Census Income\" dataset. It is not hard to imagine that financial institutions train models on similar data sets and use them to decide whether or not someone is eligible for a loan, or to set the height of an insurance premium.\n",
    "\n",
    "Before training a model, we first parse the data into three datasets: features, targets and sensitive attributes. The set of features $X$ contains the input attributes that the model uses for making the predictions, with attributes like age, education level and occupation. The targets $y$ contain the binary class labels that the model needs to predict.  These labels are $y\\in\\left\\{income>50K, income\\leq 50K\\right\\}$. Finally, the set of sensitive attributes $Z$ contains the attributes for which we want the prediction to fair. These are $z_{race}\\in\\left\\{black, white\\right\\}$ and $z_{sex}\\in\\left\\{male, female\\right\\}$. \n",
    "\n",
    "It is important to note that datasets are non-overlapping, so the sensitive attributes race and sex are **not** part of the features used for training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "def load_ICU_data(path):\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "                    'marital_status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                    'capital_gain', 'capital_loss', 'hours_per_week', 'country', 'target']\n",
    "    input_data = (pd.read_csv(path, names=column_names, \n",
    "                              na_values=\"?\", sep=r'\\s*,\\s*', engine='python')\n",
    "                  .loc[lambda df: df['race'].isin(['White', 'Black'])])\n",
    "\n",
    "    # sensitive attributes; we identify 'race' and 'sex' as sensitive attributes\n",
    "    sensitive_attribs = ['race', 'sex']\n",
    "    Z = (input_data.loc[:, sensitive_attribs]\n",
    "         .assign(race=lambda df: (df['race'] == 'White').astype(int),\n",
    "                 sex=lambda df: (df['sex'] == 'Male').astype(int)))\n",
    "\n",
    "    # targets; 1 when someone makes over 50k , otherwise 0\n",
    "    y = (input_data['target'] == '>50K').astype(int)\n",
    "\n",
    "    # features; note that the 'target' and sentive attribute columns are dropped\n",
    "    X = (input_data\n",
    "         .drop(columns=['target', 'race', 'sex'])\n",
    "         .fillna('Unknown')\n",
    "         .pipe(pd.get_dummies, drop_first=True))\n",
    "    \n",
    "    print(f\"features X: {X.shape[0]} samples, {X.shape[1]} attributes\")\n",
    "    print(f\"targets y: {y.shape[0]} samples\")\n",
    "    print(f\"sensitives Z: {Z.shape[0]} samples, {Z.shape[1]} attributes\")\n",
    "    return X, y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "features X: 30940 samples, 94 attributes\ntargets y: 30940 samples\nsensitives Z: 30940 samples, 2 attributes\n"
     ]
    }
   ],
   "source": [
    "# load ICU data set\n",
    "X, y, Z = load_ICU_data('data/adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains the information of just over 30K people. Next, we split the data into train and test sets, where the split is 50/50, and scale the features $X$ using standard scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test set\n",
    "X_train, X_test, y_train, y_test, Z_train, Z_test = train_test_split(X, y, Z, test_size=0.5, \n",
    "                                                                     stratify=y, random_state=7)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "X_train = X_train.pipe(scale_df, scaler) \n",
    "X_test = X_test.pipe(scale_df, scaler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our basic income level predictor. We use Keras to fit a simple three-layer network with ReLU activations and dropout on the training data. The output of the network is a single node with sigmoid activation, so it predicts \"the probability that this person's income is larger than 50K\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_classifier(n_features):\n",
    "    inputs = Input(shape=(n_features,))\n",
    "    dense1 = Dense(32, activation='relu')(inputs)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(32, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "    dense3 = Dense(32, activation=\"relu\")(dropout2)\n",
    "    dropout3 = Dropout(0.2)(dense3)\n",
    "    outputs = Dense(1, activation='sigmoid')(dropout3)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# initialise NeuralNet Classifier\n",
    "clf = nn_classifier(n_features=X_train.shape[1])\n",
    "\n",
    "# train on train set\n",
    "history = clf.fit(X_train.values, y_train.values, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use this classifier to make income level predictions on the test data. We determine the model performance by computing the [Area Under the Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) and the accuracy score using test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROC AUC: 0.91\nAccuracy: 85.1%\n"
     ]
    }
   ],
   "source": [
    "# predict on test set\n",
    "y_pred = pd.Series(clf.predict(X_test).ravel(), index=y_test.index)\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Accuracy: {100*accuracy_score(y_test, (y_pred>0.5)):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a ROC AUC larger than 0.9 and a prediction accuracy of 85% we can say that our basic classifier performs pretty well! However, if it is also fair in its predictions, that remains to be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative model fairness\n",
    "\n",
    "We start the investigation into the fairness of our classifier by analysing the predictions it made on the test set. The plots in the figure below show the distributions of the predicted $P(income>50K)$ given the sensitive attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "def plot_distributions(y, Z, iteration=None, val_metrics=None, p_rules=None, fname=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "    legend={'race': ['black','white'],\n",
    "            'sex': ['female','male']}\n",
    "    for idx, attr in enumerate(Z.columns):\n",
    "        for attr_val in [0, 1]:\n",
    "            ax = sns.distplot(y[Z[attr] == attr_val], hist=False, \n",
    "                              kde_kws={'shade': True,},\n",
    "                              label='{}'.format(legend[attr][attr_val]), \n",
    "                              ax=axes[idx])\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,7)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"sensitive attibute: {}\".format(attr))\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('prediction distribution')\n",
    "        ax.set_xlabel(r'$P({{income>50K}}|z_{{{}}})$'.format(attr))\n",
    "    if iteration:\n",
    "        fig.text(1.0, 0.9, f\"Training iteration #{iteration}\", fontsize='16')\n",
    "    if val_metrics is not None:\n",
    "        fig.text(1.0, 0.65, '\\n'.join([\"Prediction performance:\",\n",
    "                                       f\"- ROC AUC: {val_metrics['ROC AUC']:.2f}\",\n",
    "                                       f\"- Accuracy: {val_metrics['Accuracy']:.1f}\"]),\n",
    "                 fontsize='16')\n",
    "    if p_rules is not None:\n",
    "        fig.text(1.0, 0.4, '\\n'.join([\"Satisfied p%-rules:\"] +\n",
    "                                     [f\"- {attr}: {p_rules[attr]:.0f}%-rule\" \n",
    "                                      for attr in p_rules.keys()]), \n",
    "                 fontsize='16')\n",
    "    fig.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.626094pt\" version=\"1.1\" viewBox=\"0 0 712.215703 277.626094\" width=\"712.215703pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-07T13:09:46.186548</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.626094 \nL 712.215703 277.626094 \nL 712.215703 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 28.620625 219.777187 \nL 341.733125 219.777187 \nL 341.733125 26.877188 \nL 28.620625 26.877188 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(15.500547 244.814609)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(78.123047 244.814609)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(140.745547 244.814609)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <g style=\"fill:#262626;\" transform=\"translate(203.368047 244.814609)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <g style=\"fill:#262626;\" transform=\"translate(265.990547 244.814609)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(328.613047 244.814609)scale(0.165 -0.165)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- $P({income&gt;50K}|z_{race})$ -->\n     <g style=\"fill:#262626;\" transform=\"translate(89.506875 266.106094)scale(0.18 -0.18)\">\n      <defs>\n       <path d=\"M 16.890625 72.90625 \nL 39.703125 72.90625 \nQ 49.65625 72.90625 54.875 68.265625 \nQ 60.109375 63.625 60.109375 54.6875 \nQ 60.109375 42.671875 52.390625 35.984375 \nQ 44.671875 29.296875 30.71875 29.296875 \nL 18.3125 29.296875 \nL 12.59375 0 \nL 2.6875 0 \nz\nM 25.203125 64.796875 \nL 19.921875 37.40625 \nL 32.328125 37.40625 \nQ 40.71875 37.40625 45.203125 41.703125 \nQ 49.703125 46 49.703125 54 \nQ 49.703125 59.125 46.65625 61.953125 \nQ 43.609375 64.796875 38.09375 64.796875 \nz\n\" id=\"DejaVuSans-Oblique-80\"/>\n       <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n       <path d=\"M 18.3125 75.984375 \nL 27.296875 75.984375 \nL 25.09375 64.59375 \nL 16.109375 64.59375 \nz\nM 14.203125 54.6875 \nL 23.1875 54.6875 \nL 12.5 0 \nL 3.515625 0 \nz\n\" id=\"DejaVuSans-Oblique-105\"/>\n       <path d=\"M 55.71875 33.015625 \nL 49.3125 0 \nL 40.28125 0 \nL 46.6875 32.671875 \nQ 47.125 34.96875 47.359375 36.71875 \nQ 47.609375 38.484375 47.609375 39.5 \nQ 47.609375 43.609375 45.015625 45.890625 \nQ 42.4375 48.1875 37.796875 48.1875 \nQ 30.5625 48.1875 25.34375 43.375 \nQ 20.125 38.578125 18.5 30.328125 \nL 12.5 0 \nL 3.515625 0 \nL 14.109375 54.6875 \nL 23.09375 54.6875 \nL 21.296875 46.09375 \nQ 25.046875 50.828125 30.3125 53.40625 \nQ 35.59375 56 41.40625 56 \nQ 48.640625 56 52.609375 52.09375 \nQ 56.59375 48.1875 56.59375 41.109375 \nQ 56.59375 39.359375 56.375 37.359375 \nQ 56.15625 35.359375 55.71875 33.015625 \nz\n\" id=\"DejaVuSans-Oblique-110\"/>\n       <path d=\"M 53.609375 52.59375 \nL 51.8125 43.703125 \nQ 48.578125 46.046875 44.9375 47.21875 \nQ 41.3125 48.390625 37.40625 48.390625 \nQ 33.109375 48.390625 29.21875 46.875 \nQ 25.34375 45.359375 22.703125 42.578125 \nQ 18.5 38.328125 16.203125 32.609375 \nQ 13.921875 26.90625 13.921875 20.796875 \nQ 13.921875 13.421875 17.609375 9.8125 \nQ 21.296875 6.203125 28.8125 6.203125 \nQ 32.515625 6.203125 36.6875 7.328125 \nQ 40.875 8.453125 45.40625 10.6875 \nL 43.703125 1.8125 \nQ 39.796875 0.203125 35.671875 -0.609375 \nQ 31.546875 -1.421875 27.203125 -1.421875 \nQ 16.3125 -1.421875 10.453125 4.015625 \nQ 4.59375 9.46875 4.59375 19.578125 \nQ 4.59375 28.078125 7.640625 35.234375 \nQ 10.6875 42.390625 16.703125 48.09375 \nQ 20.796875 52 26.3125 54 \nQ 31.84375 56 38.375 56 \nQ 42.1875 56 45.9375 55.140625 \nQ 49.703125 54.296875 53.609375 52.59375 \nz\n\" id=\"DejaVuSans-Oblique-99\"/>\n       <path d=\"M 25.390625 -1.421875 \nQ 15.765625 -1.421875 10.171875 4.515625 \nQ 4.59375 10.453125 4.59375 20.703125 \nQ 4.59375 26.65625 6.515625 32.828125 \nQ 8.453125 39.015625 11.53125 43.21875 \nQ 16.359375 49.75 22.3125 52.875 \nQ 28.265625 56 35.796875 56 \nQ 45.125 56 50.859375 50.1875 \nQ 56.59375 44.390625 56.59375 35.015625 \nQ 56.59375 28.515625 54.6875 22.0625 \nQ 52.78125 15.625 49.703125 11.375 \nQ 44.921875 4.828125 38.96875 1.703125 \nQ 33.015625 -1.421875 25.390625 -1.421875 \nz\nM 13.921875 21 \nQ 13.921875 13.578125 17.015625 9.890625 \nQ 20.125 6.203125 26.421875 6.203125 \nQ 35.453125 6.203125 41.375 14.078125 \nQ 47.3125 21.96875 47.3125 34.078125 \nQ 47.3125 41.15625 44.140625 44.765625 \nQ 40.96875 48.390625 34.8125 48.390625 \nQ 29.734375 48.390625 25.78125 46.015625 \nQ 21.828125 43.65625 18.703125 38.8125 \nQ 16.40625 35.203125 15.15625 30.5625 \nQ 13.921875 25.921875 13.921875 21 \nz\n\" id=\"DejaVuSans-Oblique-111\"/>\n       <path d=\"M 89.796875 33.015625 \nL 83.40625 0 \nL 74.421875 0 \nL 80.71875 32.71875 \nQ 81.109375 34.8125 81.296875 36.328125 \nQ 81.5 37.84375 81.5 38.921875 \nQ 81.5 43.3125 79.046875 45.75 \nQ 76.609375 48.1875 72.21875 48.1875 \nQ 65.671875 48.1875 60.546875 43.28125 \nQ 55.421875 38.375 53.90625 30.515625 \nL 47.90625 0 \nL 38.921875 0 \nL 45.3125 32.71875 \nQ 45.703125 34.515625 45.890625 36.046875 \nQ 46.09375 37.59375 46.09375 38.8125 \nQ 46.09375 43.265625 43.65625 45.71875 \nQ 41.21875 48.1875 36.921875 48.1875 \nQ 30.28125 48.1875 25.140625 43.28125 \nQ 20.015625 38.375 18.5 30.515625 \nL 12.5 0 \nL 3.515625 0 \nL 14.203125 54.6875 \nL 23.1875 54.6875 \nL 21.484375 46.1875 \nQ 25.140625 50.984375 30.046875 53.484375 \nQ 34.96875 56 40.578125 56 \nQ 46.53125 56 50.359375 52.875 \nQ 54.203125 49.75 54.984375 44.1875 \nQ 59.078125 49.953125 64.46875 52.96875 \nQ 69.875 56 75.875 56 \nQ 82.90625 56 86.734375 51.953125 \nQ 90.578125 47.90625 90.578125 40.484375 \nQ 90.578125 38.875 90.375 36.9375 \nQ 90.1875 35.015625 89.796875 33.015625 \nz\n\" id=\"DejaVuSans-Oblique-109\"/>\n       <path d=\"M 48.09375 32.234375 \nQ 48.25 33.015625 48.3125 33.84375 \nQ 48.390625 34.671875 48.390625 35.5 \nQ 48.390625 41.453125 44.890625 44.921875 \nQ 41.40625 48.390625 35.40625 48.390625 \nQ 28.71875 48.390625 23.578125 44.15625 \nQ 18.453125 39.9375 15.828125 32.171875 \nz\nM 55.90625 25.203125 \nL 14.109375 25.203125 \nQ 13.8125 23.34375 13.71875 22.265625 \nQ 13.625 21.1875 13.625 20.40625 \nQ 13.625 13.625 17.796875 9.90625 \nQ 21.96875 6.203125 29.59375 6.203125 \nQ 35.453125 6.203125 40.671875 7.515625 \nQ 45.90625 8.84375 50.390625 11.375 \nL 48.6875 2.484375 \nQ 43.84375 0.53125 38.6875 -0.4375 \nQ 33.546875 -1.421875 28.21875 -1.421875 \nQ 16.84375 -1.421875 10.71875 4.015625 \nQ 4.59375 9.46875 4.59375 19.484375 \nQ 4.59375 28.03125 7.640625 35.375 \nQ 10.6875 42.71875 16.609375 48.484375 \nQ 20.40625 52.09375 25.65625 54.046875 \nQ 30.90625 56 36.8125 56 \nQ 46.09375 56 51.578125 50.4375 \nQ 57.078125 44.875 57.078125 35.5 \nQ 57.078125 33.25 56.78125 30.6875 \nQ 56.5 28.125 55.90625 25.203125 \nz\n\" id=\"DejaVuSans-Oblique-101\"/>\n       <path d=\"M 10.59375 49.21875 \nL 10.59375 58.109375 \nL 73.1875 35.40625 \nL 73.1875 27.296875 \nL 10.59375 4.59375 \nL 10.59375 13.484375 \nL 60.890625 31.296875 \nz\n\" id=\"DejaVuSans-62\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       <path d=\"M 16.890625 72.90625 \nL 26.8125 72.90625 \nL 20.796875 42.1875 \nL 59.078125 72.90625 \nL 72.21875 72.90625 \nL 28.90625 38.09375 \nL 60.59375 0 \nL 48.578125 0 \nL 19.484375 35.5 \nL 12.59375 0 \nL 2.6875 0 \nz\n\" id=\"DejaVuSans-Oblique-75\"/>\n       <path d=\"M 21 76.421875 \nL 21 -23.578125 \nL 12.703125 -23.578125 \nL 12.703125 76.421875 \nz\n\" id=\"DejaVuSans-124\"/>\n       <path d=\"M 11.625 54.6875 \nL 54.296875 54.6875 \nL 52.6875 46.484375 \nL 11.53125 7.171875 \nL 45.515625 7.171875 \nL 44.09375 0 \nL -0.296875 0 \nL 1.3125 8.203125 \nL 42.484375 47.515625 \nL 10.203125 47.515625 \nz\n\" id=\"DejaVuSans-Oblique-122\"/>\n       <path d=\"M 44.578125 46.390625 \nQ 43.21875 47.125 41.453125 47.515625 \nQ 39.703125 47.90625 37.703125 47.90625 \nQ 30.515625 47.90625 25.140625 42.453125 \nQ 19.78125 37.015625 18.015625 27.875 \nL 12.5 0 \nL 3.515625 0 \nL 14.203125 54.6875 \nL 23.1875 54.6875 \nL 21.484375 46.1875 \nQ 25.046875 50.921875 30 53.453125 \nQ 34.96875 56 40.578125 56 \nQ 42.046875 56 43.453125 55.828125 \nQ 44.875 55.671875 46.296875 55.28125 \nz\n\" id=\"DejaVuSans-Oblique-114\"/>\n       <path d=\"M 53.71875 31.203125 \nL 47.609375 0 \nL 38.625 0 \nL 40.28125 8.296875 \nQ 36.328125 3.421875 31.265625 1 \nQ 26.21875 -1.421875 20.015625 -1.421875 \nQ 13.03125 -1.421875 8.5625 2.84375 \nQ 4.109375 7.125 4.109375 13.8125 \nQ 4.109375 23.390625 11.75 28.953125 \nQ 19.390625 34.515625 32.8125 34.515625 \nL 45.3125 34.515625 \nL 45.796875 36.921875 \nQ 45.90625 37.3125 45.953125 37.765625 \nQ 46 38.234375 46 39.203125 \nQ 46 43.5625 42.453125 45.96875 \nQ 38.921875 48.390625 32.515625 48.390625 \nQ 28.125 48.390625 23.5 47.265625 \nQ 18.890625 46.140625 14.015625 43.890625 \nL 15.578125 52.203125 \nQ 20.65625 54.109375 25.515625 55.046875 \nQ 30.375 56 34.90625 56 \nQ 44.578125 56 49.625 51.796875 \nQ 54.6875 47.609375 54.6875 39.59375 \nQ 54.6875 37.984375 54.4375 35.8125 \nQ 54.203125 33.640625 53.71875 31.203125 \nz\nM 44 27.484375 \nL 35.015625 27.484375 \nQ 23.96875 27.484375 18.671875 24.53125 \nQ 13.375 21.578125 13.375 15.375 \nQ 13.375 11.078125 16.078125 8.640625 \nQ 18.796875 6.203125 23.578125 6.203125 \nQ 30.90625 6.203125 36.375 11.453125 \nQ 41.84375 16.703125 43.609375 25.484375 \nz\n\" id=\"DejaVuSans-Oblique-97\"/>\n       <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n      </defs>\n      <use transform=\"translate(0 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-80\"/>\n      <use transform=\"translate(60.302734 0.578125)\" xlink:href=\"#DejaVuSans-40\"/>\n      <use transform=\"translate(99.316406 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(127.099609 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-110\"/>\n      <use transform=\"translate(190.478516 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-99\"/>\n      <use transform=\"translate(245.458984 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-111\"/>\n      <use transform=\"translate(306.640625 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-109\"/>\n      <use transform=\"translate(404.052734 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(485.058594 0.578125)\" xlink:href=\"#DejaVuSans-62\"/>\n      <use transform=\"translate(588.330078 0.578125)\" xlink:href=\"#DejaVuSans-53\"/>\n      <use transform=\"translate(651.953125 0.578125)\" xlink:href=\"#DejaVuSans-48\"/>\n      <use transform=\"translate(715.576172 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-75\"/>\n      <use transform=\"translate(781.152344 0.578125)\" xlink:href=\"#DejaVuSans-124\"/>\n      <use transform=\"translate(814.84375 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-122\"/>\n      <use transform=\"translate(867.333984 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-114\"/>\n      <use transform=\"translate(896.113281 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-97\"/>\n      <use transform=\"translate(939.008789 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-99\"/>\n      <use transform=\"translate(977.495117 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(1023.295898 0.578125)\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"text_8\">\n     <!-- prediction distribution -->\n     <g style=\"fill:#262626;\" transform=\"translate(20.877188 222.625313)rotate(-90)scale(0.18 -0.18)\">\n      <defs>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"163.863281\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"227.339844\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"255.123047\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"310.103516\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"349.3125\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"377.095703\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"438.277344\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"501.65625\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"533.443359\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"596.919922\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"624.703125\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"676.802734\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"716.011719\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"757.125\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"784.908203\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"848.384766\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"911.763672\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"950.972656\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"978.755859\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1039.9375\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <defs>\n     <path d=\"M -21.545325 -58.82268 \nL -21.545325 -57.848906 \nL -19.467715 -57.848906 \nL -17.390105 -57.848906 \nL -15.312495 -57.848906 \nL -13.234885 -57.848906 \nL -11.157275 -57.848906 \nL -9.079665 -57.848906 \nL -7.002055 -57.848906 \nL -4.924445 -57.848906 \nL -2.846835 -57.848906 \nL -0.769225 -57.848906 \nL 1.308385 -57.848906 \nL 3.385996 -57.848906 \nL 5.463606 -57.848906 \nL 7.541216 -57.848906 \nL 9.618826 -57.848906 \nL 11.696436 -57.848906 \nL 13.774046 -57.848906 \nL 15.851656 -57.848906 \nL 17.929266 -57.848906 \nL 20.006876 -57.848906 \nL 22.084486 -57.848906 \nL 24.162096 -57.848906 \nL 26.239706 -57.848906 \nL 28.317316 -57.848906 \nL 30.394926 -57.848906 \nL 32.472536 -57.848906 \nL 34.550146 -57.848906 \nL 36.627756 -57.848906 \nL 38.705367 -57.848906 \nL 40.782977 -57.848906 \nL 42.860587 -57.848906 \nL 44.938197 -57.848906 \nL 47.015807 -57.848906 \nL 49.093417 -57.848906 \nL 51.171027 -57.848906 \nL 53.248637 -57.848906 \nL 55.326247 -57.848906 \nL 57.403857 -57.848906 \nL 59.481467 -57.848906 \nL 61.559077 -57.848906 \nL 63.636687 -57.848906 \nL 65.714297 -57.848906 \nL 67.791907 -57.848906 \nL 69.869517 -57.848906 \nL 71.947127 -57.848906 \nL 74.024738 -57.848906 \nL 76.102348 -57.848906 \nL 78.179958 -57.848906 \nL 80.257568 -57.848906 \nL 82.335178 -57.848906 \nL 84.412788 -57.848906 \nL 86.490398 -57.848906 \nL 88.568008 -57.848906 \nL 90.645618 -57.848906 \nL 92.723228 -57.848906 \nL 94.800838 -57.848906 \nL 96.878448 -57.848906 \nL 98.956058 -57.848906 \nL 101.033668 -57.848906 \nL 103.111278 -57.848906 \nL 105.188888 -57.848906 \nL 107.266498 -57.848906 \nL 109.344109 -57.848906 \nL 111.421719 -57.848906 \nL 113.499329 -57.848906 \nL 115.576939 -57.848906 \nL 117.654549 -57.848906 \nL 119.732159 -57.848906 \nL 121.809769 -57.848906 \nL 123.887379 -57.848906 \nL 125.964989 -57.848906 \nL 128.042599 -57.848906 \nL 130.120209 -57.848906 \nL 132.197819 -57.848906 \nL 134.275429 -57.848906 \nL 136.353039 -57.848906 \nL 138.430649 -57.848906 \nL 140.508259 -57.848906 \nL 142.58587 -57.848906 \nL 144.66348 -57.848906 \nL 146.74109 -57.848906 \nL 148.8187 -57.848906 \nL 150.89631 -57.848906 \nL 152.97392 -57.848906 \nL 155.05153 -57.848906 \nL 157.12914 -57.848906 \nL 159.20675 -57.848906 \nL 161.28436 -57.848906 \nL 163.36197 -57.848906 \nL 165.43958 -57.848906 \nL 167.51719 -57.848906 \nL 169.5948 -57.848906 \nL 171.67241 -57.848906 \nL 173.75002 -57.848906 \nL 175.82763 -57.848906 \nL 177.905241 -57.848906 \nL 179.982851 -57.848906 \nL 182.060461 -57.848906 \nL 184.138071 -57.848906 \nL 186.215681 -57.848906 \nL 188.293291 -57.848906 \nL 190.370901 -57.848906 \nL 192.448511 -57.848906 \nL 194.526121 -57.848906 \nL 196.603731 -57.848906 \nL 198.681341 -57.848906 \nL 200.758951 -57.848906 \nL 202.836561 -57.848906 \nL 204.914171 -57.848906 \nL 206.991781 -57.848906 \nL 209.069391 -57.848906 \nL 211.147001 -57.848906 \nL 213.224612 -57.848906 \nL 215.302222 -57.848906 \nL 217.379832 -57.848906 \nL 219.457442 -57.848906 \nL 221.535052 -57.848906 \nL 223.612662 -57.848906 \nL 225.690272 -57.848906 \nL 227.767882 -57.848906 \nL 229.845492 -57.848906 \nL 231.923102 -57.848906 \nL 234.000712 -57.848906 \nL 236.078322 -57.848906 \nL 238.155932 -57.848906 \nL 240.233542 -57.848906 \nL 242.311152 -57.848906 \nL 244.388762 -57.848906 \nL 246.466372 -57.848906 \nL 248.543983 -57.848906 \nL 250.621593 -57.848906 \nL 252.699203 -57.848906 \nL 254.776813 -57.848906 \nL 256.854423 -57.848906 \nL 258.932033 -57.848906 \nL 261.009643 -57.848906 \nL 263.087253 -57.848906 \nL 265.164863 -57.848906 \nL 267.242473 -57.848906 \nL 269.320083 -57.848906 \nL 271.397693 -57.848906 \nL 273.475303 -57.848906 \nL 275.552913 -57.848906 \nL 277.630523 -57.848906 \nL 279.708133 -57.848906 \nL 281.785743 -57.848906 \nL 283.863354 -57.848906 \nL 285.940964 -57.848906 \nL 288.018574 -57.848906 \nL 290.096184 -57.848906 \nL 292.173794 -57.848906 \nL 294.251404 -57.848906 \nL 296.329014 -57.848906 \nL 298.406624 -57.848906 \nL 300.484234 -57.848906 \nL 302.561844 -57.848906 \nL 304.639454 -57.848906 \nL 306.717064 -57.848906 \nL 308.794674 -57.848906 \nL 310.872284 -57.848906 \nL 312.949894 -57.848906 \nL 315.027504 -57.848906 \nL 317.105115 -57.848906 \nL 319.182725 -57.848906 \nL 321.260335 -57.848906 \nL 323.337945 -57.848906 \nL 325.415555 -57.848906 \nL 327.493165 -57.848906 \nL 329.570775 -57.848906 \nL 331.648385 -57.848906 \nL 333.725995 -57.848906 \nL 335.803605 -57.848906 \nL 337.881215 -57.848906 \nL 339.958825 -57.848906 \nL 342.036435 -57.848906 \nL 344.114045 -57.848906 \nL 346.191655 -57.848906 \nL 348.269265 -57.848906 \nL 350.346875 -57.848906 \nL 352.424486 -57.848906 \nL 354.502096 -57.848906 \nL 356.579706 -57.848906 \nL 358.657316 -57.848906 \nL 360.734926 -57.848906 \nL 362.812536 -57.848906 \nL 364.890146 -57.848906 \nL 366.967756 -57.848906 \nL 369.045366 -57.848906 \nL 371.122976 -57.848906 \nL 373.200586 -57.848906 \nL 375.278196 -57.848906 \nL 377.355806 -57.848906 \nL 379.433416 -57.848906 \nL 381.511026 -57.848906 \nL 383.588636 -57.848906 \nL 385.666246 -57.848906 \nL 387.743857 -57.848906 \nL 389.821467 -57.848906 \nL 391.899077 -57.848906 \nL 391.899077 -57.863233 \nL 391.899077 -57.863233 \nL 389.821467 -57.870054 \nL 387.743857 -57.87967 \nL 385.666246 -57.893012 \nL 383.588636 -57.911227 \nL 381.511026 -57.935698 \nL 379.433416 -57.968041 \nL 377.355806 -58.010097 \nL 375.278196 -58.063887 \nL 373.200586 -58.131547 \nL 371.122976 -58.215235 \nL 369.045366 -58.317 \nL 366.967756 -58.438626 \nL 364.890146 -58.581461 \nL 362.812536 -58.74623 \nL 360.734926 -58.932859 \nL 358.657316 -59.140324 \nL 356.579706 -59.366541 \nL 354.502096 -59.608323 \nL 352.424486 -59.861413 \nL 350.346875 -60.120604 \nL 348.269265 -60.37995 \nL 346.191655 -60.633051 \nL 344.114045 -60.873412 \nL 342.036435 -61.094824 \nL 339.958825 -61.29176 \nL 337.881215 -61.459736 \nL 335.803605 -61.595596 \nL 333.725995 -61.697711 \nL 331.648385 -61.766054 \nL 329.570775 -61.802152 \nL 327.493165 -61.808909 \nL 325.415555 -61.790334 \nL 323.337945 -61.751188 \nL 321.260335 -61.696593 \nL 319.182725 -61.631641 \nL 317.105115 -61.561049 \nL 315.027504 -61.488876 \nL 312.949894 -61.418346 \nL 310.872284 -61.351777 \nL 308.794674 -61.290616 \nL 306.717064 -61.235566 \nL 304.639454 -61.186792 \nL 302.561844 -61.144169 \nL 300.484234 -61.107535 \nL 298.406624 -61.076933 \nL 296.329014 -61.052801 \nL 294.251404 -61.036104 \nL 292.173794 -61.028384 \nL 290.096184 -61.031728 \nL 288.018574 -61.048677 \nL 285.940964 -61.082065 \nL 283.863354 -61.134821 \nL 281.785743 -61.209752 \nL 279.708133 -61.309327 \nL 277.630523 -61.435473 \nL 275.552913 -61.589409 \nL 273.475303 -61.77152 \nL 271.397693 -61.981276 \nL 269.320083 -62.217202 \nL 267.242473 -62.476892 \nL 265.164863 -62.757059 \nL 263.087253 -63.053629 \nL 261.009643 -63.361861 \nL 258.932033 -63.676488 \nL 256.854423 -63.991886 \nL 254.776813 -64.302255 \nL 252.699203 -64.601812 \nL 250.621593 -64.884993 \nL 248.543983 -65.14665 \nL 246.466372 -65.382241 \nL 244.388762 -65.588003 \nL 242.311152 -65.761085 \nL 240.233542 -65.899648 \nL 238.155932 -66.002908 \nL 236.078322 -66.071134 \nL 234.000712 -66.105585 \nL 231.923102 -66.108403 \nL 229.845492 -66.082463 \nL 227.767882 -66.031209 \nL 225.690272 -65.958471 \nL 223.612662 -65.868294 \nL 221.535052 -65.764778 \nL 219.457442 -65.651954 \nL 217.379832 -65.533674 \nL 215.302222 -65.413544 \nL 213.224612 -65.294862 \nL 211.147001 -65.180581 \nL 209.069391 -65.073277 \nL 206.991781 -64.975112 \nL 204.914171 -64.8878 \nL 202.836561 -64.812578 \nL 200.758951 -64.750174 \nL 198.681341 -64.700796 \nL 196.603731 -64.664142 \nL 194.526121 -64.63944 \nL 192.448511 -64.625526 \nL 190.370901 -64.620955 \nL 188.293291 -64.624153 \nL 186.215681 -64.633585 \nL 184.138071 -64.64794 \nL 182.060461 -64.666313 \nL 179.982851 -64.688359 \nL 177.905241 -64.714423 \nL 175.82763 -64.745608 \nL 173.75002 -64.783795 \nL 171.67241 -64.831594 \nL 169.5948 -64.892244 \nL 167.51719 -64.969448 \nL 165.43958 -65.067185 \nL 163.36197 -65.189476 \nL 161.28436 -65.34016 \nL 159.20675 -65.522665 \nL 157.12914 -65.739802 \nL 155.05153 -65.993598 \nL 152.97392 -66.285166 \nL 150.89631 -66.614616 \nL 148.8187 -66.981019 \nL 146.74109 -67.382405 \nL 144.66348 -67.815806 \nL 142.58587 -68.277319 \nL 140.508259 -68.762212 \nL 138.430649 -69.265042 \nL 136.353039 -69.779801 \nL 134.275429 -70.300087 \nL 132.197819 -70.819295 \nL 130.120209 -71.330841 \nL 128.042599 -71.828399 \nL 125.964989 -72.306168 \nL 123.887379 -72.75915 \nL 121.809769 -73.183423 \nL 119.732159 -73.576405 \nL 117.654549 -73.937081 \nL 115.576939 -74.266171 \nL 113.499329 -74.566236 \nL 111.421719 -74.841692 \nL 109.344109 -75.098733 \nL 107.266498 -75.34517 \nL 105.188888 -75.590202 \nL 103.111278 -75.844127 \nL 101.033668 -76.118053 \nL 98.956058 -76.423638 \nL 96.878448 -76.772905 \nL 94.800838 -77.178186 \nL 92.723228 -77.652231 \nL 90.645618 -78.20852 \nL 88.568008 -78.861783 \nL 86.490398 -79.628728 \nL 84.412788 -80.528939 \nL 82.335178 -81.585868 \nL 80.257568 -82.827823 \nL 78.179958 -84.288814 \nL 76.102348 -86.009056 \nL 74.024738 -88.034963 \nL 71.947127 -90.418394 \nL 69.869517 -93.214968 \nL 67.791907 -96.481283 \nL 65.714297 -100.270983 \nL 63.636687 -104.629677 \nL 61.559077 -109.588919 \nL 59.481467 -115.159576 \nL 57.403857 -121.325103 \nL 55.326247 -128.035382 \nL 53.248637 -135.201877 \nL 51.171027 -142.694912 \nL 49.093417 -150.343747 \nL 47.015807 -157.940034 \nL 44.938197 -165.244881 \nL 42.860587 -171.999448 \nL 40.782977 -177.938554 \nL 38.705367 -182.806417 \nL 36.627756 -186.373272 \nL 34.550146 -188.451432 \nL 32.472536 -188.909252 \nL 30.394926 -187.681628 \nL 28.317316 -184.775899 \nL 26.239706 -180.27251 \nL 24.162096 -174.320287 \nL 22.084486 -167.126756 \nL 20.006876 -158.944435 \nL 17.929266 -150.054401 \nL 15.851656 -140.748683 \nL 13.774046 -131.313052 \nL 11.696436 -122.011641 \nL 9.618826 -113.074527 \nL 7.541216 -104.689021 \nL 5.463606 -96.994948 \nL 3.385996 -90.083816 \nL 1.308385 -84.001369 \nL -0.769225 -78.75281 \nL -2.846835 -74.309823 \nL -4.924445 -70.61851 \nL -7.002055 -67.607426 \nL -9.079665 -65.195083 \nL -11.157275 -63.296424 \nL -13.234885 -61.828013 \nL -15.312495 -60.711852 \nL -17.390105 -59.877862 \nL -19.467715 -59.265211 \nL -21.545325 -58.82268 \nz\n\" id=\"m27420bba1f\" style=\"stroke:#4878d0;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p54f5e28acb)\">\n     <use style=\"fill:#4878d0;fill-opacity:0.25;stroke:#4878d0;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m27420bba1f\" y=\"277.626094\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_2\">\n    <defs>\n     <path d=\"M -12.381957 -58.578456 \nL -12.381957 -57.848906 \nL -10.396441 -57.848906 \nL -8.410925 -57.848906 \nL -6.425409 -57.848906 \nL -4.439893 -57.848906 \nL -2.454378 -57.848906 \nL -0.468862 -57.848906 \nL 1.516654 -57.848906 \nL 3.50217 -57.848906 \nL 5.487686 -57.848906 \nL 7.473202 -57.848906 \nL 9.458718 -57.848906 \nL 11.444234 -57.848906 \nL 13.42975 -57.848906 \nL 15.415266 -57.848906 \nL 17.400781 -57.848906 \nL 19.386297 -57.848906 \nL 21.371813 -57.848906 \nL 23.357329 -57.848906 \nL 25.342845 -57.848906 \nL 27.328361 -57.848906 \nL 29.313877 -57.848906 \nL 31.299393 -57.848906 \nL 33.284909 -57.848906 \nL 35.270425 -57.848906 \nL 37.25594 -57.848906 \nL 39.241456 -57.848906 \nL 41.226972 -57.848906 \nL 43.212488 -57.848906 \nL 45.198004 -57.848906 \nL 47.18352 -57.848906 \nL 49.169036 -57.848906 \nL 51.154552 -57.848906 \nL 53.140068 -57.848906 \nL 55.125584 -57.848906 \nL 57.111099 -57.848906 \nL 59.096615 -57.848906 \nL 61.082131 -57.848906 \nL 63.067647 -57.848906 \nL 65.053163 -57.848906 \nL 67.038679 -57.848906 \nL 69.024195 -57.848906 \nL 71.009711 -57.848906 \nL 72.995227 -57.848906 \nL 74.980743 -57.848906 \nL 76.966258 -57.848906 \nL 78.951774 -57.848906 \nL 80.93729 -57.848906 \nL 82.922806 -57.848906 \nL 84.908322 -57.848906 \nL 86.893838 -57.848906 \nL 88.879354 -57.848906 \nL 90.86487 -57.848906 \nL 92.850386 -57.848906 \nL 94.835902 -57.848906 \nL 96.821417 -57.848906 \nL 98.806933 -57.848906 \nL 100.792449 -57.848906 \nL 102.777965 -57.848906 \nL 104.763481 -57.848906 \nL 106.748997 -57.848906 \nL 108.734513 -57.848906 \nL 110.720029 -57.848906 \nL 112.705545 -57.848906 \nL 114.691061 -57.848906 \nL 116.676576 -57.848906 \nL 118.662092 -57.848906 \nL 120.647608 -57.848906 \nL 122.633124 -57.848906 \nL 124.61864 -57.848906 \nL 126.604156 -57.848906 \nL 128.589672 -57.848906 \nL 130.575188 -57.848906 \nL 132.560704 -57.848906 \nL 134.54622 -57.848906 \nL 136.531735 -57.848906 \nL 138.517251 -57.848906 \nL 140.502767 -57.848906 \nL 142.488283 -57.848906 \nL 144.473799 -57.848906 \nL 146.459315 -57.848906 \nL 148.444831 -57.848906 \nL 150.430347 -57.848906 \nL 152.415863 -57.848906 \nL 154.401379 -57.848906 \nL 156.386894 -57.848906 \nL 158.37241 -57.848906 \nL 160.357926 -57.848906 \nL 162.343442 -57.848906 \nL 164.328958 -57.848906 \nL 166.314474 -57.848906 \nL 168.29999 -57.848906 \nL 170.285506 -57.848906 \nL 172.271022 -57.848906 \nL 174.256538 -57.848906 \nL 176.242053 -57.848906 \nL 178.227569 -57.848906 \nL 180.213085 -57.848906 \nL 182.198601 -57.848906 \nL 184.184117 -57.848906 \nL 186.169633 -57.848906 \nL 188.155149 -57.848906 \nL 190.140665 -57.848906 \nL 192.126181 -57.848906 \nL 194.111697 -57.848906 \nL 196.097212 -57.848906 \nL 198.082728 -57.848906 \nL 200.068244 -57.848906 \nL 202.05376 -57.848906 \nL 204.039276 -57.848906 \nL 206.024792 -57.848906 \nL 208.010308 -57.848906 \nL 209.995824 -57.848906 \nL 211.98134 -57.848906 \nL 213.966856 -57.848906 \nL 215.952371 -57.848906 \nL 217.937887 -57.848906 \nL 219.923403 -57.848906 \nL 221.908919 -57.848906 \nL 223.894435 -57.848906 \nL 225.879951 -57.848906 \nL 227.865467 -57.848906 \nL 229.850983 -57.848906 \nL 231.836499 -57.848906 \nL 233.822015 -57.848906 \nL 235.80753 -57.848906 \nL 237.793046 -57.848906 \nL 239.778562 -57.848906 \nL 241.764078 -57.848906 \nL 243.749594 -57.848906 \nL 245.73511 -57.848906 \nL 247.720626 -57.848906 \nL 249.706142 -57.848906 \nL 251.691658 -57.848906 \nL 253.677174 -57.848906 \nL 255.662689 -57.848906 \nL 257.648205 -57.848906 \nL 259.633721 -57.848906 \nL 261.619237 -57.848906 \nL 263.604753 -57.848906 \nL 265.590269 -57.848906 \nL 267.575785 -57.848906 \nL 269.561301 -57.848906 \nL 271.546817 -57.848906 \nL 273.532333 -57.848906 \nL 275.517848 -57.848906 \nL 277.503364 -57.848906 \nL 279.48888 -57.848906 \nL 281.474396 -57.848906 \nL 283.459912 -57.848906 \nL 285.445428 -57.848906 \nL 287.430944 -57.848906 \nL 289.41646 -57.848906 \nL 291.401976 -57.848906 \nL 293.387492 -57.848906 \nL 295.373007 -57.848906 \nL 297.358523 -57.848906 \nL 299.344039 -57.848906 \nL 301.329555 -57.848906 \nL 303.315071 -57.848906 \nL 305.300587 -57.848906 \nL 307.286103 -57.848906 \nL 309.271619 -57.848906 \nL 311.257135 -57.848906 \nL 313.242651 -57.848906 \nL 315.228166 -57.848906 \nL 317.213682 -57.848906 \nL 319.199198 -57.848906 \nL 321.184714 -57.848906 \nL 323.17023 -57.848906 \nL 325.155746 -57.848906 \nL 327.141262 -57.848906 \nL 329.126778 -57.848906 \nL 331.112294 -57.848906 \nL 333.09781 -57.848906 \nL 335.083325 -57.848906 \nL 337.068841 -57.848906 \nL 339.054357 -57.848906 \nL 341.039873 -57.848906 \nL 343.025389 -57.848906 \nL 345.010905 -57.848906 \nL 346.996421 -57.848906 \nL 348.981937 -57.848906 \nL 350.967453 -57.848906 \nL 352.952969 -57.848906 \nL 354.938484 -57.848906 \nL 356.924 -57.848906 \nL 358.909516 -57.848906 \nL 360.895032 -57.848906 \nL 362.880548 -57.848906 \nL 364.866064 -57.848906 \nL 366.85158 -57.848906 \nL 368.837096 -57.848906 \nL 370.822612 -57.848906 \nL 372.808128 -57.848906 \nL 374.793643 -57.848906 \nL 376.779159 -57.848906 \nL 378.764675 -57.848906 \nL 380.750191 -57.848906 \nL 382.735707 -57.848906 \nL 382.735707 -57.891591 \nL 382.735707 -57.891591 \nL 380.750191 -57.915342 \nL 378.764675 -57.950218 \nL 376.779159 -58.000286 \nL 374.793643 -58.070548 \nL 372.808128 -58.166919 \nL 370.822612 -58.296085 \nL 368.837096 -58.465218 \nL 366.85158 -58.681534 \nL 364.866064 -58.951675 \nL 362.880548 -59.280978 \nL 360.895032 -59.672654 \nL 358.909516 -60.126989 \nL 356.924 -60.640676 \nL 354.938484 -61.206383 \nL 352.952969 -61.812682 \nL 350.967453 -62.4444 \nL 348.981937 -63.08342 \nL 346.996421 -63.709895 \nL 345.010905 -64.303747 \nL 343.025389 -64.846314 \nL 341.039873 -65.32192 \nL 339.054357 -65.719179 \nL 337.068841 -66.031877 \nL 335.083325 -66.259295 \nL 333.09781 -66.405968 \nL 331.112294 -66.480924 \nL 329.126778 -66.496533 \nL 327.141262 -66.467125 \nL 325.155746 -66.407583 \nL 323.17023 -66.332078 \nL 321.184714 -66.253063 \nL 319.199198 -66.180615 \nL 317.213682 -66.12213 \nL 315.228166 -66.082324 \nL 313.242651 -66.063477 \nL 311.257135 -66.06582 \nL 309.271619 -66.087999 \nL 307.286103 -66.127533 \nL 305.300587 -66.181241 \nL 303.315071 -66.245625 \nL 301.329555 -66.3172 \nL 299.344039 -66.392795 \nL 297.358523 -66.469817 \nL 295.373007 -66.546492 \nL 293.387492 -66.622055 \nL 291.401976 -66.696875 \nL 289.41646 -66.772488 \nL 287.430944 -66.85153 \nL 285.445428 -66.93754 \nL 283.459912 -67.034677 \nL 281.474396 -67.147342 \nL 279.48888 -67.27978 \nL 277.503364 -67.435685 \nL 275.517848 -67.617875 \nL 273.532333 -67.828067 \nL 271.546817 -68.066775 \nL 269.561301 -68.333352 \nL 267.575785 -68.626141 \nL 265.590269 -68.942716 \nL 263.604753 -69.280172 \nL 261.619237 -69.635406 \nL 259.633721 -70.005351 \nL 257.648205 -70.387132 \nL 255.662689 -70.77811 \nL 253.677174 -71.175839 \nL 251.691658 -71.577919 \nL 249.706142 -71.98181 \nL 247.720626 -72.384624 \nL 245.73511 -72.782951 \nL 243.749594 -73.172744 \nL 241.764078 -73.549306 \nL 239.778562 -73.907378 \nL 237.793046 -74.241325 \nL 235.80753 -74.545398 \nL 233.822015 -74.814051 \nL 231.836499 -75.042246 \nL 229.850983 -75.225743 \nL 227.865467 -75.36131 \nL 225.879951 -75.446855 \nL 223.894435 -75.481464 \nL 221.908919 -75.465356 \nL 219.923403 -75.399768 \nL 217.937887 -75.286808 \nL 215.952371 -75.129288 \nL 213.966856 -74.930575 \nL 211.98134 -74.694461 \nL 209.995824 -74.425078 \nL 208.010308 -74.126844 \nL 206.024792 -73.804445 \nL 204.039276 -73.462827 \nL 202.05376 -73.107204 \nL 200.068244 -72.743057 \nL 198.082728 -72.376118 \nL 196.097212 -72.012322 \nL 194.111697 -71.65774 \nL 192.126181 -71.318458 \nL 190.140665 -71.000436 \nL 188.155149 -70.709326 \nL 186.169633 -70.450276 \nL 184.184117 -70.227737 \nL 182.198601 -70.045277 \nL 180.213085 -69.90545 \nL 178.227569 -69.809719 \nL 176.242053 -69.758462 \nL 174.256538 -69.751059 \nL 172.271022 -69.786055 \nL 170.285506 -69.861386 \nL 168.29999 -69.97463 \nL 166.314474 -70.123263 \nL 164.328958 -70.30488 \nL 162.343442 -70.517343 \nL 160.357926 -70.758864 \nL 158.37241 -71.027984 \nL 156.386894 -71.323496 \nL 154.401379 -71.644298 \nL 152.415863 -71.989236 \nL 150.430347 -72.356949 \nL 148.444831 -72.745762 \nL 146.459315 -73.153637 \nL 144.473799 -73.578192 \nL 142.488283 -74.016782 \nL 140.502767 -74.466636 \nL 138.517251 -74.925004 \nL 136.531735 -75.3893 \nL 134.54622 -75.857212 \nL 132.560704 -76.326764 \nL 130.575188 -76.796311 \nL 128.589672 -77.264483 \nL 126.604156 -77.730091 \nL 124.61864 -78.192015 \nL 122.633124 -78.649094 \nL 120.647608 -79.100051 \nL 118.662092 -79.543465 \nL 116.676576 -79.977798 \nL 114.691061 -80.401469 \nL 112.705545 -80.812977 \nL 110.720029 -81.211047 \nL 108.734513 -81.594774 \nL 106.748997 -81.963758 \nL 104.763481 -82.318212 \nL 102.777965 -82.659029 \nL 100.792449 -82.987822 \nL 98.806933 -83.30693 \nL 96.821417 -83.619421 \nL 94.835902 -83.929091 \nL 92.850386 -84.240498 \nL 90.86487 -84.559028 \nL 88.879354 -84.891025 \nL 86.893838 -85.243992 \nL 84.908322 -85.626877 \nL 82.922806 -86.050459 \nL 80.93729 -86.527833 \nL 78.951774 -87.075017 \nL 76.966258 -87.711651 \nL 74.980743 -88.461779 \nL 72.995227 -89.354646 \nL 71.009711 -90.425393 \nL 69.024195 -91.715491 \nL 67.038679 -93.272675 \nL 65.053163 -95.150077 \nL 63.067647 -97.404235 \nL 61.082131 -100.091679 \nL 59.096615 -103.263871 \nL 57.111099 -106.960472 \nL 55.125584 -111.201181 \nL 53.140068 -115.976727 \nL 51.154552 -121.239971 \nL 49.169036 -126.898389 \nL 47.18352 -132.809409 \nL 45.198004 -138.780052 \nL 43.212488 -144.57204 \nL 41.226972 -149.912958 \nL 39.241456 -154.513261 \nL 37.25594 -158.087975 \nL 35.270425 -160.381027 \nL 33.284909 -161.189472 \nL 31.299393 -160.384558 \nL 29.313877 -157.926743 \nL 27.328361 -153.87244 \nL 25.342845 -148.371335 \nL 23.357329 -141.654427 \nL 21.371813 -134.014244 \nL 19.386297 -125.779729 \nL 17.400781 -117.288983 \nL 15.415266 -108.863125 \nL 13.42975 -100.784162 \nL 11.444234 -93.278983 \nL 9.458718 -86.510495 \nL 7.473202 -80.575922 \nL 5.487686 -75.511315 \nL 3.50217 -71.300712 \nL 1.516654 -67.88807 \nL -0.468862 -65.190132 \nL -2.454378 -63.108659 \nL -4.439893 -61.540916 \nL -6.425409 -60.387768 \nL -8.410925 -59.559215 \nL -10.396441 -58.977536 \nL -12.381957 -58.578456 \nz\n\" id=\"m816aa4adf5\" style=\"stroke:#ee854a;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p54f5e28acb)\">\n     <use style=\"fill:#ee854a;fill-opacity:0.25;stroke:#ee854a;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m816aa4adf5\" y=\"277.626094\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 28.620625 219.777187 \nL 28.620625 26.877188 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 341.733125 219.777187 \nL 341.733125 26.877188 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 28.620625 219.777187 \nL 341.733125 219.777187 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 28.620625 26.877188 \nL 341.733125 26.877188 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"text_9\">\n    <!-- sensitive attibute: race -->\n    <g style=\"fill:#262626;\" transform=\"translate(81.155156 20.877187)scale(0.18 -0.18)\">\n     <defs>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 0 \nL 11.71875 0 \nz\nM 11.71875 51.703125 \nL 22.015625 51.703125 \nL 22.015625 39.3125 \nL 11.71875 39.3125 \nz\n\" id=\"DejaVuSans-58\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"113.623047\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"177.001953\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"229.101562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"256.884766\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"296.09375\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"323.876953\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"383.056641\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"444.580078\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"476.367188\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"537.646484\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"576.855469\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"616.064453\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"643.847656\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"707.324219\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"770.703125\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"809.912109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"871.435547\" xlink:href=\"#DejaVuSans-58\"/>\n     <use x=\"905.126953\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"936.914062\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"978.027344\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1039.306641\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1094.287109\" xlink:href=\"#DejaVuSans-101\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 378.783125 219.777187 \nL 691.895625 219.777187 \nL 691.895625 26.877188 \nL 378.783125 26.877188 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_7\">\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(365.663047 244.814609)scale(0.165 -0.165)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(428.285547 244.814609)scale(0.165 -0.165)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(490.908047 244.814609)scale(0.165 -0.165)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g style=\"fill:#262626;\" transform=\"translate(553.530547 244.814609)scale(0.165 -0.165)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"text_14\">\n      <!-- 0.8 -->\n      <g style=\"fill:#262626;\" transform=\"translate(616.153047 244.814609)scale(0.165 -0.165)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"text_15\">\n      <!-- 1.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(678.775547 244.814609)scale(0.165 -0.165)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- $P({income&gt;50K}|z_{sex})$ -->\n     <g style=\"fill:#262626;\" transform=\"translate(442.549375 266.106094)scale(0.18 -0.18)\">\n      <defs>\n       <path d=\"M 50 53.078125 \nL 48.296875 44.578125 \nQ 44.734375 46.53125 40.765625 47.5 \nQ 36.8125 48.484375 32.625 48.484375 \nQ 25.53125 48.484375 21.453125 46.0625 \nQ 17.390625 43.65625 17.390625 39.5 \nQ 17.390625 34.671875 26.859375 32.078125 \nQ 27.59375 31.890625 27.9375 31.78125 \nL 30.8125 30.90625 \nQ 39.796875 28.421875 42.796875 25.6875 \nQ 45.796875 22.953125 45.796875 18.21875 \nQ 45.796875 9.515625 38.890625 4.046875 \nQ 31.984375 -1.421875 20.796875 -1.421875 \nQ 16.453125 -1.421875 11.671875 -0.578125 \nQ 6.890625 0.25 1.125 2 \nL 2.875 11.28125 \nQ 7.8125 8.734375 12.59375 7.421875 \nQ 17.390625 6.109375 21.78125 6.109375 \nQ 28.375 6.109375 32.5 8.9375 \nQ 36.625 11.765625 36.625 16.109375 \nQ 36.625 20.796875 25.78125 23.6875 \nL 24.859375 23.921875 \nL 21.78125 24.703125 \nQ 14.9375 26.515625 11.765625 29.46875 \nQ 8.59375 32.421875 8.59375 37.015625 \nQ 8.59375 45.75 15.15625 50.875 \nQ 21.734375 56 33.015625 56 \nQ 37.453125 56 41.671875 55.265625 \nQ 45.90625 54.546875 50 53.078125 \nz\n\" id=\"DejaVuSans-Oblique-115\"/>\n       <path d=\"M 60.015625 54.6875 \nL 34.90625 27.875 \nL 50.296875 0 \nL 39.984375 0 \nL 28.421875 21.6875 \nL 8.296875 0 \nL -2.59375 0 \nL 24.3125 28.8125 \nL 10.015625 54.6875 \nL 20.3125 54.6875 \nL 30.8125 34.90625 \nL 49.125 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-120\"/>\n      </defs>\n      <use transform=\"translate(0 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-80\"/>\n      <use transform=\"translate(60.302734 0.578125)\" xlink:href=\"#DejaVuSans-40\"/>\n      <use transform=\"translate(99.316406 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-105\"/>\n      <use transform=\"translate(127.099609 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-110\"/>\n      <use transform=\"translate(190.478516 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-99\"/>\n      <use transform=\"translate(245.458984 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-111\"/>\n      <use transform=\"translate(306.640625 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-109\"/>\n      <use transform=\"translate(404.052734 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(485.058594 0.578125)\" xlink:href=\"#DejaVuSans-62\"/>\n      <use transform=\"translate(588.330078 0.578125)\" xlink:href=\"#DejaVuSans-53\"/>\n      <use transform=\"translate(651.953125 0.578125)\" xlink:href=\"#DejaVuSans-48\"/>\n      <use transform=\"translate(715.576172 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-75\"/>\n      <use transform=\"translate(781.152344 0.578125)\" xlink:href=\"#DejaVuSans-124\"/>\n      <use transform=\"translate(814.84375 0.578125)\" xlink:href=\"#DejaVuSans-Oblique-122\"/>\n      <use transform=\"translate(867.333984 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-115\"/>\n      <use transform=\"translate(903.803711 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-101\"/>\n      <use transform=\"translate(946.870117 -15.828125)scale(0.7)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n      <use transform=\"translate(991.030273 0.578125)\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\"/>\n   <g id=\"PolyCollection_3\">\n    <defs>\n     <path d=\"M 341.181565 -59.108057 \nL 341.181565 -57.848906 \nL 343.1329 -57.848906 \nL 345.084234 -57.848906 \nL 347.035569 -57.848906 \nL 348.986904 -57.848906 \nL 350.938239 -57.848906 \nL 352.889574 -57.848906 \nL 354.840908 -57.848906 \nL 356.792243 -57.848906 \nL 358.743578 -57.848906 \nL 360.694913 -57.848906 \nL 362.646247 -57.848906 \nL 364.597582 -57.848906 \nL 366.548917 -57.848906 \nL 368.500252 -57.848906 \nL 370.451587 -57.848906 \nL 372.402921 -57.848906 \nL 374.354256 -57.848906 \nL 376.305591 -57.848906 \nL 378.256926 -57.848906 \nL 380.20826 -57.848906 \nL 382.159595 -57.848906 \nL 384.11093 -57.848906 \nL 386.062265 -57.848906 \nL 388.0136 -57.848906 \nL 389.964934 -57.848906 \nL 391.916269 -57.848906 \nL 393.867604 -57.848906 \nL 395.818939 -57.848906 \nL 397.770273 -57.848906 \nL 399.721608 -57.848906 \nL 401.672943 -57.848906 \nL 403.624278 -57.848906 \nL 405.575612 -57.848906 \nL 407.526947 -57.848906 \nL 409.478282 -57.848906 \nL 411.429617 -57.848906 \nL 413.380952 -57.848906 \nL 415.332286 -57.848906 \nL 417.283621 -57.848906 \nL 419.234956 -57.848906 \nL 421.186291 -57.848906 \nL 423.137625 -57.848906 \nL 425.08896 -57.848906 \nL 427.040295 -57.848906 \nL 428.99163 -57.848906 \nL 430.942965 -57.848906 \nL 432.894299 -57.848906 \nL 434.845634 -57.848906 \nL 436.796969 -57.848906 \nL 438.748304 -57.848906 \nL 440.699638 -57.848906 \nL 442.650973 -57.848906 \nL 444.602308 -57.848906 \nL 446.553643 -57.848906 \nL 448.504978 -57.848906 \nL 450.456312 -57.848906 \nL 452.407647 -57.848906 \nL 454.358982 -57.848906 \nL 456.310317 -57.848906 \nL 458.261651 -57.848906 \nL 460.212986 -57.848906 \nL 462.164321 -57.848906 \nL 464.115656 -57.848906 \nL 466.066991 -57.848906 \nL 468.018325 -57.848906 \nL 469.96966 -57.848906 \nL 471.920995 -57.848906 \nL 473.87233 -57.848906 \nL 475.823664 -57.848906 \nL 477.774999 -57.848906 \nL 479.726334 -57.848906 \nL 481.677669 -57.848906 \nL 483.629004 -57.848906 \nL 485.580338 -57.848906 \nL 487.531673 -57.848906 \nL 489.483008 -57.848906 \nL 491.434343 -57.848906 \nL 493.385677 -57.848906 \nL 495.337012 -57.848906 \nL 497.288347 -57.848906 \nL 499.239682 -57.848906 \nL 501.191016 -57.848906 \nL 503.142351 -57.848906 \nL 505.093686 -57.848906 \nL 507.045021 -57.848906 \nL 508.996356 -57.848906 \nL 510.94769 -57.848906 \nL 512.899025 -57.848906 \nL 514.85036 -57.848906 \nL 516.801695 -57.848906 \nL 518.753029 -57.848906 \nL 520.704364 -57.848906 \nL 522.655699 -57.848906 \nL 524.607034 -57.848906 \nL 526.558369 -57.848906 \nL 528.509703 -57.848906 \nL 530.461038 -57.848906 \nL 532.412373 -57.848906 \nL 534.363708 -57.848906 \nL 536.315042 -57.848906 \nL 538.266377 -57.848906 \nL 540.217712 -57.848906 \nL 542.169047 -57.848906 \nL 544.120382 -57.848906 \nL 546.071716 -57.848906 \nL 548.023051 -57.848906 \nL 549.974386 -57.848906 \nL 551.925721 -57.848906 \nL 553.877055 -57.848906 \nL 555.82839 -57.848906 \nL 557.779725 -57.848906 \nL 559.73106 -57.848906 \nL 561.682395 -57.848906 \nL 563.633729 -57.848906 \nL 565.585064 -57.848906 \nL 567.536399 -57.848906 \nL 569.487734 -57.848906 \nL 571.439068 -57.848906 \nL 573.390403 -57.848906 \nL 575.341738 -57.848906 \nL 577.293073 -57.848906 \nL 579.244408 -57.848906 \nL 581.195742 -57.848906 \nL 583.147077 -57.848906 \nL 585.098412 -57.848906 \nL 587.049747 -57.848906 \nL 589.001081 -57.848906 \nL 590.952416 -57.848906 \nL 592.903751 -57.848906 \nL 594.855086 -57.848906 \nL 596.80642 -57.848906 \nL 598.757755 -57.848906 \nL 600.70909 -57.848906 \nL 602.660425 -57.848906 \nL 604.61176 -57.848906 \nL 606.563094 -57.848906 \nL 608.514429 -57.848906 \nL 610.465764 -57.848906 \nL 612.417099 -57.848906 \nL 614.368433 -57.848906 \nL 616.319768 -57.848906 \nL 618.271103 -57.848906 \nL 620.222438 -57.848906 \nL 622.173773 -57.848906 \nL 624.125107 -57.848906 \nL 626.076442 -57.848906 \nL 628.027777 -57.848906 \nL 629.979112 -57.848906 \nL 631.930446 -57.848906 \nL 633.881781 -57.848906 \nL 635.833116 -57.848906 \nL 637.784451 -57.848906 \nL 639.735786 -57.848906 \nL 641.68712 -57.848906 \nL 643.638455 -57.848906 \nL 645.58979 -57.848906 \nL 647.541125 -57.848906 \nL 649.492459 -57.848906 \nL 651.443794 -57.848906 \nL 653.395129 -57.848906 \nL 655.346464 -57.848906 \nL 657.297799 -57.848906 \nL 659.249133 -57.848906 \nL 661.200468 -57.848906 \nL 663.151803 -57.848906 \nL 665.103138 -57.848906 \nL 667.054472 -57.848906 \nL 669.005807 -57.848906 \nL 670.957142 -57.848906 \nL 672.908477 -57.848906 \nL 674.859811 -57.848906 \nL 676.811146 -57.848906 \nL 678.762481 -57.848906 \nL 680.713816 -57.848906 \nL 682.665151 -57.848906 \nL 684.616485 -57.848906 \nL 686.56782 -57.848906 \nL 688.519155 -57.848906 \nL 690.47049 -57.848906 \nL 692.421824 -57.848906 \nL 694.373159 -57.848906 \nL 696.324494 -57.848906 \nL 698.275829 -57.848906 \nL 700.227164 -57.848906 \nL 702.178498 -57.848906 \nL 704.129833 -57.848906 \nL 706.081168 -57.848906 \nL 708.032503 -57.848906 \nL 709.983837 -57.848906 \nL 711.935172 -57.848906 \nL 713.886507 -57.848906 \nL 715.837842 -57.848906 \nL 717.789177 -57.848906 \nL 719.740511 -57.848906 \nL 721.691846 -57.848906 \nL 723.643181 -57.848906 \nL 725.594516 -57.848906 \nL 727.54585 -57.848906 \nL 729.497185 -57.848906 \nL 729.497185 -57.868013 \nL 729.497185 -57.868013 \nL 727.54585 -57.879688 \nL 725.594516 -57.897357 \nL 723.643181 -57.92342 \nL 721.691846 -57.960882 \nL 719.740511 -58.013341 \nL 717.789177 -58.084889 \nL 715.837842 -58.1799 \nL 713.886507 -58.302692 \nL 711.935172 -58.457074 \nL 709.983837 -58.645781 \nL 708.032503 -58.869878 \nL 706.081168 -59.128192 \nL 704.129833 -59.416884 \nL 702.178498 -59.729241 \nL 700.227164 -60.055804 \nL 698.275829 -60.384838 \nL 696.324494 -60.703177 \nL 694.373159 -60.997349 \nL 692.421824 -61.254862 \nL 690.47049 -61.465484 \nL 688.519155 -61.622329 \nL 686.56782 -61.722575 \nL 684.616485 -61.76771 \nL 682.665151 -61.763243 \nL 680.713816 -61.717944 \nL 678.762481 -61.642704 \nL 676.811146 -61.549204 \nL 674.859811 -61.44859 \nL 672.908477 -61.35033 \nL 670.957142 -61.261407 \nL 669.005807 -61.185928 \nL 667.054472 -61.125153 \nL 665.103138 -61.077906 \nL 663.151803 -61.04124 \nL 661.200468 -61.011241 \nL 659.249133 -60.98384 \nL 657.297799 -60.955496 \nL 655.346464 -60.923698 \nL 653.395129 -60.887226 \nL 651.443794 -60.846184 \nL 649.492459 -60.80185 \nL 647.541125 -60.756382 \nL 645.58979 -60.712488 \nL 643.638455 -60.673085 \nL 641.68712 -60.641034 \nL 639.735786 -60.61895 \nL 637.784451 -60.60912 \nL 635.833116 -60.613488 \nL 633.881781 -60.633703 \nL 631.930446 -60.671175 \nL 629.979112 -60.72713 \nL 628.027777 -60.80263 \nL 626.076442 -60.898553 \nL 624.125107 -61.01555 \nL 622.173773 -61.153972 \nL 620.222438 -61.313797 \nL 618.271103 -61.494562 \nL 616.319768 -61.695312 \nL 614.368433 -61.914558 \nL 612.417099 -62.150255 \nL 610.465764 -62.399775 \nL 608.514429 -62.659897 \nL 606.563094 -62.926803 \nL 604.61176 -63.1961 \nL 602.660425 -63.462894 \nL 600.70909 -63.721922 \nL 598.757755 -63.967759 \nL 596.80642 -64.195097 \nL 594.855086 -64.399078 \nL 592.903751 -64.575634 \nL 590.952416 -64.721807 \nL 589.001081 -64.835984 \nL 587.049747 -64.918021 \nL 585.098412 -64.969225 \nL 583.147077 -64.992207 \nL 581.195742 -64.990609 \nL 579.244408 -64.968759 \nL 577.293073 -64.931288 \nL 575.341738 -64.88277 \nL 573.390403 -64.827401 \nL 571.439068 -64.768765 \nL 569.487734 -64.709685 \nL 567.536399 -64.652159 \nL 565.585064 -64.597372 \nL 563.633729 -64.545767 \nL 561.682395 -64.497151 \nL 559.73106 -64.450831 \nL 557.779725 -64.405753 \nL 555.82839 -64.36065 \nL 553.877055 -64.314171 \nL 551.925721 -64.26501 \nL 549.974386 -64.21202 \nL 548.023051 -64.154304 \nL 546.071716 -64.091302 \nL 544.120382 -64.022844 \nL 542.169047 -63.949192 \nL 540.217712 -63.871052 \nL 538.266377 -63.789546 \nL 536.315042 -63.706173 \nL 534.363708 -63.622728 \nL 532.412373 -63.541203 \nL 530.461038 -63.463691 \nL 528.509703 -63.392281 \nL 526.558369 -63.328983 \nL 524.607034 -63.27568 \nL 522.655699 -63.234119 \nL 520.704364 -63.205933 \nL 518.753029 -63.192692 \nL 516.801695 -63.19596 \nL 514.85036 -63.217344 \nL 512.899025 -63.258525 \nL 510.94769 -63.321249 \nL 508.996356 -63.407289 \nL 507.045021 -63.518372 \nL 505.093686 -63.656084 \nL 503.142351 -63.821769 \nL 501.191016 -64.016431 \nL 499.239682 -64.240638 \nL 497.288347 -64.494455 \nL 495.337012 -64.777388 \nL 493.385677 -65.088351 \nL 491.434343 -65.425664 \nL 489.483008 -65.78708 \nL 487.531673 -66.16984 \nL 485.580338 -66.57078 \nL 483.629004 -66.986461 \nL 481.677669 -67.413328 \nL 479.726334 -67.847887 \nL 477.774999 -68.286848 \nL 475.823664 -68.727247 \nL 473.87233 -69.166507 \nL 471.920995 -69.602435 \nL 469.96966 -70.033164 \nL 468.018325 -70.457068 \nL 466.066991 -70.872674 \nL 464.115656 -71.278612 \nL 462.164321 -71.673639 \nL 460.212986 -72.056757 \nL 458.261651 -72.427422 \nL 456.310317 -72.785827 \nL 454.358982 -73.133226 \nL 452.407647 -73.472244 \nL 450.456312 -73.807119 \nL 448.504978 -74.143845 \nL 446.553643 -74.490176 \nL 444.602308 -74.855492 \nL 442.650973 -75.250563 \nL 440.699638 -75.687249 \nL 438.748304 -76.178224 \nL 436.796969 -76.736813 \nL 434.845634 -77.37703 \nL 432.894299 -78.113925 \nL 430.942965 -78.964297 \nL 428.99163 -79.947841 \nL 427.040295 -81.088723 \nL 425.08896 -82.417585 \nL 423.137625 -83.973863 \nL 421.186291 -85.808285 \nL 419.234956 -87.985277 \nL 417.283621 -90.584884 \nL 415.332286 -93.703652 \nL 413.380952 -97.453773 \nL 411.429617 -101.959622 \nL 409.478282 -107.350856 \nL 407.526947 -113.751327 \nL 405.575612 -121.26352 \nL 403.624278 -129.948876 \nL 401.672943 -139.805323 \nL 399.721608 -150.744421 \nL 397.770273 -162.571575 \nL 395.818939 -174.973406 \nL 393.867604 -187.516472 \nL 391.916269 -199.660675 \nL 389.964934 -210.788963 \nL 388.0136 -220.252468 \nL 386.062265 -227.427294 \nL 384.11093 -231.776561 \nL 382.159595 -232.909431 \nL 380.20826 -230.628382 \nL 378.256926 -224.957139 \nL 376.305591 -216.144335 \nL 374.354256 -204.641724 \nL 372.402921 -191.059812 \nL 370.451587 -176.107336 \nL 368.500252 -160.523337 \nL 366.548917 -145.011249 \nL 364.597582 -130.183391 \nL 362.646247 -116.521837 \nL 360.694913 -104.358493 \nL 358.743578 -93.873959 \nL 356.792243 -85.112099 \nL 354.840908 -78.005513 \nL 352.889574 -72.406535 \nL 350.938239 -68.118771 \nL 348.986904 -64.925298 \nL 347.035569 -62.611104 \nL 345.084234 -60.978799 \nL 343.1329 -59.857808 \nL 341.181565 -59.108057 \nz\n\" id=\"madfeac88e3\" style=\"stroke:#4878d0;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p9283f9c4ed)\">\n     <use style=\"fill:#4878d0;fill-opacity:0.25;stroke:#4878d0;stroke-width:1.5;\" x=\"0\" xlink:href=\"#madfeac88e3\" y=\"277.626094\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_4\">\n    <defs>\n     <path d=\"M 334.129231 -58.382555 \nL 334.129231 -57.848906 \nL 336.151444 -57.848906 \nL 338.173656 -57.848906 \nL 340.195869 -57.848906 \nL 342.218081 -57.848906 \nL 344.240294 -57.848906 \nL 346.262506 -57.848906 \nL 348.284719 -57.848906 \nL 350.306931 -57.848906 \nL 352.329144 -57.848906 \nL 354.351356 -57.848906 \nL 356.373569 -57.848906 \nL 358.395781 -57.848906 \nL 360.417994 -57.848906 \nL 362.440206 -57.848906 \nL 364.462419 -57.848906 \nL 366.484631 -57.848906 \nL 368.506844 -57.848906 \nL 370.529056 -57.848906 \nL 372.551269 -57.848906 \nL 374.573481 -57.848906 \nL 376.595694 -57.848906 \nL 378.617906 -57.848906 \nL 380.640119 -57.848906 \nL 382.662331 -57.848906 \nL 384.684544 -57.848906 \nL 386.706756 -57.848906 \nL 388.728969 -57.848906 \nL 390.751181 -57.848906 \nL 392.773394 -57.848906 \nL 394.795606 -57.848906 \nL 396.817819 -57.848906 \nL 398.840031 -57.848906 \nL 400.862244 -57.848906 \nL 402.884456 -57.848906 \nL 404.906669 -57.848906 \nL 406.928881 -57.848906 \nL 408.951094 -57.848906 \nL 410.973306 -57.848906 \nL 412.995519 -57.848906 \nL 415.017731 -57.848906 \nL 417.039944 -57.848906 \nL 419.062156 -57.848906 \nL 421.084369 -57.848906 \nL 423.106581 -57.848906 \nL 425.128794 -57.848906 \nL 427.151006 -57.848906 \nL 429.173219 -57.848906 \nL 431.195431 -57.848906 \nL 433.217644 -57.848906 \nL 435.239856 -57.848906 \nL 437.262069 -57.848906 \nL 439.284281 -57.848906 \nL 441.306494 -57.848906 \nL 443.328706 -57.848906 \nL 445.350919 -57.848906 \nL 447.373131 -57.848906 \nL 449.395344 -57.848906 \nL 451.417556 -57.848906 \nL 453.439769 -57.848906 \nL 455.461981 -57.848906 \nL 457.484194 -57.848906 \nL 459.506406 -57.848906 \nL 461.528619 -57.848906 \nL 463.550831 -57.848906 \nL 465.573044 -57.848906 \nL 467.595256 -57.848906 \nL 469.617469 -57.848906 \nL 471.639681 -57.848906 \nL 473.661894 -57.848906 \nL 475.684106 -57.848906 \nL 477.706319 -57.848906 \nL 479.728531 -57.848906 \nL 481.750744 -57.848906 \nL 483.772956 -57.848906 \nL 485.795169 -57.848906 \nL 487.817381 -57.848906 \nL 489.839594 -57.848906 \nL 491.861806 -57.848906 \nL 493.884019 -57.848906 \nL 495.906231 -57.848906 \nL 497.928444 -57.848906 \nL 499.950656 -57.848906 \nL 501.972869 -57.848906 \nL 503.995081 -57.848906 \nL 506.017294 -57.848906 \nL 508.039506 -57.848906 \nL 510.061719 -57.848906 \nL 512.083931 -57.848906 \nL 514.106144 -57.848906 \nL 516.128356 -57.848906 \nL 518.150569 -57.848906 \nL 520.172781 -57.848906 \nL 522.194994 -57.848906 \nL 524.217206 -57.848906 \nL 526.239419 -57.848906 \nL 528.261631 -57.848906 \nL 530.283844 -57.848906 \nL 532.306056 -57.848906 \nL 534.328269 -57.848906 \nL 536.350481 -57.848906 \nL 538.372694 -57.848906 \nL 540.394906 -57.848906 \nL 542.417119 -57.848906 \nL 544.439331 -57.848906 \nL 546.461544 -57.848906 \nL 548.483756 -57.848906 \nL 550.505969 -57.848906 \nL 552.528181 -57.848906 \nL 554.550394 -57.848906 \nL 556.572606 -57.848906 \nL 558.594819 -57.848906 \nL 560.617031 -57.848906 \nL 562.639244 -57.848906 \nL 564.661456 -57.848906 \nL 566.683669 -57.848906 \nL 568.705881 -57.848906 \nL 570.728094 -57.848906 \nL 572.750306 -57.848906 \nL 574.772519 -57.848906 \nL 576.794731 -57.848906 \nL 578.816944 -57.848906 \nL 580.839156 -57.848906 \nL 582.861369 -57.848906 \nL 584.883581 -57.848906 \nL 586.905794 -57.848906 \nL 588.928006 -57.848906 \nL 590.950219 -57.848906 \nL 592.972431 -57.848906 \nL 594.994644 -57.848906 \nL 597.016856 -57.848906 \nL 599.039069 -57.848906 \nL 601.061281 -57.848906 \nL 603.083494 -57.848906 \nL 605.105706 -57.848906 \nL 607.127919 -57.848906 \nL 609.150131 -57.848906 \nL 611.172344 -57.848906 \nL 613.194556 -57.848906 \nL 615.216769 -57.848906 \nL 617.238981 -57.848906 \nL 619.261194 -57.848906 \nL 621.283406 -57.848906 \nL 623.305619 -57.848906 \nL 625.327831 -57.848906 \nL 627.350044 -57.848906 \nL 629.372256 -57.848906 \nL 631.394469 -57.848906 \nL 633.416681 -57.848906 \nL 635.438894 -57.848906 \nL 637.461106 -57.848906 \nL 639.483319 -57.848906 \nL 641.505531 -57.848906 \nL 643.527744 -57.848906 \nL 645.549956 -57.848906 \nL 647.572169 -57.848906 \nL 649.594381 -57.848906 \nL 651.616594 -57.848906 \nL 653.638806 -57.848906 \nL 655.661019 -57.848906 \nL 657.683231 -57.848906 \nL 659.705444 -57.848906 \nL 661.727656 -57.848906 \nL 663.749869 -57.848906 \nL 665.772081 -57.848906 \nL 667.794294 -57.848906 \nL 669.816506 -57.848906 \nL 671.838719 -57.848906 \nL 673.860931 -57.848906 \nL 675.883144 -57.848906 \nL 677.905356 -57.848906 \nL 679.927569 -57.848906 \nL 681.949781 -57.848906 \nL 683.971994 -57.848906 \nL 685.994206 -57.848906 \nL 688.016419 -57.848906 \nL 690.038631 -57.848906 \nL 692.060844 -57.848906 \nL 694.083056 -57.848906 \nL 696.105269 -57.848906 \nL 698.127481 -57.848906 \nL 700.149694 -57.848906 \nL 702.171906 -57.848906 \nL 704.194119 -57.848906 \nL 706.216331 -57.848906 \nL 708.238544 -57.848906 \nL 710.260756 -57.848906 \nL 712.282969 -57.848906 \nL 714.305181 -57.848906 \nL 716.327394 -57.848906 \nL 718.349606 -57.848906 \nL 720.371819 -57.848906 \nL 722.394031 -57.848906 \nL 724.416244 -57.848906 \nL 726.438456 -57.848906 \nL 728.460669 -57.848906 \nL 730.482881 -57.848906 \nL 732.505094 -57.848906 \nL 734.527306 -57.848906 \nL 736.549519 -57.848906 \nL 736.549519 -57.896891 \nL 736.549519 -57.896891 \nL 734.527306 -57.92152 \nL 732.505094 -57.956845 \nL 730.482881 -58.006525 \nL 728.460669 -58.075024 \nL 726.438456 -58.167605 \nL 724.416244 -58.290253 \nL 722.394031 -58.449482 \nL 720.371819 -58.652033 \nL 718.349606 -58.904447 \nL 716.327394 -59.212528 \nL 714.305181 -59.580721 \nL 712.282969 -60.011461 \nL 710.260756 -60.504562 \nL 708.238544 -61.056712 \nL 706.216331 -61.661176 \nL 704.194119 -62.307755 \nL 702.171906 -62.983063 \nL 700.149694 -63.671136 \nL 698.127481 -64.354332 \nL 696.105269 -65.014468 \nL 694.083056 -65.634073 \nL 692.060844 -66.197651 \nL 690.038631 -66.692784 \nL 688.016419 -67.110986 \nL 685.994206 -67.448204 \nL 683.971994 -67.704912 \nL 681.949781 -67.885816 \nL 679.927569 -67.999213 \nL 677.905356 -68.056094 \nL 675.883144 -68.069109 \nL 673.860931 -68.051502 \nL 671.838719 -68.016128 \nL 669.816506 -67.974636 \nL 667.794294 -67.936864 \nL 665.772081 -67.910462 \nL 663.749869 -67.900752 \nL 661.727656 -67.910772 \nL 659.705444 -67.94149 \nL 657.683231 -67.992129 \nL 655.661019 -68.060574 \nL 653.638806 -68.143821 \nL 651.616594 -68.238438 \nL 649.594381 -68.341019 \nL 647.572169 -68.448589 \nL 645.549956 -68.558959 \nL 643.527744 -68.670978 \nL 641.505531 -68.784691 \nL 639.483319 -68.901368 \nL 637.461106 -69.023404 \nL 635.438894 -69.154108 \nL 633.416681 -69.297393 \nL 631.394469 -69.457414 \nL 629.372256 -69.638173 \nL 627.350044 -69.843164 \nL 625.327831 -70.075074 \nL 623.305619 -70.335592 \nL 621.283406 -70.625316 \nL 619.261194 -70.943792 \nL 617.238981 -71.289633 \nL 615.216769 -71.660721 \nL 613.194556 -72.054429 \nL 611.172344 -72.467843 \nL 609.150131 -72.897938 \nL 607.127919 -73.341683 \nL 605.105706 -73.796069 \nL 603.083494 -74.258056 \nL 601.061281 -74.724457 \nL 599.039069 -75.191788 \nL 597.016856 -75.656117 \nL 594.994644 -76.11295 \nL 592.972431 -76.557166 \nL 590.950219 -76.983053 \nL 588.928006 -77.384412 \nL 586.905794 -77.754757 \nL 584.883581 -78.087569 \nL 582.861369 -78.376589 \nL 580.839156 -78.61612 \nL 578.816944 -78.801294 \nL 576.794731 -78.928305 \nL 574.772519 -78.994566 \nL 572.750306 -78.998803 \nL 570.728094 -78.941073 \nL 568.705881 -78.822733 \nL 566.683669 -78.646357 \nL 564.661456 -78.415628 \nL 562.639244 -78.135211 \nL 560.617031 -77.810623 \nL 558.594819 -77.448104 \nL 556.572606 -77.054489 \nL 554.550394 -76.637088 \nL 552.528181 -76.203559 \nL 550.505969 -75.761783 \nL 548.483756 -75.319731 \nL 546.461544 -74.88532 \nL 544.439331 -74.466262 \nL 542.417119 -74.069904 \nL 540.394906 -73.703068 \nL 538.372694 -73.371885 \nL 536.350481 -73.081654 \nL 534.328269 -72.836714 \nL 532.306056 -72.640361 \nL 530.283844 -72.494797 \nL 528.261631 -72.40115 \nL 526.239419 -72.359531 \nL 524.217206 -72.369155 \nL 522.194994 -72.428501 \nL 520.172781 -72.535495 \nL 518.150569 -72.687699 \nL 516.128356 -72.882492 \nL 514.106144 -73.117208 \nL 512.083931 -73.389237 \nL 510.061719 -73.696065 \nL 508.039506 -74.035269 \nL 506.017294 -74.404468 \nL 503.995081 -74.801256 \nL 501.972869 -75.223127 \nL 499.950656 -75.667414 \nL 497.928444 -76.131264 \nL 495.906231 -76.611647 \nL 493.884019 -77.105401 \nL 491.861806 -77.609312 \nL 489.839594 -78.120203 \nL 487.817381 -78.635025 \nL 485.795169 -79.150931 \nL 483.772956 -79.665322 \nL 481.750744 -80.175853 \nL 479.728531 -80.680414 \nL 477.706319 -81.177088 \nL 475.684106 -81.664104 \nL 473.661894 -82.139793 \nL 471.639681 -82.602575 \nL 469.617469 -83.050968 \nL 467.595256 -83.483638 \nL 465.573044 -83.899468 \nL 463.550831 -84.297643 \nL 461.528619 -84.67773 \nL 459.506406 -85.039748 \nL 457.484194 -85.384198 \nL 455.461981 -85.712068 \nL 453.439769 -86.024795 \nL 451.417556 -86.324219 \nL 449.395344 -86.612521 \nL 447.373131 -86.892183 \nL 445.350919 -87.16598 \nL 443.328706 -87.437037 \nL 441.306494 -87.708941 \nL 439.284281 -87.985942 \nL 437.262069 -88.273225 \nL 435.239856 -88.57726 \nL 433.217644 -88.906216 \nL 431.195431 -89.270424 \nL 429.173219 -89.682867 \nL 427.151006 -90.15966 \nL 425.128794 -90.720467 \nL 423.106581 -91.388773 \nL 421.084369 -92.191921 \nL 419.062156 -93.160772 \nL 417.039944 -94.328845 \nL 415.017731 -95.730806 \nL 412.995519 -97.400172 \nL 410.973306 -99.366183 \nL 408.951094 -101.649895 \nL 406.928881 -104.25967 \nL 404.906669 -107.186401 \nL 402.884456 -110.398972 \nL 400.862244 -113.840581 \nL 398.840031 -117.426595 \nL 396.817819 -121.04461 \nL 394.795606 -124.557212 \nL 392.773394 -127.807706 \nL 390.751181 -130.628705 \nL 388.728969 -132.853088 \nL 386.706756 -134.32644 \nL 384.684544 -134.919755 \nL 382.662331 -134.541033 \nL 380.640119 -133.144401 \nL 378.617906 -130.735602 \nL 376.595694 -127.373095 \nL 374.573481 -123.16453 \nL 372.551269 -118.258926 \nL 370.529056 -112.835425 \nL 368.506844 -107.089861 \nL 366.484631 -101.220638 \nL 364.462419 -95.415389 \nL 362.440206 -89.839695 \nL 360.417994 -84.628795 \nL 358.395781 -79.882763 \nL 356.373569 -75.665184 \nL 354.351356 -72.00495 \nL 352.329144 -68.900503 \nL 350.306931 -66.325689 \nL 348.284719 -64.236381 \nL 346.262506 -62.577065 \nL 344.240294 -61.286826 \nL 342.218081 -60.304287 \nL 340.195869 -59.571339 \nL 338.173656 -59.03563 \nL 336.151444 -58.651924 \nL 334.129231 -58.382555 \nz\n\" id=\"m62d5b3bfaa\" style=\"stroke:#ee854a;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p9283f9c4ed)\">\n     <use style=\"fill:#ee854a;fill-opacity:0.25;stroke:#ee854a;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m62d5b3bfaa\" y=\"277.626094\"/>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 378.783125 219.777187 \nL 378.783125 26.877188 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 691.895625 219.777187 \nL 691.895625 26.877188 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 378.783125 219.777187 \nL 691.895625 219.777187 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 378.783125 26.877188 \nL 691.895625 26.877188 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.875;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- sensitive attibute: sex -->\n    <g style=\"fill:#262626;\" transform=\"translate(435.623594 20.877187)scale(0.18 -0.18)\">\n     <defs>\n      <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"113.623047\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"177.001953\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"229.101562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"256.884766\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"296.09375\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"323.876953\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"383.056641\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"444.580078\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"476.367188\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"537.646484\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"576.855469\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"616.064453\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"643.847656\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"707.324219\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"770.703125\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"809.912109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"871.435547\" xlink:href=\"#DejaVuSans-58\"/>\n     <use x=\"905.126953\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"936.914062\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"989.013672\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1048.787109\" xlink:href=\"#DejaVuSans-120\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p54f5e28acb\">\n   <rect height=\"192.9\" width=\"313.1125\" x=\"28.620625\" y=\"26.877188\"/>\n  </clipPath>\n  <clipPath id=\"p9283f9c4ed\">\n   <rect height=\"192.9\" width=\"313.1125\" x=\"378.783125\" y=\"26.877188\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "fig = plot_distributions(y_pred, Z_test, fname='images/biased_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><img src=\"images/biased_training.png\" alt=\"Architecture\" width=\"750\"/><br></center>\n",
    "\n",
    "The figure shows for both race (=left plot) and sex (=right plot) the blue prediction distributions have a large peak at the low end of the probability range. This means that when a person is $black$ and/or $female$ there is a much higher probability of the classifier predicting an income below 50K compared to when someone is $white$ and/or $male$.\n",
    "\n",
    "So, the results of the qualitative analysis are quite clear: the predictions are definitely not fair when considered in the context of race and sex. When it comes to assigning the high-income levels, our model favours the usual suspects: white males."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative model fairness\n",
    "\n",
    "In order to get a 'quantitative' measure of how fair our classifier is, we take inspiration from the U.S. Equal Employment Opportunity Commission (EEOC). They use the so-called [80% rule](https://en.wikipedia.org/wiki/Disparate_impact#The_80%_rule) to quantify the disparate impact on a group of people of a protected characteristic. Zafar et al. show in their paper [\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf) how a more generic version of this rule, called the p%-rule, can be used to quantify fairness of a classifier. This rule is defined as follows:\n",
    "\n",
    "> A classifier that makes a binary class prediction $\\hat{y} \\in \\left\\{0,1 \\right\\}$ given a binary sensitive attribute $z\\in \\left\\{0,1 \\right\\}$ satisfies the p%-rule\n",
    "if the following inequality holds:\n",
    ">\n",
    ">$$\\min\\left(\\frac{P(\\hat{y}=1|z=1)}{P(\\hat{y}=1|z=0)}, \\frac{P(\\hat{y}=1|z=0)}{P(\\hat{y}=1|z=1)}\\right)\\geq\\frac{p}{100}$$\n",
    "\n",
    "The rule states that the ratio between the probability of a positive outcome given the sensitive attribute being true and the same probability given the sensitive attribute being false is no less than p:100. So, when a classifier is completely fair it will satisfy a 100%-rule. In contrast, when it is completely unfair it satisfies a %0-rule. \n",
    "\n",
    "In determining the fairness our or classifier we will follow the EEOC and say that a model is fair when it satisfies at least an 80%-rule. So, let's compute the p%-rules for the classifier and put a number on its fairness. Note that we will threshold our classifier at 0.5 to make its prediction it binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "def p_rule(y_pred, z_values, threshold=0.5):\n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = y_z_1.mean() / y_z_0.mean()\n",
    "    return np.min([odds, 1/odds]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The classifier satisfies the following %p-rules:\n\tgiven attribute race; 45%-rule\n\tgiven attribute sex;  33%-rule\n"
     ]
    }
   ],
   "source": [
    "print(\"The classifier satisfies the following %p-rules:\")\n",
    "print(f\"\\tgiven attribute race; {p_rule(y_pred, Z_test['race']):.0f}%-rule\")\n",
    "print(f\"\\tgiven attribute sex;  {p_rule(y_pred, Z_test['sex']):.0f}%-rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for both sensitive attributes the classifier satisfies a p%-rule that is significantly lower than 80%. This supports our earlier conclusion that the trained classifier is unfair in making its predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fighting the bias\n",
    "\n",
    "It is important to stress that training a fair model is not straightforward. One might be tempted to think that simply removing sensitive information from the training data is enough. Our classifier did not have access to the race and sex attributes and still we ended up with a model that is biased against women and black people. This begs the question: what caused our classifier to behave this way? \n",
    "\n",
    "The observed behaviour is most likely caused by biases in the training data. To understand how this works, consider the following two examples of image classification errors:\n",
    "\n",
    "<center><br><img src=\"images/missclassified_images.png\" alt=\"GAN Images\" width=\"300\"/><br></center>\n",
    "\n",
    "The classifier that made these errors was trained on data in which some ethnic and racial minorities are overrepresented by small number of classes. For example, black people are often shown playing basketball and Asian people playing ping-pong. The model picks up on these biases and uses them for making predictions. However, once unleashed into the wild it will encounter images in which these minorities are doing things other than playing basketball or ping-pong. Still relying on its learned biases, the model can misclassify these images in quite painful ways.\n",
    "\n",
    "Now, the UCI dataset, used for training our classifier, has similar kinds of biases in the data. The dataset is based on census data from 1994, a time in which income inequality was just as much of an issue as it is nowadays. Not surprisingly, most of the high earners in the data are white males, while women and black people are more often part of the low-income group. Our predictive model can indirectly learn these biases, for example, through characteristics like education level and zip-code of residence. As a result, we end-up with the unfair predictions observed in previous section, even after having removed the race and sex attributes.\n",
    "\n",
    "How can we go about fixing this issue? In general, there are two approaches we can take. We can somehow try to de-bias the dataset, for example by adding additional data that comes from a more representative sample. Alternatively, we can constrain the model so that it is forced into making fairer predictions. In the next section, we will show how adversarial networks can help in taking the second approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial networks FTW\n",
    "\n",
    "In 2014, Goodfellow et al. published their seminal [paper](https://arxiv.org/abs/1406.2661) on Generative Adversarial Networks (GANs). They introduce GANs as a system of two neural networks, a generative model and an adversarial classifier, which are competing with each other in a zero-sum game. In the game, the generative model focusses on producing samples that are indistinguishable from real data, while the adversarial classifier tries to identify if samples came from the generative model or from the real data. Both networks are trained simultaneously such that the first improves at producing realistic samples, while the second becomes better at spotting the fakes from the real. The figure below shows some examples of images that were generated by a GAN:\n",
    "\n",
    "<center><br><img src=\"images/gan_examples.png\" alt=\"GAN Images\" width=\"500\"/><br></center>\n",
    "\n",
    "Our procedure for training a fair income classifier takes inspiration from GANs: it leverages adversarial networks to enforce the so-called [pivotal property](https://en.wikipedia.org/wiki/Pivotal_quantity) on the predictive model. This statistical property assures that the outcome distribution of the model no longer depends on so-called nuisance parameters. These parameters are not of immediate interest, but must be accounted for in a statistical analysis. By taking the sensitive attributes as our nuisance parameters we can enforce predictions that are independent of, in our case, race and sex. This is exactly what we need for making fair predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial training procedure\n",
    "\n",
    "The starting point for adversarial training our classifier is the extension of the original network architecture with an adversarial component. The figure below shows what this extended architecture looks like:\n",
    "\n",
    "<center><br><img src=\"images/architecture.png\" alt=\"Architecture\" width=\"750\"/><br></center>\n",
    "\n",
    "At first glance, this system of two neural networks looks very similar to the one used for training GANs. However, there are some key differences. First, the generative model has been replaced by a predictive model. So, instead of generating synthetic data it now generates actual predictions $\\hat{y}$ based on the input $X$. Second, the task of the adversarial is no longer to distinguish real from generated data. Instead, it predicts the sensitive attribute values $\\hat{z}\\in\\hat{Z}$ from the predicted $\\hat{y}$ of the classifier. Finally, the objectives that both nets try to optimize are based on the prediction losses of the target and sensitive attributes, these are denoted by $Loss_{y}(\\theta_{clf})$ and $Loss_{Z}(\\theta_{clf},\\theta_{adv})$ in the figure.\n",
    "\n",
    "Lets consider the nature of the zero-sum game the classifier and adversarial are engaged in. For the classifier the objective of twofold: make the best possible income level predictions whilst ensuring that race or sex cannot be derived from them. This is captured by the following objective function:\n",
    "\n",
    "$$\\min_{\\theta_{clf}}\\left[Loss_{y}(\\theta_{clf})-\\lambda Loss_{Z}(\\theta_{clf},\\theta_{adv})\\right].$$\n",
    "\n",
    "So, it learns to minimize its own prediction losses while maximizing that of the adversarial (due to $\\lambda$ being positive and minimizing a negated loss is the same as maximizing it). Note that increasing the size of $\\lambda$ steers the classifier towards fairer predictions while sacrificing prediction accuracy. The objective during the game is simpler For the adversarial: predict race and sex based on the income level predictions of the classifier. This is captured in the following objective function:\n",
    "\n",
    "$$\\min_{\\theta_{adv}}\\left[Loss_{Z}(\\theta_{clf},\\theta_{adv})\\right].$$\n",
    "\n",
    "The adversarial does not care about the prediction accuracy of the classifier. It is only concerned with minimizing its own prediction losses.\n",
    "\n",
    "Now that our classifier is upgraded with an adversarial component, we turn to the adversarial training procedure. In short, we can summarize this procedure in the following 3 steps:\n",
    "\n",
    "1. Pre-train the classifier on the full data set.\n",
    "2. Pre-train the adversarial on the predictions of the pre-trained classifier.\n",
    "3. During $T$ iterations simultaneously train the adversarial and classifier networks:\n",
    "   - first train the adversarial for a single epoch while keeping the classifier fixed\n",
    "   - then train the classifier on a single sampled mini batch while keeping the adversarial fixed.\n",
    "\n",
    "The actual adversarial training starts only after the first two pre-training steps. It is then that the training procedure mimics the zero-sum game during which our classifier will (hopefully) learn how make predictions that are both accurate and fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair income predictions\n",
    "\n",
    "Finally, we are ready to adverserial train a fair classifier. We kick-off by initializing our newly upgraded classifier and pre-train both the classifier and adverserial networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "\n",
    "class FairClassifier(object):\n",
    "    \n",
    "    def __init__(self, n_features, n_sensitive, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        clf_inputs = Input(shape=(n_features,))\n",
    "        adv_inputs = Input(shape=(1,))\n",
    "        \n",
    "        clf_net = self._create_clf_net(clf_inputs)\n",
    "        adv_net = self._create_adv_net(adv_inputs, n_sensitive)\n",
    "        self._trainable_clf_net = self._make_trainable(clf_net)\n",
    "        self._trainable_adv_net = self._make_trainable(adv_net)\n",
    "        self._clf = self._compile_clf(clf_net)\n",
    "        self._clf_w_adv = self._compile_clf_w_adv(clf_inputs, clf_net, adv_net)\n",
    "        self._adv = self._compile_adv(clf_inputs, clf_net, adv_net, n_sensitive)\n",
    "        self._val_metrics = None\n",
    "        self._fairness_metrics = None\n",
    "        \n",
    "        self.predict = self._clf.predict\n",
    "        \n",
    "    def _make_trainable(self, net):\n",
    "        def make_trainable(flag):\n",
    "            net.trainable = flag\n",
    "            for layer in net.layers:\n",
    "                layer.trainable = flag\n",
    "        return make_trainable\n",
    "        \n",
    "    def _create_clf_net(self, inputs):\n",
    "        dense1 = Dense(32, activation='relu')(inputs)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        dense2 = Dense(32, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        dense3 = Dense(32, activation='relu')(dropout2)\n",
    "        dropout3 = Dropout(0.2)(dense3)\n",
    "        outputs = Dense(1, activation='sigmoid', name='y')(dropout3)\n",
    "        return Model(inputs=[inputs], outputs=[outputs])\n",
    "        \n",
    "    def _create_adv_net(self, inputs, n_sensitive):\n",
    "        dense1 = Dense(32, activation='relu')(inputs)\n",
    "        dense2 = Dense(32, activation='relu')(dense1)\n",
    "        dense3 = Dense(32, activation='relu')(dense2)\n",
    "        outputs = [Dense(1, activation='sigmoid')(dense3) for _ in range(n_sensitive)]\n",
    "        return Model(inputs=[inputs], outputs=outputs)\n",
    "\n",
    "    def _compile_clf(self, clf_net):\n",
    "        clf = clf_net\n",
    "        self._trainable_clf_net(True)\n",
    "        clf.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        return clf\n",
    "        \n",
    "    def _compile_clf_w_adv(self, inputs, clf_net, adv_net):\n",
    "        clf_w_adv = Model(inputs=[inputs], outputs=[clf_net(inputs)]+adv_net(clf_net(inputs)))\n",
    "        self._trainable_clf_net(True)\n",
    "        self._trainable_adv_net(False)\n",
    "        loss_weights = [1.]+[-lambda_param for lambda_param in self.lambdas]\n",
    "        clf_w_adv.compile(loss=['binary_crossentropy']*(len(loss_weights)), \n",
    "                          loss_weights=loss_weights,\n",
    "                          optimizer='adam')\n",
    "        return clf_w_adv\n",
    "\n",
    "    def _compile_adv(self, inputs, clf_net, adv_net, n_sensitive):\n",
    "        adv = Model(inputs=[inputs], outputs=adv_net(clf_net(inputs)))\n",
    "        self._trainable_clf_net(False)\n",
    "        self._trainable_adv_net(True)\n",
    "        adv.compile(loss=['binary_crossentropy']*n_sensitive, optimizer='adam')\n",
    "        return adv\n",
    "\n",
    "    def _compute_class_weights(self, data_set):\n",
    "        class_values = [0, 1]\n",
    "        class_weights = []\n",
    "        if len(data_set.shape) == 1:\n",
    "            balanced_weights = compute_class_weight('balanced', class_values, data_set)\n",
    "            class_weights.append(dict(zip(class_values, balanced_weights)))\n",
    "        else:\n",
    "            n_attr =  data_set.shape[1]\n",
    "            for attr_idx in range(n_attr):\n",
    "                balanced_weights = compute_class_weight('balanced', class_values,\n",
    "                                                        np.array(data_set)[:,attr_idx])\n",
    "                class_weights.append(dict(zip(class_values, balanced_weights)))\n",
    "        print(f'-compute-target-class_weights > {class_weights}')\n",
    "        return class_weights\n",
    "    \n",
    "    def _compute_target_class_weights(self, y):\n",
    "        class_values  = [0,1]\n",
    "        balanced_weights =  compute_class_weight('balanced', class_values, y)\n",
    "        class_weights = {'y': dict(zip(class_values, balanced_weights))}\n",
    "        print(f'-compute-class-weights > {class_weights}')\n",
    "        return class_weights\n",
    "        \n",
    "    def pretrain(self, x, y, z, epochs=10, verbose=0):\n",
    "        self._trainable_clf_net(True)\n",
    "        self._clf.fit(x.values, y.values, epochs=epochs, verbose=verbose)\n",
    "        self._trainable_clf_net(False)\n",
    "        self._trainable_adv_net(True)\n",
    "        class_weight_adv = self._compute_class_weights(z)\n",
    "        print(f'pretrain-class-weight-adv > {class_weight_adv}')\n",
    "        self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), class_weight=class_weight_adv, \n",
    "                      epochs=epochs, verbose=verbose)\n",
    "        \n",
    "    def fit(self, x, y, z, validation_data=None, T_iter=250, batch_size=128,\n",
    "            save_figs=False):\n",
    "        n_sensitive = z.shape[1]\n",
    "        if validation_data is not None:\n",
    "            x_val, y_val, z_val = validation_data\n",
    "\n",
    "        class_weight_adv = self._compute_class_weights(z)\n",
    "        class_weight_clf_w_adv = [{0:1., 1:1.}]+class_weight_adv\n",
    "        self._val_metrics = pd.DataFrame()\n",
    "        self._fairness_metrics = pd.DataFrame()  \n",
    "        for idx in range(T_iter):\n",
    "            if validation_data is not None:\n",
    "                y_pred = pd.Series(self._clf.predict(x_val).ravel(), index=y_val.index)\n",
    "                self._val_metrics.loc[idx, 'ROC AUC'] = roc_auc_score(y_val, y_pred)\n",
    "                self._val_metrics.loc[idx, 'Accuracy'] = (accuracy_score(y_val, (y_pred>0.5))*100)\n",
    "                for sensitive_attr in z_val.columns:\n",
    "                    self._fairness_metrics.loc[idx, sensitive_attr] = p_rule(y_pred,\n",
    "                                                                             z_val[sensitive_attr])\n",
    "                display.clear_output(wait=True)\n",
    "                plot_distributions(y_pred, z_val, idx+1, self._val_metrics.loc[idx],\n",
    "                                   self._fairness_metrics.loc[idx], \n",
    "                                   fname=f'output/{idx+1:08d}.png' if save_figs else None)\n",
    "                plt.show(plt.gcf())\n",
    "            \n",
    "            # train adverserial\n",
    "            self._trainable_clf_net(False)\n",
    "            self._trainable_adv_net(True)\n",
    "            self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), batch_size=batch_size, \n",
    "                          class_weight=class_weight_adv, epochs=1, verbose=0)\n",
    "            \n",
    "            # train classifier\n",
    "            self._trainable_clf_net(True)\n",
    "            self._trainable_adv_net(False)\n",
    "            indices = np.random.permutation(len(x))[:batch_size]\n",
    "            self._clf_w_adv.train_on_batch(x.values[indices], \n",
    "                                           [y.values[indices]]+np.hsplit(z.values[indices], n_sensitive),\n",
    "                                           class_weight=class_weight_clf_w_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-compute-target-class_weights > [{0: 4.901774397972116, 1: 0.5567952778577598}, {0: 1.5110373119749951, 1: 0.7472707950922616}]\npretrain-class-weight-adv > [{0: 4.901774397972116, 1: 0.5567952778577598}, {0: 1.5110373119749951, 1: 0.7472707950922616}]\n"
     ]
    }
   ],
   "source": [
    "# initialise FairClassifier\n",
    "clf = FairClassifier(n_features=X_train.shape[1], n_sensitive=Z_train.shape[1],\n",
    "                     lambdas=[130., 30.])\n",
    "\n",
    "# pre-train both adverserial and classifier networks\n",
    "clf.pretrain(X_train, y_train, Z_train, verbose=0, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supplied $\\lambda$ values, that tune fairness versus accuracy, are set to $\\lambda_{race}=130$ and $\\lambda_{sex}=30$. We heuristically found that these settings result in a balanced increase of the p%-rule values during training. Apparently, it is slightly harder to enforce fairness for the racial attributes than for sex.\n",
    "\n",
    "Now that both networks have been pre-trained, the adversarial training can start. We will simultaneously train both networks for 165 iterations while tracking the performance of the classifier on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "if create_gif:\n",
    "    !rm output/*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8892bd3406d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# adverserial train on train set and validate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m clf.fit(X_train, y_train, Z_train, \n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         T_iter=165, save_figs=create_gif)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# adverserial train on train set and validate on test set\n",
    "clf.fit(X_train, y_train, Z_train, \n",
    "        validation_data=(X_test, y_test, Z_test),\n",
    "        T_iter=5, save_figs=create_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "if create_gif:\n",
    "    !convert -loop 0 -delay 0 output/*.png -delay 500 output/00000165.png images/training.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><img src=\"images/training.gif\" alt=\"Architecture\" width=\"900\"/><br></center>\n",
    "\n",
    "The plots above show how the prediction distributions, the satisfied p%-rules and the prediction performance of our classifier evolve during adversarial training. At iteration #1, when training is just starting out, the predictions are very much the same as observed for the previously trained classifier: both high in bias and in prediction performance. As the training progresses, we see that the predictions are gradually become more and more fair while prediction performance is slightly declining. Finally, after 165 iterations of training, we see that the classifier satisfies the 80%-rule for both sensitive attributes while achieving a ROC AUC 0.85 and an accuracy of 82%. \n",
    "\n",
    "So, it seems that the training procedure works quite well. After sacrificing only 7% of prediction performance, we end up with a classifier that makes fair predictions when it comes to race and sex. A pretty decent result!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post we have shown that bringing fairness to predictive models is not as straight forward as 'just' removing some sensitive attributes from the training data. It requires clever techniques, like adverserial training, to correct for the often deeply biased training data and force our models into making fair predictions. And yes, making fair predictions comes at a cost: it will reduce the performance of your model (hopefully, only by a little as was the case in our example). However, in many cases this will be a relatively small price to pay for leaving behind the biased world of yesterday and predicting our way into a fairer tomorrow!\n",
    "\n",
    "*Shout-out to [Henk](https://godatadriven.com/players/henk-griffioen) who was so kind to review this work and provide his comments!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}