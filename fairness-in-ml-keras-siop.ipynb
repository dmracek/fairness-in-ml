{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sklearn: 0.19.1\npandas: 0.22.0\nkeras: 2.1.5\nThe nb_black extension is already loaded. To reload it, use:\n  %reload_ext nb_black\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 68;\n                var nbb_unformatted_code = \"# HIDE\\nimport pandas as pd\\nimport numpy as np\\nnp.random.seed(7)\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set(style=\\\"white\\\", palette=\\\"muted\\\", color_codes=True, context=\\\"talk\\\")\\nfrom IPython import display\\n%matplotlib inline\\n\\nimport sklearn as sk\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_recall_curve, f1_score, auc\\nfrom sklearn.utils.class_weight import compute_class_weight\\n\\nimport keras as ke\\nimport keras.backend as K\\nfrom keras.layers import Input, Dense, Dropout\\nfrom keras.models import load_model\\nfrom keras.models import Model\\n\\ncreate_gif = True\\n\\nfrom fairness.helpers import plot_distribution\\n\\nfrom matplotlib import pyplot\\n\\nprint(f\\\"sklearn: {sk.__version__}\\\")\\nprint(f\\\"pandas: {pd.__version__}\\\")\\nprint(f\\\"keras: {ke.__version__}\\\")\\n\\n%load_ext nb_black\";\n                var nbb_formatted_code = \"# HIDE\\nimport pandas as pd\\nimport numpy as np\\n\\nnp.random.seed(7)\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nsns.set(style=\\\"white\\\", palette=\\\"muted\\\", color_codes=True, context=\\\"talk\\\")\\nfrom IPython import display\\n\\n%matplotlib inline\\n\\nimport sklearn as sk\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import (\\n    accuracy_score,\\n    roc_auc_score,\\n    recall_score,\\n    precision_recall_curve,\\n    f1_score,\\n    auc,\\n)\\nfrom sklearn.utils.class_weight import compute_class_weight\\n\\nimport keras as ke\\nimport keras.backend as K\\nfrom keras.layers import Input, Dense, Dropout\\nfrom keras.models import load_model\\nfrom keras.models import Model\\n\\ncreate_gif = True\\n\\nfrom fairness.helpers import plot_distribution\\n\\nfrom matplotlib import pyplot\\n\\nprint(f\\\"sklearn: {sk.__version__}\\\")\\nprint(f\\\"pandas: {pd.__version__}\\\")\\nprint(f\\\"keras: {ke.__version__}\\\")\\n\\n%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# HIDE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_recall_curve, f1_score, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import keras as ke\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "\n",
    "create_gif = True\n",
    "\n",
    "from fairness.helpers import plot_distribution\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "print(f\"sklearn: {sk.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"keras: {ke.__version__}\")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards fairness in machine learning with adversarial networks \n",
    "\n",
    "## Introduction\n",
    "\n",
    "From credit ratings to housing allocation, machine learning models are increasingly used to automate 'everyday' decision making processes. With the growing impact on society, more and more concerns are being voiced about the loss of transparency, accountability and fairness of the algorithms making the decisions. We as data scientists need to step-up our game and look for ways to mitigate emergent discrimination in our models. We need to make sure that our predictions do not disproportionately hurt people with certain sensitive characteristics (e.g., gender, ethnicity).\n",
    "\n",
    "\n",
    "Luckily, [last year's NIPS conference](https://blog.godatadriven.com/gdd-nips-2017) showed that the field is actively investigating how to bring fairness to predictive models. The number of papers published on the topic is rapidly increasing, a signal that fairness is finally being taken seriously. This point is also nicely made in the cartoon below, which was taken from the excellent [CS 294: Fairness in Machine Learning](https://fairmlclass.github.io/) course taught at UC Berkley.\n",
    "\n",
    "<center><br><img src=\"images/fairness_plot.svg\" alt=\"Fairness\" width=\"500\"/><br></center>\n",
    "\n",
    "Some approaches focus on interpretability and transparency by allowing deeper interrogation of complex, black box models. Other approaches, make trained models more robust and fair in their predictions by taking the route of constraining and changing the optimization objective. We will consider the latter approach and show how adversarial networks can bring fairness to our predictive models. \n",
    "\n",
    "In this blog post, we will train a model for making income level predictions, analyse the fairness of its predictions and then show how adversarial training can be used to make it fair. The used approach is based on the 2017 NIPS paper [\"Learning to Pivot with Adversarial Networks\"](https://papers.nips.cc/paper/6699-learning-to-pivot-with-adversarial-networks) by Louppe et al. \n",
    "\n",
    "*Note that most of the code has been omitted, you can find the Jupyter notebook with all the code [here](https://github.com/equialgo/fairness-in-ml/blob/master/fairness-in-ml.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making income predictions\n",
    "\n",
    "Let's start by training a basic classifier that can predict whether or not a person's income is larger than 50K dollar a year. Too make these income level predictions we turn to the [adult UCI](https://archive.ics.uci.edu/ml/datasets/Adult) dataset, which is also referred to as \"Census Income\" dataset. It is not hard to imagine that financial institutions train models on similar data sets and use them to decide whether or not someone is eligible for a loan, or to set the height of an insurance premium.\n",
    "\n",
    "Before training a model, we first parse the data into three datasets: features, targets and sensitive attributes. The set of features $X$ contains the input attributes that the model uses for making the predictions, with attributes like age, education level and occupation. The targets $y$ contain the binary class labels that the model needs to predict.  These labels are $y\\in\\left\\{income>50K, income\\leq 50K\\right\\}$. Finally, the set of sensitive attributes $Z$ contains the attributes for which we want the prediction to fair. These are $z_{race}\\in\\left\\{black, white\\right\\}$ and $z_{sex}\\in\\left\\{male, female\\right\\}$. \n",
    "\n",
    "It is important to note that datasets are non-overlapping, so the sensitive attributes race and sex are **not** part of the features used for training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 211;\n                var nbb_unformatted_code = \"input_data = (pd.read_csv('data/siop_2020/siop_2020_train_filled.csv', sep=r'\\\\s*,\\\\s*', engine='python'))\\ndv_data = (pd.read_csv('data/siop_2020/siop_2020_dev_filled.csv', sep=r'\\\\s*,\\\\s*', engine='python'))\";\n                var nbb_formatted_code = \"input_data = pd.read_csv(\\n    \\\"data/siop_2020/siop_2020_train_filled.csv\\\", sep=r\\\"\\\\s*,\\\\s*\\\", engine=\\\"python\\\"\\n)\\ndv_data = pd.read_csv(\\n    \\\"data/siop_2020/siop_2020_dev_filled.csv\\\", sep=r\\\"\\\\s*,\\\\s*\\\", engine=\\\"python\\\"\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "input_data = (pd.read_csv('data/siop_2020/siop_2020_train_filled.csv', sep=r'\\s*,\\s*', engine='python'))\n",
    "dv_data = (pd.read_csv('data/siop_2020/siop_2020_dev_filled.csv', sep=r'\\s*,\\s*', engine='python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 212;\n                var nbb_unformatted_code = \"#input_data['High_Performer_Retained'] = np.where((input_data['High_Performer'] == 1) & (input_data['Retained'] == 1), 1, 0)\";\n                var nbb_formatted_code = \"# input_data['High_Performer_Retained'] = np.where((input_data['High_Performer'] == 1) & (input_data['Retained'] == 1), 1, 0)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "#input_data['High_Performer_Retained'] = np.where((input_data['High_Performer'] == 1) & (input_data['Retained'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 213;\n                var nbb_unformatted_code = \"# sensitive attributes; we identify 'race' and 'sex' as sensitive attributes\\nsensitive_attribs = ['Protected_Group']\\n\\nZ = (input_data.loc[:, sensitive_attribs])\\n\\n# targets; 1 when someone makes over 50k , otherwise 0\\ny = (input_data['Retained'])\\n#y = (input_data['High_Performer'])\\n\\n#y = (input_data.loc[:, ['High_Performer', 'Retained']])\\n#y['High_Performer_Retained'] = np.where((y['High_Performer'] == 1) & (y['Retained'] == 1), 1, 0)\\n#y = (y.loc[:, 'High_Performer_Retained'])\\n\\n# features; note that the 'target' and sentive attribute columns are dropped\\nX = (input_data\\n     .drop(columns=['UNIQUE_ID', 'Overall_Rating', 'Technical_Skills', 'Teamwork',\\n                    'Customer_Service', 'Hire_Again', 'High_Performer', 'Protected_Group',\\n                    'Retained', 'split']))\";\n                var nbb_formatted_code = \"# sensitive attributes; we identify 'race' and 'sex' as sensitive attributes\\nsensitive_attribs = [\\\"Protected_Group\\\"]\\n\\nZ = input_data.loc[:, sensitive_attribs]\\n\\n# targets; 1 when someone makes over 50k , otherwise 0\\ny = input_data[\\\"Retained\\\"]\\n# y = (input_data['High_Performer'])\\n\\n# y = (input_data.loc[:, ['High_Performer', 'Retained']])\\n# y['High_Performer_Retained'] = np.where((y['High_Performer'] == 1) & (y['Retained'] == 1), 1, 0)\\n# y = (y.loc[:, 'High_Performer_Retained'])\\n\\n# features; note that the 'target' and sentive attribute columns are dropped\\nX = input_data.drop(\\n    columns=[\\n        \\\"UNIQUE_ID\\\",\\n        \\\"Overall_Rating\\\",\\n        \\\"Technical_Skills\\\",\\n        \\\"Teamwork\\\",\\n        \\\"Customer_Service\\\",\\n        \\\"Hire_Again\\\",\\n        \\\"High_Performer\\\",\\n        \\\"Protected_Group\\\",\\n        \\\"Retained\\\",\\n        \\\"split\\\",\\n    ]\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# sensitive attributes; we identify 'race' and 'sex' as sensitive attributes\n",
    "sensitive_attribs = ['Protected_Group']\n",
    "\n",
    "Z = (input_data.loc[:, sensitive_attribs])\n",
    "\n",
    "# targets; 1 when someone makes over 50k , otherwise 0\n",
    "y = (input_data['Retained'])\n",
    "#y = (input_data['High_Performer'])\n",
    "\n",
    "#y = (input_data.loc[:, ['High_Performer', 'Retained']])\n",
    "#y['High_Performer_Retained'] = np.where((y['High_Performer'] == 1) & (y['Retained'] == 1), 1, 0)\n",
    "#y = (y.loc[:, 'High_Performer_Retained'])\n",
    "\n",
    "# features; note that the 'target' and sentive attribute columns are dropped\n",
    "X = (input_data\n",
    "     .drop(columns=['UNIQUE_ID', 'Overall_Rating', 'Technical_Skills', 'Teamwork',\n",
    "                    'Customer_Service', 'Hire_Again', 'High_Performer', 'Protected_Group',\n",
    "                    'Retained', 'split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 214;\n                var nbb_unformatted_code = \"list_category_features = [elem for elem in (list(X)) if not 'Time' in elem]\\n\\nfor f in list_category_features:\\n    X[f + '_Dummy'] = X[f].astype(str)\\n\\nlist_dummy_features = [elem for elem in (list(X)) if elem.endswith('_Dummy')]\\ndf_get_dummies = X[list_dummy_features]\\n\\ndf_dummies = pd.get_dummies(df_get_dummies, drop_first=True)\\nX = X.join(df_dummies)\\nX = X.loc[:, ~X.columns.str.endswith('Dummy')]\";\n                var nbb_formatted_code = \"list_category_features = [elem for elem in (list(X)) if not \\\"Time\\\" in elem]\\n\\nfor f in list_category_features:\\n    X[f + \\\"_Dummy\\\"] = X[f].astype(str)\\n\\nlist_dummy_features = [elem for elem in (list(X)) if elem.endswith(\\\"_Dummy\\\")]\\ndf_get_dummies = X[list_dummy_features]\\n\\ndf_dummies = pd.get_dummies(df_get_dummies, drop_first=True)\\nX = X.join(df_dummies)\\nX = X.loc[:, ~X.columns.str.endswith(\\\"Dummy\\\")]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "list_category_features = [elem for elem in (list(X)) if not 'Time' in elem]\n",
    "\n",
    "for f in list_category_features:\n",
    "    X[f + '_Dummy'] = X[f].astype(str)\n",
    "\n",
    "list_dummy_features = [elem for elem in (list(X)) if elem.endswith('_Dummy')]\n",
    "df_get_dummies = X[list_dummy_features]\n",
    "\n",
    "df_dummies = pd.get_dummies(df_get_dummies, drop_first=True)\n",
    "X = X.join(df_dummies)\n",
    "X = X.loc[:, ~X.columns.str.endswith('Dummy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "602\n661\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 215;\n                var nbb_unformatted_code = \"X_dev = (dv_data\\n    .drop(columns=['UNIQUE_ID', 'split']))\\n\\nfor f in list_category_features:\\n    X_dev[f + '_Dummy'] = X_dev[f].astype(str)\\n    \\ndf_dev_get_dummies = X_dev[list_dummy_features]\\ndf_dev_dummies = pd.get_dummies(df_dev_get_dummies, drop_first=False)\\n\\ndf_dev_dummies.columns=df_dev_dummies.columns.str.replace('\\\\.0','')\\n\\nX_dev = X_dev.join(df_dev_dummies)\\nX_dev = X_dev.loc[:, ~X_dev.columns.str.endswith('Dummy')]\\n\\nprint(len(list(X)))\\nprint(len(list(X_dev)))\";\n                var nbb_formatted_code = \"X_dev = dv_data.drop(columns=[\\\"UNIQUE_ID\\\", \\\"split\\\"])\\n\\nfor f in list_category_features:\\n    X_dev[f + \\\"_Dummy\\\"] = X_dev[f].astype(str)\\n\\ndf_dev_get_dummies = X_dev[list_dummy_features]\\ndf_dev_dummies = pd.get_dummies(df_dev_get_dummies, drop_first=False)\\n\\ndf_dev_dummies.columns = df_dev_dummies.columns.str.replace(\\\"\\\\.0\\\", \\\"\\\")\\n\\nX_dev = X_dev.join(df_dev_dummies)\\nX_dev = X_dev.loc[:, ~X_dev.columns.str.endswith(\\\"Dummy\\\")]\\n\\nprint(len(list(X)))\\nprint(len(list(X_dev)))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "X_dev = (dv_data\n",
    "    .drop(columns=['UNIQUE_ID', 'split']))\n",
    "\n",
    "for f in list_category_features:\n",
    "    X_dev[f + '_Dummy'] = X_dev[f].astype(str)\n",
    "    \n",
    "df_dev_get_dummies = X_dev[list_dummy_features]\n",
    "df_dev_dummies = pd.get_dummies(df_dev_get_dummies, drop_first=False)\n",
    "\n",
    "df_dev_dummies.columns=df_dev_dummies.columns.str.replace('\\.0','')\n",
    "\n",
    "X_dev = X_dev.join(df_dev_dummies)\n",
    "X_dev = X_dev.loc[:, ~X_dev.columns.str.endswith('Dummy')]\n",
    "\n",
    "print(len(list(X)))\n",
    "print(len(list(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 216;\n                var nbb_unformatted_code = \"\\\"\\\"\\\"\\nDesc:\\n Using set() to find the difference between two lists in Python\\n\\\"\\\"\\\"\\n\\ndef list_diff(list1, list2): \\n    return (list(set(list1) - set(list2)))\\n\\na = list(X)\\nb = list(X_dev)\\n\\nmisc_columns = list_diff(a, b)\\n\\nfor mc in misc_columns:\\n    X_dev[mc] = 0\\n\\nX_dev = X_dev[(list(X))]\";\n                var nbb_formatted_code = \"\\\"\\\"\\\"\\nDesc:\\n Using set() to find the difference between two lists in Python\\n\\\"\\\"\\\"\\n\\n\\ndef list_diff(list1, list2):\\n    return list(set(list1) - set(list2))\\n\\n\\na = list(X)\\nb = list(X_dev)\\n\\nmisc_columns = list_diff(a, b)\\n\\nfor mc in misc_columns:\\n    X_dev[mc] = 0\\n\\nX_dev = X_dev[(list(X))]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "\"\"\"\n",
    "Desc:\n",
    " Using set() to find the difference between two lists in Python\n",
    "\"\"\"\n",
    "\n",
    "def list_diff(list1, list2): \n",
    "    return (list(set(list1) - set(list2)))\n",
    "\n",
    "a = list(X)\n",
    "b = list(X_dev)\n",
    "\n",
    "misc_columns = list_diff(a, b)\n",
    "\n",
    "for mc in misc_columns:\n",
    "    X_dev[mc] = 0\n",
    "\n",
    "X_dev = X_dev[(list(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 217;\n                var nbb_unformatted_code = \"n_features = X.shape[1]\\nn_sensitive = Z.shape[1]\\n\\n# split into train/test set\\n(X_train, X_test, y_train, y_test,\\n Z_train, Z_test) = train_test_split(X, y, Z, test_size=0.5,\\n                                     stratify=input_data[['Retained', 'Protected_Group']], random_state=7)\\n\\n# standardize the data\\nscaler = StandardScaler().fit(X_train)\\nscale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), \\n                                           columns=df.columns, index=df.index)\\nX_train = X_train.pipe(scale_df, scaler) \\nX_test = X_test.pipe(scale_df, scaler)\\nX_dev = X_dev.pipe(scale_df, scaler)\";\n                var nbb_formatted_code = \"n_features = X.shape[1]\\nn_sensitive = Z.shape[1]\\n\\n# split into train/test set\\n(X_train, X_test, y_train, y_test, Z_train, Z_test) = train_test_split(\\n    X,\\n    y,\\n    Z,\\n    test_size=0.5,\\n    stratify=input_data[[\\\"Retained\\\", \\\"Protected_Group\\\"]],\\n    random_state=7,\\n)\\n\\n# standardize the data\\nscaler = StandardScaler().fit(X_train)\\nscale_df = lambda df, scaler: pd.DataFrame(\\n    scaler.transform(df), columns=df.columns, index=df.index\\n)\\nX_train = X_train.pipe(scale_df, scaler)\\nX_test = X_test.pipe(scale_df, scaler)\\nX_dev = X_dev.pipe(scale_df, scaler)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "n_features = X.shape[1]\n",
    "n_sensitive = Z.shape[1]\n",
    "\n",
    "# split into train/test set\n",
    "(X_train, X_test, y_train, y_test,\n",
    " Z_train, Z_test) = train_test_split(X, y, Z, test_size=0.5,\n",
    "                                     stratify=input_data[['Retained', 'Protected_Group']], random_state=7)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), \n",
    "                                           columns=df.columns, index=df.index)\n",
    "X_train = X_train.pipe(scale_df, scaler) \n",
    "X_test = X_test.pipe(scale_df, scaler)\n",
    "X_dev = X_dev.pipe(scale_df, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 218;\n                var nbb_unformatted_code = \"Z_train['sex'] = Z_train['Protected_Group']\\nZ_train['race'] = Z_train['Protected_Group']\\n\\nZ_test['sex'] = Z_test['Protected_Group']\\nZ_test['race'] = Z_test['Protected_Group']\\n\\nZ_train = Z_train.loc[:, ['race', 'sex']]\\nZ_test = Z_test.loc[:, ['race', 'sex']]\\n\\n#print(X_test.mean)\\n#print(y_test.mean)\\n#print(Z_test.mean)\";\n                var nbb_formatted_code = \"Z_train[\\\"sex\\\"] = Z_train[\\\"Protected_Group\\\"]\\nZ_train[\\\"race\\\"] = Z_train[\\\"Protected_Group\\\"]\\n\\nZ_test[\\\"sex\\\"] = Z_test[\\\"Protected_Group\\\"]\\nZ_test[\\\"race\\\"] = Z_test[\\\"Protected_Group\\\"]\\n\\nZ_train = Z_train.loc[:, [\\\"race\\\", \\\"sex\\\"]]\\nZ_test = Z_test.loc[:, [\\\"race\\\", \\\"sex\\\"]]\\n\\n# print(X_test.mean)\\n# print(y_test.mean)\\n# print(Z_test.mean)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "Z_train['sex'] = Z_train['Protected_Group']\n",
    "Z_train['race'] = Z_train['Protected_Group']\n",
    "\n",
    "Z_test['sex'] = Z_test['Protected_Group']\n",
    "Z_test['race'] = Z_test['Protected_Group']\n",
    "\n",
    "Z_train = Z_train.loc[:, ['race', 'sex']]\n",
    "Z_test = Z_test.loc[:, ['race', 'sex']]\n",
    "\n",
    "#print(X_test.mean)\n",
    "#print(y_test.mean)\n",
    "#print(Z_test.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains the information of just over 30K people. Next, we split the data into train and test sets, where the split is 50/50, and scale the features $X$ using standard scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our basic income level predictor. We use Keras to fit a simple three-layer network with ReLU activations and dropout on the training data. The output of the network is a single node with sigmoid activation, so it predicts \"the probability that this person's income is larger than 50K\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 219;\n                var nbb_unformatted_code = \"#def nn_classifier(n_features):\\n#    inputs = Input(shape=(n_features,))\\n#    dense1 = Dense(32, activation='relu')(inputs)\\n#    dropout1 = Dropout(0.2)(dense1)\\n#    dense2 = Dense(32, activation='relu')(dropout1)\\n#    dropout2 = Dropout(0.2)(dense2)\\n#    dense3 = Dense(32, activation=\\\"relu\\\")(dropout2)\\n#    dropout3 = Dropout(0.2)(dense3)\\n#    outputs = Dense(1, activation='sigmoid')(dropout3)\\n#    model = Model(inputs=[inputs], outputs=[outputs])\\n#    model.compile(loss='binary_crossentropy', optimizer='adam')\\n#    return model\\n#\\n## initialise NeuralNet Classifier\\n#clf = nn_classifier(n_features=X_train.shape[1])\\n#\\n## train on train set\\n#history = clf.fit(X_train.values, y_train.values, epochs=20, verbose=0)\";\n                var nbb_formatted_code = \"# def nn_classifier(n_features):\\n#    inputs = Input(shape=(n_features,))\\n#    dense1 = Dense(32, activation='relu')(inputs)\\n#    dropout1 = Dropout(0.2)(dense1)\\n#    dense2 = Dense(32, activation='relu')(dropout1)\\n#    dropout2 = Dropout(0.2)(dense2)\\n#    dense3 = Dense(32, activation=\\\"relu\\\")(dropout2)\\n#    dropout3 = Dropout(0.2)(dense3)\\n#    outputs = Dense(1, activation='sigmoid')(dropout3)\\n#    model = Model(inputs=[inputs], outputs=[outputs])\\n#    model.compile(loss='binary_crossentropy', optimizer='adam')\\n#    return model\\n#\\n## initialise NeuralNet Classifier\\n# clf = nn_classifier(n_features=X_train.shape[1])\\n#\\n## train on train set\\n# history = clf.fit(X_train.values, y_train.values, epochs=20, verbose=0)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "#def nn_classifier(n_features):\n",
    "#    inputs = Input(shape=(n_features,))\n",
    "#    dense1 = Dense(32, activation='relu')(inputs)\n",
    "#    dropout1 = Dropout(0.2)(dense1)\n",
    "#    dense2 = Dense(32, activation='relu')(dropout1)\n",
    "#    dropout2 = Dropout(0.2)(dense2)\n",
    "#    dense3 = Dense(32, activation=\"relu\")(dropout2)\n",
    "#    dropout3 = Dropout(0.2)(dense3)\n",
    "#    outputs = Dense(1, activation='sigmoid')(dropout3)\n",
    "#    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "#    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "#    return model\n",
    "#\n",
    "## initialise NeuralNet Classifier\n",
    "#clf = nn_classifier(n_features=X_train.shape[1])\n",
    "#\n",
    "## train on train set\n",
    "#history = clf.fit(X_train.values, y_train.values, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use this classifier to make income level predictions on the test data. We determine the model performance by computing the [Area Under the Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) and the accuracy score using test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 220;\n                var nbb_unformatted_code = \"# predict on test set\\n#y_pred = pd.Series(clf.predict(X_test).ravel(), index=y_test.index)\\n#print(f\\\"ROC AUC: {roc_auc_score(y_test, y_pred):.2f}\\\")\\n#print(f\\\"Accuracy: {100*accuracy_score(y_test, (y_pred>0.5)):.1f}%\\\")\";\n                var nbb_formatted_code = \"# predict on test set\\n# y_pred = pd.Series(clf.predict(X_test).ravel(), index=y_test.index)\\n# print(f\\\"ROC AUC: {roc_auc_score(y_test, y_pred):.2f}\\\")\\n# print(f\\\"Accuracy: {100*accuracy_score(y_test, (y_pred>0.5)):.1f}%\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# predict on test set\n",
    "#y_pred = pd.Series(clf.predict(X_test).ravel(), index=y_test.index)\n",
    "#print(f\"ROC AUC: {roc_auc_score(y_test, y_pred):.2f}\")\n",
    "#print(f\"Accuracy: {100*accuracy_score(y_test, (y_pred>0.5)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 221;\n                var nbb_unformatted_code = \"# HIDE\\n\\ndef plot_distributions(y, Z, iteration=None, val_metrics=None, p_rules=None, fname=None):\\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\\n    legend={'race': ['black','white'],\\n            'sex': ['female','male']}\\n    for idx, attr in enumerate(Z.columns):\\n        for attr_val in [0, 1]:\\n            ax = sns.distplot(y[Z[attr] == attr_val], hist=False, \\n                              kde_kws={'shade': True,},\\n                              label='{}'.format(legend[attr][attr_val]), \\n                              ax=axes[idx])\\n        ax.set_xlim(0,1)\\n        ax.set_ylim(0,7)\\n        ax.set_yticks([])\\n        ax.set_title(\\\"sensitive attibute: {}\\\".format(attr))\\n        if idx == 0:\\n            ax.set_ylabel('prediction distribution')\\n        ax.set_xlabel(r'$P({{income>50K}}|z_{{{}}})$'.format(attr))\\n    if iteration:\\n        fig.text(1.0, 0.9, f\\\"Training iteration #{iteration}\\\", fontsize='16')\\n    if val_metrics is not None:\\n        fig.text(1.0, 0.65, '\\\\n'.join([\\\"Prediction performance:\\\",\\n                                       f\\\"- ROC AUC: {val_metrics['roc_auc_d']:.2f}\\\",\\n                                       f\\\"- Recall: {val_metrics['recall_d']:.3f}\\\",\\n                                       f\\\"- Accuracy: {val_metrics['accuracy_d']:.1f}\\\"]),\\n                 fontsize='16')\\n    if p_rules is not None:\\n        fig.text(1.0, 0.4, '\\\\n'.join([\\\"Satisfied p%-rules:\\\"] +\\n                                     [f\\\"- {attr}: {p_rules[attr]:.0f}%-rule\\\" \\n                                      for attr in p_rules.keys()]), \\n                 fontsize='16')\\n    fig.tight_layout()\\n    if fname is not None:\\n        plt.savefig(fname, bbox_inches='tight')\\n    return fig\\n\\n\\n\\nfrom sklearn.metrics import confusion_matrix\\n\\ndef get_sensitivity(y_true, y_pred):\\n\\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\\n    sensitivity  = tp / (tp + fn)\\n    return sensitivity\";\n                var nbb_formatted_code = \"# HIDE\\n\\n\\ndef plot_distributions(\\n    y, Z, iteration=None, val_metrics=None, p_rules=None, fname=None\\n):\\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\\n    legend = {\\\"race\\\": [\\\"black\\\", \\\"white\\\"], \\\"sex\\\": [\\\"female\\\", \\\"male\\\"]}\\n    for idx, attr in enumerate(Z.columns):\\n        for attr_val in [0, 1]:\\n            ax = sns.distplot(\\n                y[Z[attr] == attr_val],\\n                hist=False,\\n                kde_kws={\\n                    \\\"shade\\\": True,\\n                },\\n                label=\\\"{}\\\".format(legend[attr][attr_val]),\\n                ax=axes[idx],\\n            )\\n        ax.set_xlim(0, 1)\\n        ax.set_ylim(0, 7)\\n        ax.set_yticks([])\\n        ax.set_title(\\\"sensitive attibute: {}\\\".format(attr))\\n        if idx == 0:\\n            ax.set_ylabel(\\\"prediction distribution\\\")\\n        ax.set_xlabel(r\\\"$P({{income>50K}}|z_{{{}}})$\\\".format(attr))\\n    if iteration:\\n        fig.text(1.0, 0.9, f\\\"Training iteration #{iteration}\\\", fontsize=\\\"16\\\")\\n    if val_metrics is not None:\\n        fig.text(\\n            1.0,\\n            0.65,\\n            \\\"\\\\n\\\".join(\\n                [\\n                    \\\"Prediction performance:\\\",\\n                    f\\\"- ROC AUC: {val_metrics['roc_auc_d']:.2f}\\\",\\n                    f\\\"- Recall: {val_metrics['recall_d']:.3f}\\\",\\n                    f\\\"- Accuracy: {val_metrics['accuracy_d']:.1f}\\\",\\n                ]\\n            ),\\n            fontsize=\\\"16\\\",\\n        )\\n    if p_rules is not None:\\n        fig.text(\\n            1.0,\\n            0.4,\\n            \\\"\\\\n\\\".join(\\n                [\\\"Satisfied p%-rules:\\\"]\\n                + [f\\\"- {attr}: {p_rules[attr]:.0f}%-rule\\\" for attr in p_rules.keys()]\\n            ),\\n            fontsize=\\\"16\\\",\\n        )\\n    fig.tight_layout()\\n    if fname is not None:\\n        plt.savefig(fname, bbox_inches=\\\"tight\\\")\\n    return fig\\n\\n\\nfrom sklearn.metrics import confusion_matrix\\n\\n\\ndef get_sensitivity(y_true, y_pred):\\n\\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\\n    sensitivity = tp / (tp + fn)\\n    return sensitivity\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# HIDE\n",
    "\n",
    "def plot_distributions(y, Z, iteration=None, val_metrics=None, p_rules=None, fname=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "    legend={'race': ['black','white'],\n",
    "            'sex': ['female','male']}\n",
    "    for idx, attr in enumerate(Z.columns):\n",
    "        for attr_val in [0, 1]:\n",
    "            ax = sns.distplot(y[Z[attr] == attr_val], hist=False, \n",
    "                              kde_kws={'shade': True,},\n",
    "                              label='{}'.format(legend[attr][attr_val]), \n",
    "                              ax=axes[idx])\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,7)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"sensitive attibute: {}\".format(attr))\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('prediction distribution')\n",
    "        ax.set_xlabel(r'$P({{income>50K}}|z_{{{}}})$'.format(attr))\n",
    "    if iteration:\n",
    "        fig.text(1.0, 0.9, f\"Training iteration #{iteration}\", fontsize='16')\n",
    "    if val_metrics is not None:\n",
    "        fig.text(1.0, 0.65, '\\n'.join([\"Prediction performance:\",\n",
    "                                       f\"- ROC AUC: {val_metrics['roc_auc_d']:.2f}\",\n",
    "                                       f\"- Recall: {val_metrics['recall_d']:.3f}\",\n",
    "                                       f\"- Accuracy: {val_metrics['accuracy_d']:.1f}\"]),\n",
    "                 fontsize='16')\n",
    "    if p_rules is not None:\n",
    "        fig.text(1.0, 0.4, '\\n'.join([\"Satisfied p%-rules:\"] +\n",
    "                                     [f\"- {attr}: {p_rules[attr]:.0f}%-rule\" \n",
    "                                      for attr in p_rules.keys()]), \n",
    "                 fontsize='16')\n",
    "    fig.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_sensitivity(y_true, y_pred):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity  = tp / (tp + fn)\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a ROC AUC larger than 0.9 and a prediction accuracy of 85% we can say that our basic classifier performs pretty well! However, if it is also fair in its predictions, that remains to be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative model fairness\n",
    "\n",
    "We start the investigation into the fairness of our classifier by analysing the predictions it made on the test set. The plots in the figure below show the distributions of the predicted $P(income>50K)$ given the sensitive attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 222;\n                var nbb_unformatted_code = \"#fig = plot_distributions(y_pred, Z_test, fname='images/biased_training.png')\";\n                var nbb_formatted_code = \"# fig = plot_distributions(y_pred, Z_test, fname='images/biased_training.png')\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "#fig = plot_distributions(y_pred, Z_test, fname='images/biased_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><img src=\"images/biased_training.png\" alt=\"Architecture\" width=\"750\"/><br></center>\n",
    "\n",
    "The figure shows for both race (=left plot) and sex (=right plot) the blue prediction distributions have a large peak at the low end of the probability range. This means that when a person is $black$ and/or $female$ there is a much higher probability of the classifier predicting an income below 50K compared to when someone is $white$ and/or $male$.\n",
    "\n",
    "So, the results of the qualitative analysis are quite clear: the predictions are definitely not fair when considered in the context of race and sex. When it comes to assigning the high-income levels, our model favours the usual suspects: white males."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative model fairness\n",
    "\n",
    "In order to get a 'quantitative' measure of how fair our classifier is, we take inspiration from the U.S. Equal Employment Opportunity Commission (EEOC). They use the so-called [80% rule](https://en.wikipedia.org/wiki/Disparate_impact#The_80%_rule) to quantify the disparate impact on a group of people of a protected characteristic. Zafar et al. show in their paper [\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf) how a more generic version of this rule, called the p%-rule, can be used to quantify fairness of a classifier. This rule is defined as follows:\n",
    "\n",
    "> A classifier that makes a binary class prediction $\\hat{y} \\in \\left\\{0,1 \\right\\}$ given a binary sensitive attribute $z\\in \\left\\{0,1 \\right\\}$ satisfies the p%-rule\n",
    "if the following inequality holds:\n",
    ">\n",
    ">$$\\min\\left(\\frac{P(\\hat{y}=1|z=1)}{P(\\hat{y}=1|z=0)}, \\frac{P(\\hat{y}=1|z=0)}{P(\\hat{y}=1|z=1)}\\right)\\geq\\frac{p}{100}$$\n",
    "\n",
    "The rule states that the ratio between the probability of a positive outcome given the sensitive attribute being true and the same probability given the sensitive attribute being false is no less than p:100. So, when a classifier is completely fair it will satisfy a 100%-rule. In contrast, when it is completely unfair it satisfies a %0-rule. \n",
    "\n",
    "In determining the fairness our or classifier we will follow the EEOC and say that a model is fair when it satisfies at least an 80%-rule. So, let's compute the p%-rules for the classifier and put a number on its fairness. Note that we will threshold our classifier at 0.5 to make its prediction it binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 223;\n                var nbb_unformatted_code = \"# HIDE\\ndef p_rule(y_pred, z_values, threshold=0.5):\\n    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\\n    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\\n    odds = y_z_1.mean() / y_z_0.mean()\\n    return np.min([odds, 1/odds]) * 100\\n\\ndef sr_rule(y_pred, threshold=0.5):\\n    BT = threshold \\n    df = pd.DataFrame(y_pred, columns=['raw'])\\n    df['raw_bin'] = df['raw'].apply(lambda x: 1 if x >= BT else 0)\\n    return df['raw_bin'].mean()\\n\\ndef return_metric(y_pred, y_val, z_val):\\n    thresholds = np.arange(0.0, 1.0, 0.001)\\n    rscore = np.zeros(shape=(len(thresholds)))\\n    pscore = np.zeros(shape=(len(thresholds)))\\n    srscore = np.zeros(shape=(len(thresholds)))\\n\\n    # Fit the model\\n    for index, elem in enumerate(thresholds):\\n        # Corrected probabilities\\n        y_pred_prob = (y_pred > elem).astype('int')\\n        # Calculate the f-score\\n        rscore[index] = recall_score(y_val, y_pred_prob)\\n        pscore[index] = (p_rule(y_pred_prob, z_val['race'], threshold=elem)) / 100\\n        srscore[index] = 1 - abs(sr_rule(y_pred_prob, threshold=elem)-.50)\\n\\n    rscore = np.nan_to_num(rscore)\\n    pscore = np.nan_to_num(pscore)\\n    srscore = np.nan_to_num(srscore)\\n\\n    super_threshold_indices = srscore < .98\\n    srscore[super_threshold_indices] = 0\\n\\n    metric = rscore * pscore * srscore\\n\\n    index = np.argmax(metric)\\n    thresholdOpt = round(thresholds[index], ndigits = 4)\\n    metricOpt = round(metric[index], ndigits = 4)\\n    rscoreOpt = round(rscore[index], ndigits = 4)\\n    pscoreOpt = round(pscore[index], ndigits = 4)\\n    srscoreOpt = round(srscore[index], ndigits = 4)\\n\\n    print('Best Threshold: {} with Metric: {}'.format(thresholdOpt, metricOpt))\\n    print('Best Threshold: {} with Recall: {}'.format(thresholdOpt, rscoreOpt))\\n    print('Best Threshold: {} with Pvalue: {}'.format(thresholdOpt, pscoreOpt))\\n    print('Best Threshold: {} with SR: {}'.format(thresholdOpt, srscoreOpt))\\n\\n    return thresholdOpt, metricOpt, rscoreOpt, pscoreOpt, srscoreOpt\";\n                var nbb_formatted_code = \"# HIDE\\ndef p_rule(y_pred, z_values, threshold=0.5):\\n    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\\n    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\\n    odds = y_z_1.mean() / y_z_0.mean()\\n    return np.min([odds, 1 / odds]) * 100\\n\\n\\ndef sr_rule(y_pred, threshold=0.5):\\n    BT = threshold\\n    df = pd.DataFrame(y_pred, columns=[\\\"raw\\\"])\\n    df[\\\"raw_bin\\\"] = df[\\\"raw\\\"].apply(lambda x: 1 if x >= BT else 0)\\n    return df[\\\"raw_bin\\\"].mean()\\n\\n\\ndef return_metric(y_pred, y_val, z_val):\\n    thresholds = np.arange(0.0, 1.0, 0.001)\\n    rscore = np.zeros(shape=(len(thresholds)))\\n    pscore = np.zeros(shape=(len(thresholds)))\\n    srscore = np.zeros(shape=(len(thresholds)))\\n\\n    # Fit the model\\n    for index, elem in enumerate(thresholds):\\n        # Corrected probabilities\\n        y_pred_prob = (y_pred > elem).astype(\\\"int\\\")\\n        # Calculate the f-score\\n        rscore[index] = recall_score(y_val, y_pred_prob)\\n        pscore[index] = (p_rule(y_pred_prob, z_val[\\\"race\\\"], threshold=elem)) / 100\\n        srscore[index] = 1 - abs(sr_rule(y_pred_prob, threshold=elem) - 0.50)\\n\\n    rscore = np.nan_to_num(rscore)\\n    pscore = np.nan_to_num(pscore)\\n    srscore = np.nan_to_num(srscore)\\n\\n    super_threshold_indices = srscore < 0.98\\n    srscore[super_threshold_indices] = 0\\n\\n    metric = rscore * pscore * srscore\\n\\n    index = np.argmax(metric)\\n    thresholdOpt = round(thresholds[index], ndigits=4)\\n    metricOpt = round(metric[index], ndigits=4)\\n    rscoreOpt = round(rscore[index], ndigits=4)\\n    pscoreOpt = round(pscore[index], ndigits=4)\\n    srscoreOpt = round(srscore[index], ndigits=4)\\n\\n    print(\\\"Best Threshold: {} with Metric: {}\\\".format(thresholdOpt, metricOpt))\\n    print(\\\"Best Threshold: {} with Recall: {}\\\".format(thresholdOpt, rscoreOpt))\\n    print(\\\"Best Threshold: {} with Pvalue: {}\\\".format(thresholdOpt, pscoreOpt))\\n    print(\\\"Best Threshold: {} with SR: {}\\\".format(thresholdOpt, srscoreOpt))\\n\\n    return thresholdOpt, metricOpt, rscoreOpt, pscoreOpt, srscoreOpt\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# HIDE\n",
    "def p_rule(y_pred, z_values, threshold=0.5):\n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = y_z_1.mean() / y_z_0.mean()\n",
    "    return np.min([odds, 1/odds]) * 100\n",
    "\n",
    "def sr_rule(y_pred, threshold=0.5):\n",
    "    BT = threshold \n",
    "    df = pd.DataFrame(y_pred, columns=['raw'])\n",
    "    df['raw_bin'] = df['raw'].apply(lambda x: 1 if x >= BT else 0)\n",
    "    return df['raw_bin'].mean()\n",
    "\n",
    "def return_metric(y_pred, y_val, z_val):\n",
    "    thresholds = np.arange(0.0, 1.0, 0.001)\n",
    "    rscore = np.zeros(shape=(len(thresholds)))\n",
    "    pscore = np.zeros(shape=(len(thresholds)))\n",
    "    srscore = np.zeros(shape=(len(thresholds)))\n",
    "\n",
    "    # Fit the model\n",
    "    for index, elem in enumerate(thresholds):\n",
    "        # Corrected probabilities\n",
    "        y_pred_prob = (y_pred > elem).astype('int')\n",
    "        # Calculate the f-score\n",
    "        rscore[index] = recall_score(y_val, y_pred_prob)\n",
    "        pscore[index] = (p_rule(y_pred_prob, z_val['race'], threshold=elem)) / 100\n",
    "        srscore[index] = 1 - abs(sr_rule(y_pred_prob, threshold=elem)-.50)\n",
    "\n",
    "    rscore = np.nan_to_num(rscore)\n",
    "    pscore = np.nan_to_num(pscore)\n",
    "    srscore = np.nan_to_num(srscore)\n",
    "\n",
    "    super_threshold_indices = srscore < .98\n",
    "    srscore[super_threshold_indices] = 0\n",
    "\n",
    "    metric = rscore * pscore * srscore\n",
    "\n",
    "    index = np.argmax(metric)\n",
    "    thresholdOpt = round(thresholds[index], ndigits = 4)\n",
    "    metricOpt = round(metric[index], ndigits = 4)\n",
    "    rscoreOpt = round(rscore[index], ndigits = 4)\n",
    "    pscoreOpt = round(pscore[index], ndigits = 4)\n",
    "    srscoreOpt = round(srscore[index], ndigits = 4)\n",
    "\n",
    "    print('Best Threshold: {} with Metric: {}'.format(thresholdOpt, metricOpt))\n",
    "    print('Best Threshold: {} with Recall: {}'.format(thresholdOpt, rscoreOpt))\n",
    "    print('Best Threshold: {} with Pvalue: {}'.format(thresholdOpt, pscoreOpt))\n",
    "    print('Best Threshold: {} with SR: {}'.format(thresholdOpt, srscoreOpt))\n",
    "\n",
    "    return thresholdOpt, metricOpt, rscoreOpt, pscoreOpt, srscoreOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 224;\n                var nbb_unformatted_code = \"#print(\\\"The classifier satisfies the following %p-rules:\\\")\\n#print(f\\\"\\\\tgiven attribute race; {p_rule(y_pred, Z_test['race']):.0f}%-rule\\\")\\n#print(f\\\"\\\\tgiven attribute sex; {p_rule(y_pred, Z_test['sex']):.0f}%-rule\\\")\";\n                var nbb_formatted_code = \"# print(\\\"The classifier satisfies the following %p-rules:\\\")\\n# print(f\\\"\\\\tgiven attribute race; {p_rule(y_pred, Z_test['race']):.0f}%-rule\\\")\\n# print(f\\\"\\\\tgiven attribute sex; {p_rule(y_pred, Z_test['sex']):.0f}%-rule\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "#print(\"The classifier satisfies the following %p-rules:\")\n",
    "#print(f\"\\tgiven attribute race; {p_rule(y_pred, Z_test['race']):.0f}%-rule\")\n",
    "#print(f\"\\tgiven attribute sex; {p_rule(y_pred, Z_test['sex']):.0f}%-rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for both sensitive attributes the classifier satisfies a p%-rule that is significantly lower than 80%. This supports our earlier conclusion that the trained classifier is unfair in making its predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fighting the bias\n",
    "\n",
    "It is important to stress that training a fair model is not straightforward. One might be tempted to think that simply removing sensitive information from the training data is enough. Our classifier did not have access to the race and sex attributes and still we ended up with a model that is biased against women and black people. This begs the question: what caused our classifier to behave this way? \n",
    "\n",
    "The observed behaviour is most likely caused by biases in the training data. To understand how this works, consider the following two examples of image classification errors:\n",
    "\n",
    "<center><br><img src=\"images/missclassified_images.png\" alt=\"GAN Images\" width=\"300\"/><br></center>\n",
    "\n",
    "The classifier that made these errors was trained on data in which some ethnic and racial minorities are overrepresented by small number of classes. For example, black people are often shown playing basketball and Asian people playing ping-pong. The model picks up on these biases and uses them for making predictions. However, once unleashed into the wild it will encounter images in which these minorities are doing things other than playing basketball or ping-pong. Still relying on its learned biases, the model can misclassify these images in quite painful ways.\n",
    "\n",
    "Now, the UCI dataset, used for training our classifier, has similar kinds of biases in the data. The dataset is based on census data from 1994, a time in which income inequality was just as much of an issue as it is nowadays. Not surprisingly, most of the high earners in the data are white males, while women and black people are more often part of the low-income group. Our predictive model can indirectly learn these biases, for example, through characteristics like education level and zip-code of residence. As a result, we end-up with the unfair predictions observed in previous section, even after having removed the race and sex attributes.\n",
    "\n",
    "How can we go about fixing this issue? In general, there are two approaches we can take. We can somehow try to de-bias the dataset, for example by adding additional data that comes from a more representative sample. Alternatively, we can constrain the model so that it is forced into making fairer predictions. In the next section, we will show how adversarial networks can help in taking the second approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial networks FTW\n",
    "\n",
    "In 2014, Goodfellow et al. published their seminal [paper](https://arxiv.org/abs/1406.2661) on Generative Adversarial Networks (GANs). They introduce GANs as a system of two neural networks, a generative model and an adversarial classifier, which are competing with each other in a zero-sum game. In the game, the generative model focusses on producing samples that are indistinguishable from real data, while the adversarial classifier tries to identify if samples came from the generative model or from the real data. Both networks are trained simultaneously such that the first improves at producing realistic samples, while the second becomes better at spotting the fakes from the real. The figure below shows some examples of images that were generated by a GAN:\n",
    "\n",
    "<center><br><img src=\"images/gan_examples.png\" alt=\"GAN Images\" width=\"500\"/><br></center>\n",
    "\n",
    "Our procedure for training a fair income classifier takes inspiration from GANs: it leverages adversarial networks to enforce the so-called [pivotal property](https://en.wikipedia.org/wiki/Pivotal_quantity) on the predictive model. This statistical property assures that the outcome distribution of the model no longer depends on so-called nuisance parameters. These parameters are not of immediate interest, but must be accounted for in a statistical analysis. By taking the sensitive attributes as our nuisance parameters we can enforce predictions that are independent of, in our case, race and sex. This is exactly what we need for making fair predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial training procedure\n",
    "\n",
    "The starting point for adversarial training our classifier is the extension of the original network architecture with an adversarial component. The figure below shows what this extended architecture looks like:\n",
    "\n",
    "<center><br><img src=\"images/architecture.png\" alt=\"Architecture\" width=\"750\"/><br></center>\n",
    "\n",
    "At first glance, this system of two neural networks looks very similar to the one used for training GANs. However, there are some key differences. First, the generative model has been replaced by a predictive model. So, instead of generating synthetic data it now generates actual predictions $\\hat{y}$ based on the input $X$. Second, the task of the adversarial is no longer to distinguish real from generated data. Instead, it predicts the sensitive attribute values $\\hat{z}\\in\\hat{Z}$ from the predicted $\\hat{y}$ of the classifier. Finally, the objectives that both nets try to optimize are based on the prediction losses of the target and sensitive attributes, these are denoted by $Loss_{y}(\\theta_{clf})$ and $Loss_{Z}(\\theta_{clf},\\theta_{adv})$ in the figure.\n",
    "\n",
    "Lets consider the nature of the zero-sum game the classifier and adversarial are engaged in. For the classifier the objective of twofold: make the best possible income level predictions whilst ensuring that race or sex cannot be derived from them. This is captured by the following objective function:\n",
    "\n",
    "$$\\min_{\\theta_{clf}}\\left[Loss_{y}(\\theta_{clf})-\\lambda Loss_{Z}(\\theta_{clf},\\theta_{adv})\\right].$$\n",
    "\n",
    "So, it learns to minimize its own prediction losses while maximizing that of the adversarial (due to $\\lambda$ being positive and minimizing a negated loss is the same as maximizing it). Note that increasing the size of $\\lambda$ steers the classifier towards fairer predictions while sacrificing prediction accuracy. The objective during the game is simpler For the adversarial: predict race and sex based on the income level predictions of the classifier. This is captured in the following objective function:\n",
    "\n",
    "$$\\min_{\\theta_{adv}}\\left[Loss_{Z}(\\theta_{clf},\\theta_{adv})\\right].$$\n",
    "\n",
    "The adversarial does not care about the prediction accuracy of the classifier. It is only concerned with minimizing its own prediction losses.\n",
    "\n",
    "Now that our classifier is upgraded with an adversarial component, we turn to the adversarial training procedure. In short, we can summarize this procedure in the following 3 steps:\n",
    "\n",
    "1. Pre-train the classifier on the full data set.\n",
    "2. Pre-train the adversarial on the predictions of the pre-trained classifier.\n",
    "3. During $T$ iterations simultaneously train the adversarial and classifier networks:\n",
    "   - first train the adversarial for a single epoch while keeping the classifier fixed\n",
    "   - then train the classifier on a single sampled mini batch while keeping the adversarial fixed.\n",
    "\n",
    "The actual adversarial training starts only after the first two pre-training steps. It is then that the training procedure mimics the zero-sum game during which our classifier will (hopefully) learn how make predictions that are both accurate and fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair income predictions\n",
    "\n",
    "Finally, we are ready to adverserial train a fair classifier. We kick-off by initializing our newly upgraded classifier and pre-train both the classifier and adverserial networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 240;\n                var nbb_unformatted_code = \"\\n# HIDE\\n\\nclass FairClassifier(object):\\n    \\n    def __init__(self, n_features, n_sensitive, lambdas):\\n        self.lambdas = lambdas\\n        \\n        clf_inputs = Input(shape=(n_features,))\\n        adv_inputs = Input(shape=(1,))\\n        \\n        clf_net = self._create_clf_net(clf_inputs)\\n        adv_net = self._create_adv_net(adv_inputs, n_sensitive)\\n        self._trainable_clf_net = self._make_trainable(clf_net)\\n        self._trainable_adv_net = self._make_trainable(adv_net)\\n        self._clf = self._compile_clf(clf_net)\\n        self._clf_w_adv = self._compile_clf_w_adv(clf_inputs, clf_net, adv_net)\\n        self._adv = self._compile_adv(clf_inputs, clf_net, adv_net, n_sensitive)\\n        self._val_metrics = None\\n        self._fairness_metrics = None\\n        \\n        self.predict = self._clf.predict\\n        \\n    def _make_trainable(self, net):\\n        def make_trainable(flag):\\n            net.trainable = flag\\n            for layer in net.layers:\\n                layer.trainable = flag\\n        return make_trainable\\n        \\n    def _create_clf_net(self, inputs):\\n        dense1 = Dense(32, activation='relu')(inputs)\\n        dropout1 = Dropout(0.2)(dense1)\\n        dense2 = Dense(32, activation='relu')(dropout1)\\n        dropout2 = Dropout(0.2)(dense2)\\n        dense3 = Dense(32, activation='relu')(dropout2)\\n        dropout3 = Dropout(0.2)(dense3)\\n        outputs = Dense(1, activation='sigmoid', name='y')(dropout3)\\n        return Model(inputs=[inputs], outputs=[outputs])\\n        \\n    def _create_adv_net(self, inputs, n_sensitive):\\n        dense1 = Dense(32, activation='relu')(inputs)\\n        dense2 = Dense(32, activation='relu')(dense1)\\n        dense3 = Dense(32, activation='relu')(dense2)\\n        outputs = [Dense(1, activation='sigmoid')(dense3) for _ in range(n_sensitive)]\\n        return Model(inputs=[inputs], outputs=outputs)\\n\\n    def _compile_clf(self, clf_net):\\n        clf = clf_net\\n        self._trainable_clf_net(True)\\n        clf.compile(loss='binary_crossentropy', optimizer='adam')\\n        return clf\\n        \\n    def _compile_clf_w_adv(self, inputs, clf_net, adv_net):\\n        clf_w_adv = Model(inputs=[inputs], outputs=[clf_net(inputs)]+adv_net(clf_net(inputs)))\\n        self._trainable_clf_net(True)\\n        self._trainable_adv_net(False)\\n        loss_weights = [1.]+[-lambda_param for lambda_param in self.lambdas]\\n        clf_w_adv.compile(loss=['binary_crossentropy']*(len(loss_weights)), \\n                          loss_weights=loss_weights,\\n                          optimizer='adam')\\n        return clf_w_adv\\n\\n    def _compile_adv(self, inputs, clf_net, adv_net, n_sensitive):\\n        adv = Model(inputs=[inputs], outputs=adv_net(clf_net(inputs)))\\n        self._trainable_clf_net(False)\\n        self._trainable_adv_net(True)\\n        adv.compile(loss=['binary_crossentropy']*n_sensitive, optimizer='adam')\\n        return adv\\n\\n    def _compute_class_weights(self, data_set):\\n        class_values = [0, 1]\\n        class_weights = []\\n        if len(data_set.shape) == 1:\\n            balanced_weights = compute_class_weight('balanced', class_values, data_set)\\n            class_weights.append(dict(zip(class_values, balanced_weights)))\\n        else:\\n            n_attr =  data_set.shape[1]\\n            for attr_idx in range(n_attr):\\n                balanced_weights = compute_class_weight('balanced', class_values,\\n                                                        np.array(data_set)[:,attr_idx])\\n                class_weights.append(dict(zip(class_values, balanced_weights)))\\n        print(f'-compute-target-class_weights > {class_weights}')\\n        return class_weights\\n    \\n    def _compute_target_class_weights(self, y):\\n        class_values  = [0,1]\\n        balanced_weights =  compute_class_weight('balanced', class_values, y)\\n        class_weights = {'y': dict(zip(class_values, balanced_weights))}\\n        print(f'-compute-class-weights > {class_weights}')\\n        return class_weights\\n        \\n    def pretrain(self, x, y, z, epochs=10, verbose=0):\\n        self._trainable_clf_net(True)\\n        self._clf.fit(x.values, y.values, epochs=epochs, verbose=verbose)\\n        self._trainable_clf_net(False)\\n        self._trainable_adv_net(True)\\n        class_weight_adv = self._compute_class_weights(z)\\n        print(f'pretrain-class-weight-adv > {class_weight_adv}')\\n        self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), class_weight=class_weight_adv, \\n                      epochs=epochs, verbose=verbose)\\n        \\n    def fit(self, x, y, z, validation_data=None, T_iter=250, batch_size=128,\\n            save_figs=False):\\n        n_sensitive = z.shape[1]\\n        if validation_data is not None:\\n            x_val, y_val, z_val = validation_data\\n\\n        class_weight_adv = self._compute_class_weights(z)\\n        class_weight_clf_w_adv = [{0:1., 1:5.}]+class_weight_adv\\n        print(f' class_weight_clf_w_adv-- {class_weight_clf_w_adv}')\\n        self._val_metrics = pd.DataFrame()\\n        self._fairness_metrics = pd.DataFrame()  \\n        for idx in range(T_iter):\\n            if validation_data is not None:\\n                self._clf.save(f'output/models/{idx:03d}.h5')  # creates a HDF5 file 'my_model.h5'\\n                y_pred = pd.Series(self._clf.predict(x_val).ravel(), index=y_val.index)\\n                print(f'Model= {idx}')\\n                thresholdOpt, metricOpt, rscoreOpt, pscoreOpt, srscoreOpt = return_metric(y_pred, y_val, z_val)              \\n\\n                self._val_metrics.loc[idx, 'model'] = idx\\n                self._val_metrics.loc[idx, 'thresholdOpt'] = thresholdOpt\\n                self._val_metrics.loc[idx, 'metricOpt'] = metricOpt\\n                self._val_metrics.loc[idx, 'rscoreOpt'] = rscoreOpt\\n                self._val_metrics.loc[idx, 'pscoreOpt'] = pscoreOpt\\n                self._val_metrics.loc[idx, 'srscoreOpt'] = srscoreOpt\\n                \\n                #pyplot.plot(recall_prc, precision_prc, marker='.', label='NN')\\n                #pyplot.scatter(recall_prc[f1_ix], precision_prc[f1_ix], marker='o', color='black', label='Best f1')\\n                #pyplot.scatter(recall_prc[recall_ix], precision_prc[recall_ix], marker='o', color='orange', label='Best Recall')\\n                # axis labels\\n                #pyplot.xlabel('Recall')\\n                #pyplot.ylabel('Precision')\\n                #pyplot.legend()\\n                ## show the plot\\n                #pyplot.show()\\n\\n            self._val_metrics = self._val_metrics.sort_values(by=['metricOpt'], ascending=False)\\n            self._val_metrics.to_csv('output/_val_metrics.csv', index=False)\\n\\n            # train adverserial\\n            self._trainable_clf_net(False)\\n            self._trainable_adv_net(True)\\n            self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), batch_size=batch_size, \\n                          class_weight=class_weight_adv, epochs=1, verbose=0)\\n            \\n            # train classifier\\n            self._trainable_clf_net(True)\\n            self._trainable_adv_net(False)\\n            indices = np.random.permutation(len(x))[:batch_size]\\n            self._clf_w_adv.train_on_batch(x.values[indices], \\n                                           [y.values[indices]]+np.hsplit(z.values[indices], n_sensitive),\\n                                           class_weight=class_weight_clf_w_adv)\\n        \";\n                var nbb_formatted_code = \"# HIDE\\n\\n\\nclass FairClassifier(object):\\n    def __init__(self, n_features, n_sensitive, lambdas):\\n        self.lambdas = lambdas\\n\\n        clf_inputs = Input(shape=(n_features,))\\n        adv_inputs = Input(shape=(1,))\\n\\n        clf_net = self._create_clf_net(clf_inputs)\\n        adv_net = self._create_adv_net(adv_inputs, n_sensitive)\\n        self._trainable_clf_net = self._make_trainable(clf_net)\\n        self._trainable_adv_net = self._make_trainable(adv_net)\\n        self._clf = self._compile_clf(clf_net)\\n        self._clf_w_adv = self._compile_clf_w_adv(clf_inputs, clf_net, adv_net)\\n        self._adv = self._compile_adv(clf_inputs, clf_net, adv_net, n_sensitive)\\n        self._val_metrics = None\\n        self._fairness_metrics = None\\n\\n        self.predict = self._clf.predict\\n\\n    def _make_trainable(self, net):\\n        def make_trainable(flag):\\n            net.trainable = flag\\n            for layer in net.layers:\\n                layer.trainable = flag\\n\\n        return make_trainable\\n\\n    def _create_clf_net(self, inputs):\\n        dense1 = Dense(32, activation=\\\"relu\\\")(inputs)\\n        dropout1 = Dropout(0.2)(dense1)\\n        dense2 = Dense(32, activation=\\\"relu\\\")(dropout1)\\n        dropout2 = Dropout(0.2)(dense2)\\n        dense3 = Dense(32, activation=\\\"relu\\\")(dropout2)\\n        dropout3 = Dropout(0.2)(dense3)\\n        outputs = Dense(1, activation=\\\"sigmoid\\\", name=\\\"y\\\")(dropout3)\\n        return Model(inputs=[inputs], outputs=[outputs])\\n\\n    def _create_adv_net(self, inputs, n_sensitive):\\n        dense1 = Dense(32, activation=\\\"relu\\\")(inputs)\\n        dense2 = Dense(32, activation=\\\"relu\\\")(dense1)\\n        dense3 = Dense(32, activation=\\\"relu\\\")(dense2)\\n        outputs = [Dense(1, activation=\\\"sigmoid\\\")(dense3) for _ in range(n_sensitive)]\\n        return Model(inputs=[inputs], outputs=outputs)\\n\\n    def _compile_clf(self, clf_net):\\n        clf = clf_net\\n        self._trainable_clf_net(True)\\n        clf.compile(loss=\\\"binary_crossentropy\\\", optimizer=\\\"adam\\\")\\n        return clf\\n\\n    def _compile_clf_w_adv(self, inputs, clf_net, adv_net):\\n        clf_w_adv = Model(\\n            inputs=[inputs], outputs=[clf_net(inputs)] + adv_net(clf_net(inputs))\\n        )\\n        self._trainable_clf_net(True)\\n        self._trainable_adv_net(False)\\n        loss_weights = [1.0] + [-lambda_param for lambda_param in self.lambdas]\\n        clf_w_adv.compile(\\n            loss=[\\\"binary_crossentropy\\\"] * (len(loss_weights)),\\n            loss_weights=loss_weights,\\n            optimizer=\\\"adam\\\",\\n        )\\n        return clf_w_adv\\n\\n    def _compile_adv(self, inputs, clf_net, adv_net, n_sensitive):\\n        adv = Model(inputs=[inputs], outputs=adv_net(clf_net(inputs)))\\n        self._trainable_clf_net(False)\\n        self._trainable_adv_net(True)\\n        adv.compile(loss=[\\\"binary_crossentropy\\\"] * n_sensitive, optimizer=\\\"adam\\\")\\n        return adv\\n\\n    def _compute_class_weights(self, data_set):\\n        class_values = [0, 1]\\n        class_weights = []\\n        if len(data_set.shape) == 1:\\n            balanced_weights = compute_class_weight(\\\"balanced\\\", class_values, data_set)\\n            class_weights.append(dict(zip(class_values, balanced_weights)))\\n        else:\\n            n_attr = data_set.shape[1]\\n            for attr_idx in range(n_attr):\\n                balanced_weights = compute_class_weight(\\n                    \\\"balanced\\\", class_values, np.array(data_set)[:, attr_idx]\\n                )\\n                class_weights.append(dict(zip(class_values, balanced_weights)))\\n        print(f\\\"-compute-target-class_weights > {class_weights}\\\")\\n        return class_weights\\n\\n    def _compute_target_class_weights(self, y):\\n        class_values = [0, 1]\\n        balanced_weights = compute_class_weight(\\\"balanced\\\", class_values, y)\\n        class_weights = {\\\"y\\\": dict(zip(class_values, balanced_weights))}\\n        print(f\\\"-compute-class-weights > {class_weights}\\\")\\n        return class_weights\\n\\n    def pretrain(self, x, y, z, epochs=10, verbose=0):\\n        self._trainable_clf_net(True)\\n        self._clf.fit(x.values, y.values, epochs=epochs, verbose=verbose)\\n        self._trainable_clf_net(False)\\n        self._trainable_adv_net(True)\\n        class_weight_adv = self._compute_class_weights(z)\\n        print(f\\\"pretrain-class-weight-adv > {class_weight_adv}\\\")\\n        self._adv.fit(\\n            x.values,\\n            np.hsplit(z.values, z.shape[1]),\\n            class_weight=class_weight_adv,\\n            epochs=epochs,\\n            verbose=verbose,\\n        )\\n\\n    def fit(\\n        self, x, y, z, validation_data=None, T_iter=250, batch_size=128, save_figs=False\\n    ):\\n        n_sensitive = z.shape[1]\\n        if validation_data is not None:\\n            x_val, y_val, z_val = validation_data\\n\\n        class_weight_adv = self._compute_class_weights(z)\\n        class_weight_clf_w_adv = [{0: 1.0, 1: 5.0}] + class_weight_adv\\n        print(f\\\" class_weight_clf_w_adv-- {class_weight_clf_w_adv}\\\")\\n        self._val_metrics = pd.DataFrame()\\n        self._fairness_metrics = pd.DataFrame()\\n        for idx in range(T_iter):\\n            if validation_data is not None:\\n                self._clf.save(\\n                    f\\\"output/models/{idx:03d}.h5\\\"\\n                )  # creates a HDF5 file 'my_model.h5'\\n                y_pred = pd.Series(self._clf.predict(x_val).ravel(), index=y_val.index)\\n                print(f\\\"Model= {idx}\\\")\\n                (\\n                    thresholdOpt,\\n                    metricOpt,\\n                    rscoreOpt,\\n                    pscoreOpt,\\n                    srscoreOpt,\\n                ) = return_metric(y_pred, y_val, z_val)\\n\\n                self._val_metrics.loc[idx, \\\"model\\\"] = idx\\n                self._val_metrics.loc[idx, \\\"thresholdOpt\\\"] = thresholdOpt\\n                self._val_metrics.loc[idx, \\\"metricOpt\\\"] = metricOpt\\n                self._val_metrics.loc[idx, \\\"rscoreOpt\\\"] = rscoreOpt\\n                self._val_metrics.loc[idx, \\\"pscoreOpt\\\"] = pscoreOpt\\n                self._val_metrics.loc[idx, \\\"srscoreOpt\\\"] = srscoreOpt\\n\\n                # pyplot.plot(recall_prc, precision_prc, marker='.', label='NN')\\n                # pyplot.scatter(recall_prc[f1_ix], precision_prc[f1_ix], marker='o', color='black', label='Best f1')\\n                # pyplot.scatter(recall_prc[recall_ix], precision_prc[recall_ix], marker='o', color='orange', label='Best Recall')\\n                # axis labels\\n                # pyplot.xlabel('Recall')\\n                # pyplot.ylabel('Precision')\\n                # pyplot.legend()\\n                ## show the plot\\n                # pyplot.show()\\n\\n            self._val_metrics = self._val_metrics.sort_values(\\n                by=[\\\"metricOpt\\\"], ascending=False\\n            )\\n            self._val_metrics.to_csv(\\\"output/_val_metrics.csv\\\", index=False)\\n\\n            # train adverserial\\n            self._trainable_clf_net(False)\\n            self._trainable_adv_net(True)\\n            self._adv.fit(\\n                x.values,\\n                np.hsplit(z.values, z.shape[1]),\\n                batch_size=batch_size,\\n                class_weight=class_weight_adv,\\n                epochs=1,\\n                verbose=0,\\n            )\\n\\n            # train classifier\\n            self._trainable_clf_net(True)\\n            self._trainable_adv_net(False)\\n            indices = np.random.permutation(len(x))[:batch_size]\\n            self._clf_w_adv.train_on_batch(\\n                x.values[indices],\\n                [y.values[indices]] + np.hsplit(z.values[indices], n_sensitive),\\n                class_weight=class_weight_clf_w_adv,\\n            )\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "\n",
    "# HIDE\n",
    "\n",
    "class FairClassifier(object):\n",
    "    \n",
    "    def __init__(self, n_features, n_sensitive, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        clf_inputs = Input(shape=(n_features,))\n",
    "        adv_inputs = Input(shape=(1,))\n",
    "        \n",
    "        clf_net = self._create_clf_net(clf_inputs)\n",
    "        adv_net = self._create_adv_net(adv_inputs, n_sensitive)\n",
    "        self._trainable_clf_net = self._make_trainable(clf_net)\n",
    "        self._trainable_adv_net = self._make_trainable(adv_net)\n",
    "        self._clf = self._compile_clf(clf_net)\n",
    "        self._clf_w_adv = self._compile_clf_w_adv(clf_inputs, clf_net, adv_net)\n",
    "        self._adv = self._compile_adv(clf_inputs, clf_net, adv_net, n_sensitive)\n",
    "        self._val_metrics = None\n",
    "        self._fairness_metrics = None\n",
    "        \n",
    "        self.predict = self._clf.predict\n",
    "        \n",
    "    def _make_trainable(self, net):\n",
    "        def make_trainable(flag):\n",
    "            net.trainable = flag\n",
    "            for layer in net.layers:\n",
    "                layer.trainable = flag\n",
    "        return make_trainable\n",
    "        \n",
    "    def _create_clf_net(self, inputs):\n",
    "        dense1 = Dense(32, activation='relu')(inputs)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        dense2 = Dense(32, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        dense3 = Dense(32, activation='relu')(dropout2)\n",
    "        dropout3 = Dropout(0.2)(dense3)\n",
    "        outputs = Dense(1, activation='sigmoid', name='y')(dropout3)\n",
    "        return Model(inputs=[inputs], outputs=[outputs])\n",
    "        \n",
    "    def _create_adv_net(self, inputs, n_sensitive):\n",
    "        dense1 = Dense(32, activation='relu')(inputs)\n",
    "        dense2 = Dense(32, activation='relu')(dense1)\n",
    "        dense3 = Dense(32, activation='relu')(dense2)\n",
    "        outputs = [Dense(1, activation='sigmoid')(dense3) for _ in range(n_sensitive)]\n",
    "        return Model(inputs=[inputs], outputs=outputs)\n",
    "\n",
    "    def _compile_clf(self, clf_net):\n",
    "        clf = clf_net\n",
    "        self._trainable_clf_net(True)\n",
    "        clf.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        return clf\n",
    "        \n",
    "    def _compile_clf_w_adv(self, inputs, clf_net, adv_net):\n",
    "        clf_w_adv = Model(inputs=[inputs], outputs=[clf_net(inputs)]+adv_net(clf_net(inputs)))\n",
    "        self._trainable_clf_net(True)\n",
    "        self._trainable_adv_net(False)\n",
    "        loss_weights = [1.]+[-lambda_param for lambda_param in self.lambdas]\n",
    "        clf_w_adv.compile(loss=['binary_crossentropy']*(len(loss_weights)), \n",
    "                          loss_weights=loss_weights,\n",
    "                          optimizer='adam')\n",
    "        return clf_w_adv\n",
    "\n",
    "    def _compile_adv(self, inputs, clf_net, adv_net, n_sensitive):\n",
    "        adv = Model(inputs=[inputs], outputs=adv_net(clf_net(inputs)))\n",
    "        self._trainable_clf_net(False)\n",
    "        self._trainable_adv_net(True)\n",
    "        adv.compile(loss=['binary_crossentropy']*n_sensitive, optimizer='adam')\n",
    "        return adv\n",
    "\n",
    "    def _compute_class_weights(self, data_set):\n",
    "        class_values = [0, 1]\n",
    "        class_weights = []\n",
    "        if len(data_set.shape) == 1:\n",
    "            balanced_weights = compute_class_weight('balanced', class_values, data_set)\n",
    "            class_weights.append(dict(zip(class_values, balanced_weights)))\n",
    "        else:\n",
    "            n_attr =  data_set.shape[1]\n",
    "            for attr_idx in range(n_attr):\n",
    "                balanced_weights = compute_class_weight('balanced', class_values,\n",
    "                                                        np.array(data_set)[:,attr_idx])\n",
    "                class_weights.append(dict(zip(class_values, balanced_weights)))\n",
    "        print(f'-compute-target-class_weights > {class_weights}')\n",
    "        return class_weights\n",
    "    \n",
    "    def _compute_target_class_weights(self, y):\n",
    "        class_values  = [0,1]\n",
    "        balanced_weights =  compute_class_weight('balanced', class_values, y)\n",
    "        class_weights = {'y': dict(zip(class_values, balanced_weights))}\n",
    "        print(f'-compute-class-weights > {class_weights}')\n",
    "        return class_weights\n",
    "        \n",
    "    def pretrain(self, x, y, z, epochs=10, verbose=0):\n",
    "        self._trainable_clf_net(True)\n",
    "        self._clf.fit(x.values, y.values, epochs=epochs, verbose=verbose)\n",
    "        self._trainable_clf_net(False)\n",
    "        self._trainable_adv_net(True)\n",
    "        class_weight_adv = self._compute_class_weights(z)\n",
    "        print(f'pretrain-class-weight-adv > {class_weight_adv}')\n",
    "        self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), class_weight=class_weight_adv, \n",
    "                      epochs=epochs, verbose=verbose)\n",
    "        \n",
    "    def fit(self, x, y, z, validation_data=None, T_iter=250, batch_size=128,\n",
    "            save_figs=False):\n",
    "        n_sensitive = z.shape[1]\n",
    "        if validation_data is not None:\n",
    "            x_val, y_val, z_val = validation_data\n",
    "\n",
    "        class_weight_adv = self._compute_class_weights(z)\n",
    "        class_weight_clf_w_adv = [{0:1., 1:5.}]+class_weight_adv\n",
    "        print(f' class_weight_clf_w_adv-- {class_weight_clf_w_adv}')\n",
    "        self._val_metrics = pd.DataFrame()\n",
    "        self._fairness_metrics = pd.DataFrame()  \n",
    "        for idx in range(T_iter):\n",
    "            if validation_data is not None:\n",
    "                self._clf.save(f'output/models/{idx:03d}.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "                y_pred = pd.Series(self._clf.predict(x_val).ravel(), index=y_val.index)\n",
    "                print(f'Model= {idx}')\n",
    "                thresholdOpt, metricOpt, rscoreOpt, pscoreOpt, srscoreOpt = return_metric(y_pred, y_val, z_val)              \n",
    "\n",
    "                self._val_metrics.loc[idx, 'model'] = idx\n",
    "                self._val_metrics.loc[idx, 'thresholdOpt'] = thresholdOpt\n",
    "                self._val_metrics.loc[idx, 'metricOpt'] = metricOpt\n",
    "                self._val_metrics.loc[idx, 'rscoreOpt'] = rscoreOpt\n",
    "                self._val_metrics.loc[idx, 'pscoreOpt'] = pscoreOpt\n",
    "                self._val_metrics.loc[idx, 'srscoreOpt'] = srscoreOpt\n",
    "                \n",
    "                #pyplot.plot(recall_prc, precision_prc, marker='.', label='NN')\n",
    "                #pyplot.scatter(recall_prc[f1_ix], precision_prc[f1_ix], marker='o', color='black', label='Best f1')\n",
    "                #pyplot.scatter(recall_prc[recall_ix], precision_prc[recall_ix], marker='o', color='orange', label='Best Recall')\n",
    "                # axis labels\n",
    "                #pyplot.xlabel('Recall')\n",
    "                #pyplot.ylabel('Precision')\n",
    "                #pyplot.legend()\n",
    "                ## show the plot\n",
    "                #pyplot.show()\n",
    "\n",
    "            self._val_metrics = self._val_metrics.sort_values(by=['metricOpt'], ascending=False)\n",
    "            self._val_metrics.to_csv('output/_val_metrics.csv', index=False)\n",
    "\n",
    "            # train adverserial\n",
    "            self._trainable_clf_net(False)\n",
    "            self._trainable_adv_net(True)\n",
    "            self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), batch_size=batch_size, \n",
    "                          class_weight=class_weight_adv, epochs=1, verbose=0)\n",
    "            \n",
    "            # train classifier\n",
    "            self._trainable_clf_net(True)\n",
    "            self._trainable_adv_net(False)\n",
    "            indices = np.random.permutation(len(x))[:batch_size]\n",
    "            self._clf_w_adv.train_on_batch(x.values[indices], \n",
    "                                           [y.values[indices]]+np.hsplit(z.values[indices], n_sensitive),\n",
    "                                           class_weight=class_weight_clf_w_adv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-compute-target-class_weights > [{0: 0.7283807887956663, 1: 1.5946630026034134}, {0: 0.7283807887956663, 1: 1.5946630026034134}]\npretrain-class-weight-adv > [{0: 0.7283807887956663, 1: 1.5946630026034134}, {0: 0.7283807887956663, 1: 1.5946630026034134}]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 241;\n                var nbb_unformatted_code = \"# initialise FairClassifier\\nclf = FairClassifier(n_features=X_train.shape[1], n_sensitive=Z_train.shape[1], lambdas=[1., 1.])\\n\\n# pre-train both adverserial and classifier networks\\nclf.pretrain(X_train, y_train, Z_train, verbose=0, epochs=5)\";\n                var nbb_formatted_code = \"# initialise FairClassifier\\nclf = FairClassifier(\\n    n_features=X_train.shape[1], n_sensitive=Z_train.shape[1], lambdas=[1.0, 1.0]\\n)\\n\\n# pre-train both adverserial and classifier networks\\nclf.pretrain(X_train, y_train, Z_train, verbose=0, epochs=5)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# initialise FairClassifier\n",
    "clf = FairClassifier(n_features=X_train.shape[1], n_sensitive=Z_train.shape[1], lambdas=[1., 1.])\n",
    "\n",
    "# pre-train both adverserial and classifier networks\n",
    "clf.pretrain(X_train, y_train, Z_train, verbose=0, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supplied $\\lambda$ values, that tune fairness versus accuracy, are set to $\\lambda_{race}=130$ and $\\lambda_{sex}=30$. We heuristically found that these settings result in a balanced increase of the p%-rule values during training. Apparently, it is slightly harder to enforce fairness for the racial attributes than for sex.\n",
    "\n",
    "Now that both networks have been pre-trained, the adversarial training can start. We will simultaneously train both networks for 165 iterations while tracking the performance of the classifier on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 242;\n                var nbb_unformatted_code = \"# HIDE\\n#if create_gif:\\n #   !rm output/*.png\";\n                var nbb_formatted_code = \"# HIDE\\n# if create_gif:\\n#   !rm output/*.png\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# HIDE\n",
    "#if create_gif:\n",
    " #   !rm output/*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-compute-target-class_weights > [{0: 0.7283807887956663, 1: 1.5946630026034134}, {0: 0.7283807887956663, 1: 1.5946630026034134}]\n",
      " class_weight_clf_w_adv-- [{0: 1.0, 1: 5.0}, {0: 0.7283807887956663, 1: 1.5946630026034134}, {0: 0.7283807887956663, 1: 1.5946630026034134}]\n",
      "Model= 0\n",
      "Best Threshold: 0.492 with Metric: 0.4125\n",
      "Best Threshold: 0.492 with Recall: 0.5853\n",
      "Best Threshold: 0.492 with Pvalue: 0.7179\n",
      "Best Threshold: 0.492 with SR: 0.9816\n",
      "Model= 1\n",
      "Best Threshold: 0.497 with Metric: 0.4135\n",
      "Best Threshold: 0.497 with Recall: 0.5846\n",
      "Best Threshold: 0.497 with Pvalue: 0.7204\n",
      "Best Threshold: 0.497 with SR: 0.982\n",
      "Model= 2\n",
      "Best Threshold: 0.502 with Metric: 0.4133\n",
      "Best Threshold: 0.502 with Recall: 0.5849\n",
      "Best Threshold: 0.502 with Pvalue: 0.7192\n",
      "Best Threshold: 0.502 with SR: 0.9826\n",
      "Model= 3\n",
      "Best Threshold: 0.507 with Metric: 0.4159\n",
      "Best Threshold: 0.507 with Recall: 0.5836\n",
      "Best Threshold: 0.507 with Pvalue: 0.7249\n",
      "Best Threshold: 0.507 with SR: 0.983\n",
      "Model= 4\n",
      "Best Threshold: 0.511 with Metric: 0.4181\n",
      "Best Threshold: 0.511 with Recall: 0.586\n",
      "Best Threshold: 0.511 with Pvalue: 0.7275\n",
      "Best Threshold: 0.511 with SR: 0.9807\n",
      "Model= 5\n",
      "Best Threshold: 0.516 with Metric: 0.4197\n",
      "Best Threshold: 0.516 with Recall: 0.5846\n",
      "Best Threshold: 0.516 with Pvalue: 0.7312\n",
      "Best Threshold: 0.516 with SR: 0.9818\n",
      "Model= 6\n",
      "Best Threshold: 0.521 with Metric: 0.4212\n",
      "Best Threshold: 0.521 with Recall: 0.584\n",
      "Best Threshold: 0.521 with Pvalue: 0.734\n",
      "Best Threshold: 0.521 with SR: 0.9827\n",
      "Model= 7\n",
      "Best Threshold: 0.526 with Metric: 0.4235\n",
      "Best Threshold: 0.526 with Recall: 0.5832\n",
      "Best Threshold: 0.526 with Pvalue: 0.7383\n",
      "Best Threshold: 0.526 with SR: 0.9835\n",
      "Model= 8\n",
      "Best Threshold: 0.531 with Metric: 0.4277\n",
      "Best Threshold: 0.531 with Recall: 0.5845\n",
      "Best Threshold: 0.531 with Pvalue: 0.7455\n",
      "Best Threshold: 0.531 with SR: 0.9816\n",
      "Model= 9\n",
      "Best Threshold: 0.536 with Metric: 0.428\n",
      "Best Threshold: 0.536 with Recall: 0.5848\n",
      "Best Threshold: 0.536 with Pvalue: 0.7463\n",
      "Best Threshold: 0.536 with SR: 0.9806\n",
      "Model= 10\n",
      "Best Threshold: 0.543 with Metric: 0.4269\n",
      "Best Threshold: 0.543 with Recall: 0.5782\n",
      "Best Threshold: 0.543 with Pvalue: 0.7476\n",
      "Best Threshold: 0.543 with SR: 0.9876\n",
      "Model= 11\n",
      "Best Threshold: 0.548 with Metric: 0.4265\n",
      "Best Threshold: 0.548 with Recall: 0.5796\n",
      "Best Threshold: 0.548 with Pvalue: 0.7473\n",
      "Best Threshold: 0.548 with SR: 0.9845\n",
      "Model= 12\n",
      "Best Threshold: 0.554 with Metric: 0.4295\n",
      "Best Threshold: 0.554 with Recall: 0.58\n",
      "Best Threshold: 0.554 with Pvalue: 0.7525\n",
      "Best Threshold: 0.554 with SR: 0.9842\n",
      "Model= 13\n",
      "Best Threshold: 0.559 with Metric: 0.4322\n",
      "Best Threshold: 0.559 with Recall: 0.5834\n",
      "Best Threshold: 0.559 with Pvalue: 0.7558\n",
      "Best Threshold: 0.559 with SR: 0.9801\n",
      "Model= 14\n",
      "Best Threshold: 0.566 with Metric: 0.4314\n",
      "Best Threshold: 0.566 with Recall: 0.5809\n",
      "Best Threshold: 0.566 with Pvalue: 0.7559\n",
      "Best Threshold: 0.566 with SR: 0.9825\n",
      "Model= 15\n",
      "Best Threshold: 0.572 with Metric: 0.4341\n",
      "Best Threshold: 0.572 with Recall: 0.5818\n",
      "Best Threshold: 0.572 with Pvalue: 0.7601\n",
      "Best Threshold: 0.572 with SR: 0.9816\n",
      "Model= 16\n",
      "Best Threshold: 0.579 with Metric: 0.4348\n",
      "Best Threshold: 0.579 with Recall: 0.5808\n",
      "Best Threshold: 0.579 with Pvalue: 0.7616\n",
      "Best Threshold: 0.579 with SR: 0.9829\n",
      "Model= 17\n",
      "Best Threshold: 0.585 with Metric: 0.4369\n",
      "Best Threshold: 0.585 with Recall: 0.5829\n",
      "Best Threshold: 0.585 with Pvalue: 0.7637\n",
      "Best Threshold: 0.585 with SR: 0.9813\n",
      "Model= 18\n",
      "Best Threshold: 0.592 with Metric: 0.4365\n",
      "Best Threshold: 0.592 with Recall: 0.5824\n",
      "Best Threshold: 0.592 with Pvalue: 0.7634\n",
      "Best Threshold: 0.592 with SR: 0.9817\n",
      "Model= 19\n",
      "Best Threshold: 0.599 with Metric: 0.4375\n",
      "Best Threshold: 0.599 with Recall: 0.5835\n",
      "Best Threshold: 0.599 with Pvalue: 0.7645\n",
      "Best Threshold: 0.599 with SR: 0.9807\n",
      "Model= 20\n",
      "Best Threshold: 0.606 with Metric: 0.4374\n",
      "Best Threshold: 0.606 with Recall: 0.5831\n",
      "Best Threshold: 0.606 with Pvalue: 0.765\n",
      "Best Threshold: 0.606 with SR: 0.9806\n",
      "Model= 21\n",
      "Best Threshold: 0.613 with Metric: 0.4392\n",
      "Best Threshold: 0.613 with Recall: 0.5824\n",
      "Best Threshold: 0.613 with Pvalue: 0.769\n",
      "Best Threshold: 0.613 with SR: 0.9806\n",
      "Model= 22\n",
      "Best Threshold: 0.621 with Metric: 0.4409\n",
      "Best Threshold: 0.621 with Recall: 0.5808\n",
      "Best Threshold: 0.621 with Pvalue: 0.7726\n",
      "Best Threshold: 0.621 with SR: 0.9824\n",
      "Model= 23\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-2fa3c3ff9b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m clf.fit(X_train, y_train, Z_train, \n\u001b[1;32m      3\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         T_iter=50, save_figs=create_gif)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-240-c3a9c1bd182a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, z, validation_data, T_iter, batch_size, save_figs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model= {idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mthresholdOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrscoreOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpscoreOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrscoreOpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-00daf2df7b47>\u001b[0m in \u001b[0;36mreturn_metric\u001b[0;34m(y_pred, y_val, z_val)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mrscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msrscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mrscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-00daf2df7b47>\u001b[0m in \u001b[0;36msr_rule\u001b[0;34m(y_pred, threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mBT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_bin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mBT\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fairness-in-ml/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-00daf2df7b47>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mBT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_bin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mBT\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# adverserial train on train set and validate on test set\n",
    "clf.fit(X_train, y_train, Z_train, \n",
    "        validation_data=(X_test, y_test, Z_test),\n",
    "        T_iter=50, save_figs=create_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 41;\n                var nbb_unformatted_code = \"# HIDE\\n#if create_gif:\\n#    !convert -loop 0 -delay 0 output/*.png -delay 500 output/00000050.png images/training.gif\";\n                var nbb_formatted_code = \"# HIDE\\n# if create_gif:\\n#    !convert -loop 0 -delay 0 output/*.png -delay 500 output/00000050.png images/training.gif\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# HIDE\n",
    "#if create_gif:\n",
    "#    !convert -loop 0 -delay 0 output/*.png -delay 500 output/00000050.png images/training.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><img src=\"images/training.gif\" alt=\"Architecture\" width=\"900\"/><br></center>\n",
    "\n",
    "The plots above show how the prediction distributions, the satisfied p%-rules and the prediction performance of our classifier evolve during adversarial training. At iteration #1, when training is just starting out, the predictions are very much the same as observed for the previously trained classifier: both high in bias and in prediction performance. As the training progresses, we see that the predictions are gradually become more and more fair while prediction performance is slightly declining. Finally, after 165 iterations of training, we see that the classifier satisfies the 80%-rule for both sensitive attributes while achieving a ROC AUC 0.85 and an accuracy of 82%. \n",
    "\n",
    "So, it seems that the training procedure works quite well. After sacrificing only 7% of prediction performance, we end up with a classifier that makes fair predictions when it comes to race and sex. A pretty decent result!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post we have shown that bringing fairness to predictive models is not as straight forward as 'just' removing some sensitive attributes from the training data. It requires clever techniques, like adverserial training, to correct for the often deeply biased training data and force our models into making fair predictions. And yes, making fair predictions comes at a cost: it will reduce the performance of your model (hopefully, only by a little as was the case in our example). However, in many cases this will be a relatively small price to pay for leaving behind the biased world of yesterday and predicting our way into a fairer tomorrow!\n",
    "\n",
    "*Shout-out to [Henk](https://godatadriven.com/players/henk-griffioen) who was so kind to review this work and provide his comments!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4915555555555556"
      ]
     },
     "metadata": {},
     "execution_count": 229
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 229;\n                var nbb_unformatted_code = \"# Retained\\n\\nBT_R = .489\\n\\nmodel = load_model('output/models/final/model_r.h5')\\ny_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\\ndf_r = pd.DataFrame(y_pred, columns=['r_raw'])\\ndf_r['r_bin'] = df_r['r_raw'].apply(lambda x: 1 if x >= BT_R else 0)\\ndf_r['r_bin'].mean()\";\n                var nbb_formatted_code = \"# Retained\\n\\nBT_R = 0.489\\n\\nmodel = load_model(\\\"output/models/final/model_r.h5\\\")\\ny_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\\ndf_r = pd.DataFrame(y_pred, columns=[\\\"r_raw\\\"])\\ndf_r[\\\"r_bin\\\"] = df_r[\\\"r_raw\\\"].apply(lambda x: 1 if x >= BT_R else 0)\\ndf_r[\\\"r_bin\\\"].mean()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Retained\n",
    "\n",
    "BT_R = .489\n",
    "\n",
    "model = load_model('output/models/final/model_r.h5')\n",
    "y_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\n",
    "df_r = pd.DataFrame(y_pred, columns=['r_raw'])\n",
    "df_r['r_bin'] = df_r['r_raw'].apply(lambda x: 1 if x >= BT_R else 0)\n",
    "df_r['r_bin'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5128888888888888"
      ]
     },
     "metadata": {},
     "execution_count": 230
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 230;\n                var nbb_unformatted_code = \"# High Performing\\n\\nBT_HP = .063\\n\\nmodel = load_model('output/models/final/model_hp.h5')\\ny_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\\ndf_hp = pd.DataFrame(y_pred, columns=['hp_raw'])\\ndf_hp['hp_bin'] = df_hp['hp_raw'].apply(lambda x: 1 if x >= BT_HP else 0)\\ndf_hp['hp_bin'].mean()\";\n                var nbb_formatted_code = \"# High Performing\\n\\nBT_HP = 0.063\\n\\nmodel = load_model(\\\"output/models/final/model_hp.h5\\\")\\ny_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\\ndf_hp = pd.DataFrame(y_pred, columns=[\\\"hp_raw\\\"])\\ndf_hp[\\\"hp_bin\\\"] = df_hp[\\\"hp_raw\\\"].apply(lambda x: 1 if x >= BT_HP else 0)\\ndf_hp[\\\"hp_bin\\\"].mean()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# High Performing\n",
    "\n",
    "BT_HP = .063\n",
    "\n",
    "model = load_model('output/models/final/model_hp.h5')\n",
    "y_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\n",
    "df_hp = pd.DataFrame(y_pred, columns=['hp_raw'])\n",
    "df_hp['hp_bin'] = df_hp['hp_raw'].apply(lambda x: 1 if x >= BT_HP else 0)\n",
    "df_hp['hp_bin'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5146666666666667"
      ]
     },
     "metadata": {},
     "execution_count": 231
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 231;\n                var nbb_unformatted_code = \"# HPR\\n\\nBT_HPR = .042\\n\\nmodel = load_model('output/models/final/model_hpr.h5')\\ny_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\\ndf_hpr = pd.DataFrame(y_pred, columns=['hpr_raw'])\\ndf_hpr['hpr_bin'] = df_hpr['hpr_raw'].apply(lambda x: 1 if x >= BT_HPR else 0)\\ndf_hpr['hpr_bin'].mean()\";\n                var nbb_formatted_code = \"# HPR\\n\\nBT_HPR = 0.042\\n\\nmodel = load_model(\\\"output/models/final/model_hpr.h5\\\")\\ny_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\\ndf_hpr = pd.DataFrame(y_pred, columns=[\\\"hpr_raw\\\"])\\ndf_hpr[\\\"hpr_bin\\\"] = df_hpr[\\\"hpr_raw\\\"].apply(lambda x: 1 if x >= BT_HPR else 0)\\ndf_hpr[\\\"hpr_bin\\\"].mean()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# HPR\n",
    "\n",
    "BT_HPR = .042\n",
    "\n",
    "model = load_model('output/models/final/model_hpr.h5')\n",
    "y_pred = pd.Series(model.predict(X_dev).ravel(), index=X_dev.index)\n",
    "df_hpr = pd.DataFrame(y_pred, columns=['hpr_raw'])\n",
    "df_hpr['hpr_bin'] = df_hpr['hpr_raw'].apply(lambda x: 1 if x >= BT_HPR else 0)\n",
    "df_hpr['hpr_bin'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 234;\n                var nbb_unformatted_code = \"pdList = [df_hpr, df_hp, df_r]  # List of your dataframes\\nresults = pd.concat(pdList, axis=1)\\nresults['hpr_bin_comp'] = results['hpr_bin'] * 1\\nresults['hp_bin_comp'] = results['hp_bin'] * 0\\nresults['r_bin_comp'] = results['r_bin'] * 0\\n\\nidx_comp = results.columns.str.endswith('_comp')\\nresults['bin_comp'] = results.iloc[:,idx_comp].sum(axis=1)\\n\\nresults['hpr_raw_weighted'] = results['hpr_raw'] * 1\\nresults['hp_raw_weighted'] = results['hp_raw'] * 0\\nresults['r_raw_weighted'] = results['r_raw'] * 0\\n\\nidx_raw = results.columns.str.endswith('_weighted')\\nresults['raw_comp'] = results.iloc[:,idx_raw].sum(axis=1)\\n\\nresults = results.sort_values(by=['bin_comp', 'raw_comp'], ascending=False)\\nresults.to_csv('./output/preds/results_22.csv', index=False)\";\n                var nbb_formatted_code = \"pdList = [df_hpr, df_hp, df_r]  # List of your dataframes\\nresults = pd.concat(pdList, axis=1)\\nresults[\\\"hpr_bin_comp\\\"] = results[\\\"hpr_bin\\\"] * 1\\nresults[\\\"hp_bin_comp\\\"] = results[\\\"hp_bin\\\"] * 0\\nresults[\\\"r_bin_comp\\\"] = results[\\\"r_bin\\\"] * 0\\n\\nidx_comp = results.columns.str.endswith(\\\"_comp\\\")\\nresults[\\\"bin_comp\\\"] = results.iloc[:, idx_comp].sum(axis=1)\\n\\nresults[\\\"hpr_raw_weighted\\\"] = results[\\\"hpr_raw\\\"] * 1\\nresults[\\\"hp_raw_weighted\\\"] = results[\\\"hp_raw\\\"] * 0\\nresults[\\\"r_raw_weighted\\\"] = results[\\\"r_raw\\\"] * 0\\n\\nidx_raw = results.columns.str.endswith(\\\"_weighted\\\")\\nresults[\\\"raw_comp\\\"] = results.iloc[:, idx_raw].sum(axis=1)\\n\\nresults = results.sort_values(by=[\\\"bin_comp\\\", \\\"raw_comp\\\"], ascending=False)\\nresults.to_csv(\\\"./output/preds/results_22.csv\\\", index=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "pdList = [df_hpr, df_hp, df_r]  # List of your dataframes\n",
    "results = pd.concat(pdList, axis=1)\n",
    "results['hpr_bin_comp'] = results['hpr_bin'] * 1\n",
    "results['hp_bin_comp'] = results['hp_bin'] * 0\n",
    "results['r_bin_comp'] = results['r_bin'] * 0\n",
    "\n",
    "idx_comp = results.columns.str.endswith('_comp')\n",
    "results['bin_comp'] = results.iloc[:,idx_comp].sum(axis=1)\n",
    "\n",
    "results['hpr_raw_weighted'] = results['hpr_raw'] * 1\n",
    "results['hp_raw_weighted'] = results['hp_raw'] * 0\n",
    "results['r_raw_weighted'] = results['r_raw'] * 0\n",
    "\n",
    "idx_raw = results.columns.str.endswith('_weighted')\n",
    "results['raw_comp'] = results.iloc[:,idx_raw].sum(axis=1)\n",
    "\n",
    "results = results.sort_values(by=['bin_comp', 'raw_comp'], ascending=False)\n",
    "results.to_csv('./output/preds/results_22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 235;\n                var nbb_unformatted_code = \"dv_ids = dv_data['UNIQUE_ID']\\ndv_ = (dv_ids.to_frame()).join(results)\\ndv_ = dv_.sort_values(by=['bin_comp', 'raw_comp'], ascending=False)\\ndv_['rank'] = dv_.raw_comp.rank(method='dense', ascending=False).astype(int)\\ndv_['Hire'] = dv_['rank'].apply(lambda x: 1 if x <= 1125 else 0)\\ndv_ = dv_.loc[:, ['UNIQUE_ID', 'Hire']]\\ndv_.to_csv('./output/preds/sub_22.csv', index=False)\";\n                var nbb_formatted_code = \"dv_ids = dv_data[\\\"UNIQUE_ID\\\"]\\ndv_ = (dv_ids.to_frame()).join(results)\\ndv_ = dv_.sort_values(by=[\\\"bin_comp\\\", \\\"raw_comp\\\"], ascending=False)\\ndv_[\\\"rank\\\"] = dv_.raw_comp.rank(method=\\\"dense\\\", ascending=False).astype(int)\\ndv_[\\\"Hire\\\"] = dv_[\\\"rank\\\"].apply(lambda x: 1 if x <= 1125 else 0)\\ndv_ = dv_.loc[:, [\\\"UNIQUE_ID\\\", \\\"Hire\\\"]]\\ndv_.to_csv(\\\"./output/preds/sub_22.csv\\\", index=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "dv_ids = dv_data['UNIQUE_ID']\n",
    "dv_ = (dv_ids.to_frame()).join(results)\n",
    "dv_ = dv_.sort_values(by=['bin_comp', 'raw_comp'], ascending=False)\n",
    "dv_['rank'] = dv_.raw_comp.rank(method='dense', ascending=False).astype(int)\n",
    "dv_['Hire'] = dv_['rank'].apply(lambda x: 1 if x <= 1125 else 0)\n",
    "dv_ = dv_.loc[:, ['UNIQUE_ID', 'Hire']]\n",
    "dv_.to_csv('./output/preds/sub_22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 21;\n                var nbb_unformatted_code = \"sub2 = {\\\"Percentage_of_true_top_performers_hired\\\": 0.569, \\\"Percentage_of_true_retained_hired\\\": 0.596, \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.568, \\\"Adverse_impact_ratio\\\": 0.8402, \\\"Final_score\\\": 41.5184}\\nsub3 = {\\\"Percentage_of_true_top_performers_hired\\\": 0.578, \\\"Percentage_of_true_retained_hired\\\": 0.589, \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.571, \\\"Adverse_impact_ratio\\\": 0.8679, \\\"Final_score\\\": 44.5299}\\nsub4 = {\\\"Percentage_of_true_top_performers_hired\\\": 0.585, \\\"Percentage_of_true_retained_hired\\\": 0.597, \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.583, \\\"Adverse_impact_ratio\\\": 0.9545, \\\"Final_score\\\": 54.177}\\nsub5 = {\\\"Percentage_of_true_top_performers_hired\\\": 0.582, \\\"Percentage_of_true_retained_hired\\\": 0.604, \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.591, \\\"Adverse_impact_ratio\\\": 0.936, \\\"Final_score\\\": 52.7986}\\nsub6 = {\\\"Percentage_of_true_top_performers_hired\\\": 0.582, \\\"Percentage_of_true_retained_hired\\\": 0.603, \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.587, \\\"Adverse_impact_ratio\\\": 0.9508, \\\"Final_score\\\": 54.0576}\\nsub7 = {\\\"Percentage_of_true_top_performers_hired\\\": 0.565, \\\"Percentage_of_true_retained_hired\\\": 0.599, \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.568, \\\"Adverse_impact_ratio\\\": 1.0077, \\\"Final_score\\\": 56.7013}\";\n                var nbb_formatted_code = \"sub2 = {\\n    \\\"Percentage_of_true_top_performers_hired\\\": 0.569,\\n    \\\"Percentage_of_true_retained_hired\\\": 0.596,\\n    \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.568,\\n    \\\"Adverse_impact_ratio\\\": 0.8402,\\n    \\\"Final_score\\\": 41.5184,\\n}\\nsub3 = {\\n    \\\"Percentage_of_true_top_performers_hired\\\": 0.578,\\n    \\\"Percentage_of_true_retained_hired\\\": 0.589,\\n    \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.571,\\n    \\\"Adverse_impact_ratio\\\": 0.8679,\\n    \\\"Final_score\\\": 44.5299,\\n}\\nsub4 = {\\n    \\\"Percentage_of_true_top_performers_hired\\\": 0.585,\\n    \\\"Percentage_of_true_retained_hired\\\": 0.597,\\n    \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.583,\\n    \\\"Adverse_impact_ratio\\\": 0.9545,\\n    \\\"Final_score\\\": 54.177,\\n}\\nsub5 = {\\n    \\\"Percentage_of_true_top_performers_hired\\\": 0.582,\\n    \\\"Percentage_of_true_retained_hired\\\": 0.604,\\n    \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.591,\\n    \\\"Adverse_impact_ratio\\\": 0.936,\\n    \\\"Final_score\\\": 52.7986,\\n}\\nsub6 = {\\n    \\\"Percentage_of_true_top_performers_hired\\\": 0.582,\\n    \\\"Percentage_of_true_retained_hired\\\": 0.603,\\n    \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.587,\\n    \\\"Adverse_impact_ratio\\\": 0.9508,\\n    \\\"Final_score\\\": 54.0576,\\n}\\nsub7 = {\\n    \\\"Percentage_of_true_top_performers_hired\\\": 0.565,\\n    \\\"Percentage_of_true_retained_hired\\\": 0.599,\\n    \\\"Percentage_of_true_retained_top_performers_hired\\\": 0.568,\\n    \\\"Adverse_impact_ratio\\\": 1.0077,\\n    \\\"Final_score\\\": 56.7013,\\n}\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "sub2  = {\"Percentage_of_true_top_performers_hired\": 0.569, \"Percentage_of_true_retained_hired\": 0.596, \"Percentage_of_true_retained_top_performers_hired\": 0.568, \"Adverse_impact_ratio\": 0.8402, \"Final_score\": 41.5184}\n",
    "sub3  = {\"Percentage_of_true_top_performers_hired\": 0.578, \"Percentage_of_true_retained_hired\": 0.589, \"Percentage_of_true_retained_top_performers_hired\": 0.571, \"Adverse_impact_ratio\": 0.8679, \"Final_score\": 44.5299}\n",
    "sub4  = {\"Percentage_of_true_top_performers_hired\": 0.585, \"Percentage_of_true_retained_hired\": 0.597, \"Percentage_of_true_retained_top_performers_hired\": 0.583, \"Adverse_impact_ratio\": 0.9545, \"Final_score\": 54.177}\n",
    "sub5  = {\"Percentage_of_true_top_performers_hired\": 0.582, \"Percentage_of_true_retained_hired\": 0.604, \"Percentage_of_true_retained_top_performers_hired\": 0.591, \"Adverse_impact_ratio\": 0.936, \"Final_score\": 52.7986}\n",
    "sub6  = {\"Percentage_of_true_top_performers_hired\": 0.582, \"Percentage_of_true_retained_hired\": 0.603, \"Percentage_of_true_retained_top_performers_hired\": 0.587, \"Adverse_impact_ratio\": 0.9508, \"Final_score\": 54.0576}\n",
    "sub7  = {\"Percentage_of_true_top_performers_hired\": 0.565, \"Percentage_of_true_retained_hired\": 0.599, \"Percentage_of_true_retained_top_performers_hired\": 0.568, \"Adverse_impact_ratio\": 1.0077, \"Final_score\": 56.7013}\n",
    "sub8  = {\"Percentage_of_true_top_performers_hired\": 0.566, \"Percentage_of_true_retained_hired\": 0.600, \"Percentage_of_true_retained_top_performers_hired\": 0.568, \"Adverse_impact_ratio\": 1.0077, \"Final_score\": 56.7516}\n",
    "subx  = {\"Percentage_of_true_top_performers_hired\": 0.566, \"Percentage_of_true_retained_hired\": 0.597, \"Percentage_of_true_retained_top_performers_hired\": 0.566, \"Adverse_impact_ratio\": 1.0039, \"Final_score\": 56.9841}\n",
    "sub9  = {\"Percentage_of_true_top_performers_hired\": 0.556, \"Percentage_of_true_retained_hired\": 0.600, \"Percentage_of_true_retained_top_performers_hired\": 0.561, \"Adverse_impact_ratio\": 0.9885, \"Final_score\": 55.7692}\n",
    "sub10 = {\"Percentage_of_true_top_performers_hired\": 0.557, \"Percentage_of_true_retained_hired\": 0.602, \"Percentage_of_true_retained_top_performers_hired\": 0.566, \"Adverse_impact_ratio\": 0.9962, \"Final_score\": 56.8718}\n",
    "sub11 = {\"Percentage_of_true_top_performers_hired\": 0.565, \"Percentage_of_true_retained_hired\": 0.587, \"Percentage_of_true_retained_top_performers_hired\": 0.566, \"Adverse_impact_ratio\": 0.9962, \"Final_score\": 56.6904}\n",
    "sub12 = {\"Percentage_of_true_top_performers_hired\": 0.575, \"Percentage_of_true_retained_hired\": 0.588, \"Percentage_of_true_retained_top_performers_hired\": 0.58, \"Adverse_impact_ratio\": 0.9809, \"Final_score\": 56.1616}\n",
    "sub13 = {\"Percentage_of_true_top_performers_hired\": 0.557, \"Percentage_of_true_retained_hired\": 0.601, \"Percentage_of_true_retained_top_performers_hired\": 0.562, \"Adverse_impact_ratio\": 0.9885, \"Final_score\": 55.9073}\n",
    "sub14 = {\"Percentage_of_true_top_performers_hired\": 0.575, \"Percentage_of_true_retained_hired\": 0.588, \"Percentage_of_true_retained_top_performers_hired\": 0.58, \"Adverse_impact_ratio\": 0.9809, \"Final_score\": 56.1616}\n",
    "sub15 = {\"Percentage_of_true_top_performers_hired\": 0.579, \"Percentage_of_true_retained_hired\": 0.575, \"Percentage_of_true_retained_top_performers_hired\": 0.576, \"Adverse_impact_ratio\": 1.0431, \"Final_score\": 53.3733}\n",
    "sub16 = {\"Percentage_of_true_top_performers_hired\": 0.543, \"Percentage_of_true_retained_hired\": 0.605, \"Percentage_of_true_retained_top_performers_hired\": 0.555, \"Adverse_impact_ratio\": 0.9434, \"Final_score\": 50.8189}\n",
    "sub17 = {\"Percentage_of_true_top_performers_hired\": 0.574, \"Percentage_of_true_retained_hired\": 0.584, \"Percentage_of_true_retained_top_performers_hired\": 0.562, \"Adverse_impact_ratio\": 0.9069, \"Final_score\": 47.7464}\n",
    "sub20 = {\"Percentage_of_true_top_performers_hired\": 0.563, \"Percentage_of_true_retained_hired\": 0.603, \"Percentage_of_true_retained_top_performers_hired\": 0.573, \"Adverse_impact_ratio\": 0.9885, \"Final_score\": 56.6474}\n",
    "sub21 = {\"Percentage_of_true_top_performers_hired\": 0.568, \"Percentage_of_true_retained_hired\": 0.596, \"Percentage_of_true_retained_top_performers_hired\": 0.564, \"Adverse_impact_ratio\": 1.0077, \"Final_score\": 56.5431}\n",
    "\n",
    "subx  = {\"Percentage_of_true_top_performers_hired\": 0.566, \"Percentage_of_true_retained_hired\": 0.597, \"Percentage_of_true_retained_top_performers_hired\": 0.566, \"Adverse_impact_ratio\": 1.0039, \"Final_score\": 56.9841}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9232679844458846"
      ]
     },
     "metadata": {},
     "execution_count": 98
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 98;\n                var nbb_unformatted_code = \"56.9841 / 61.72\";\n                var nbb_formatted_code = \"56.9841 / 61.72\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "56.9841 / 61.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE\n",
    "\n",
    "#class FairClassifier(object):\n",
    "#    \n",
    "#    def __init__(self, n_features, n_sensitive, lambdas):\n",
    "#        self.lambdas = lambdas\n",
    "#        \n",
    "#        clf_inputs = Input(shape=(n_features,))\n",
    "#        adv_inputs = Input(shape=(1,))\n",
    "#        \n",
    "#        clf_net = self._create_clf_net(clf_inputs)\n",
    "#        adv_net = self._create_adv_net(adv_inputs, n_sensitive)\n",
    "#        self._trainable_clf_net = self._make_trainable(clf_net)\n",
    "#        self._trainable_adv_net = self._make_trainable(adv_net)\n",
    "#        self._clf = self._compile_clf(clf_net)\n",
    "#        self._clf_w_adv = self._compile_clf_w_adv(clf_inputs, clf_net, adv_net)\n",
    "#        self._adv = self._compile_adv(clf_inputs, clf_net, adv_net, n_sensitive)\n",
    "#        self._val_metrics = None\n",
    "#        self._fairness_metrics = None\n",
    "#        \n",
    "#        self.predict = self._clf.predict\n",
    "#        \n",
    "#    def _make_trainable(self, net):\n",
    "#        def make_trainable(flag):\n",
    "#            net.trainable = flag\n",
    "#            for layer in net.layers:\n",
    "#                layer.trainable = flag\n",
    "#        return make_trainable\n",
    "#        \n",
    "#    def _create_clf_net(self, inputs):\n",
    "#        dense1 = Dense(32, activation='relu')(inputs)\n",
    "#        dropout1 = Dropout(0.2)(dense1)\n",
    "#        dense2 = Dense(32, activation='relu')(dropout1)\n",
    "#        dropout2 = Dropout(0.2)(dense2)\n",
    "#        dense3 = Dense(32, activation='relu')(dropout2)\n",
    "#        dropout3 = Dropout(0.2)(dense3)\n",
    "#        outputs = Dense(1, activation='sigmoid', name='y')(dropout3)\n",
    "#        return Model(inputs=[inputs], outputs=[outputs])\n",
    "#        \n",
    "#    def _create_adv_net(self, inputs, n_sensitive):\n",
    "#        dense1 = Dense(32, activation='relu')(inputs)\n",
    "#        dense2 = Dense(32, activation='relu')(dense1)\n",
    "#        dense3 = Dense(32, activation='relu')(dense2)\n",
    "#        outputs = [Dense(1, activation='sigmoid')(dense3) for _ in range(n_sensitive)]\n",
    "#        return Model(inputs=[inputs], outputs=outputs)\n",
    "#\n",
    "#    def _compile_clf(self, clf_net):\n",
    "#        clf = clf_net\n",
    "#        self._trainable_clf_net(True)\n",
    "#        clf.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "#        return clf\n",
    "#        \n",
    "#    def _compile_clf_w_adv(self, inputs, clf_net, adv_net):\n",
    "#        clf_w_adv = Model(inputs=[inputs], outputs=[clf_net(inputs)]+adv_net(clf_net(inputs)))\n",
    "#        self._trainable_clf_net(True)\n",
    "#        self._trainable_adv_net(False)\n",
    "#        loss_weights = [1.]+[-lambda_param for lambda_param in self.lambdas]\n",
    "#        clf_w_adv.compile(loss=['binary_crossentropy']*(len(loss_weights)), \n",
    "#                          loss_weights=loss_weights,\n",
    "#                          optimizer='adam')\n",
    "#        return clf_w_adv\n",
    "#\n",
    "#    def _compile_adv(self, inputs, clf_net, adv_net, n_sensitive):\n",
    "#        adv = Model(inputs=[inputs], outputs=adv_net(clf_net(inputs)))\n",
    "#        self._trainable_clf_net(False)\n",
    "#        self._trainable_adv_net(True)\n",
    "#        adv.compile(loss=['binary_crossentropy']*n_sensitive, optimizer='adam')\n",
    "#        return adv\n",
    "#\n",
    "#    def _compute_class_weights(self, data_set):\n",
    "#        class_values = [0, 1]\n",
    "#        class_weights = []\n",
    "#        if len(data_set.shape) == 1:\n",
    "#            balanced_weights = compute_class_weight('balanced', class_values, data_set)\n",
    "#            class_weights.append(dict(zip(class_values, balanced_weights)))\n",
    "#        else:\n",
    "#            n_attr =  data_set.shape[1]\n",
    "#            for attr_idx in range(n_attr):\n",
    "#                balanced_weights = compute_class_weight('balanced', class_values,\n",
    "#                                                        np.array(data_set)[:,attr_idx])\n",
    "#                class_weights.append(dict(zip(class_values, balanced_weights)))\n",
    "#        print(f'-compute-target-class_weights > {class_weights}')\n",
    "#        return class_weights\n",
    "#    \n",
    "#    def _compute_target_class_weights(self, y):\n",
    "#        class_values  = [0,1]\n",
    "#        balanced_weights =  compute_class_weight('balanced', class_values, y)\n",
    "#        class_weights = {'y': dict(zip(class_values, balanced_weights))}\n",
    "#        print(f'-compute-class-weights > {class_weights}')\n",
    "#        return class_weights\n",
    "#        \n",
    "#    def pretrain(self, x, y, z, epochs=10, verbose=0):\n",
    "#        self._trainable_clf_net(True)\n",
    "#        self._clf.fit(x.values, y.values, epochs=epochs, verbose=verbose)\n",
    "#        self._trainable_clf_net(False)\n",
    "#        self._trainable_adv_net(True)\n",
    "#        class_weight_adv = self._compute_class_weights(z)\n",
    "#        print(f'pretrain-class-weight-adv > {class_weight_adv}')\n",
    "#        self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), class_weight=class_weight_adv, \n",
    "#                      epochs=epochs, verbose=verbose)\n",
    "#        \n",
    "#    def fit(self, x, y, z, validation_data=None, T_iter=250, batch_size=128,\n",
    "#            save_figs=False):\n",
    "#        n_sensitive = z.shape[1]\n",
    "#        if validation_data is not None:\n",
    "#            x_val, y_val, z_val = validation_data\n",
    "#\n",
    "#        class_weight_adv = self._compute_class_weights(z)\n",
    "#        class_weight_clf_w_adv = [{0:1., 1:1.}]+class_weight_adv\n",
    "#        self._val_metrics = pd.DataFrame()\n",
    "#        self._fairness_metrics = pd.DataFrame()  \n",
    "#        for idx in range(T_iter):\n",
    "#            if validation_data is not None:\n",
    "#                self._clf.save(f'output/models/{idx:03d}.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#                y_pred = pd.Series(self._clf.predict(x_val).ravel(), index=y_val.index)\n",
    "#                lr_probs = self._clf.predict(x_val)\n",
    "#                #y_pred_argmax = lr_probs.argmax(axis=-1)\n",
    "#                y_pred_round = np.round(y_pred)\n",
    "#                #print(f'y_pred is : {y_pred}')\n",
    "#                #print(f'lr_probs is : {lr_probs}')\n",
    "#                #print(f'y_pred_argmax is : {y_pred_argmax}')\n",
    "#                #print(f'y_pred_round is : {y_pred_round}')\n",
    "#                #round_y_pred = np.round(y_pred)\n",
    "#                self._val_metrics.loc[idx, 'roc_auc_d'] = roc_auc_score(y_val, y_pred)\n",
    "#                self._val_metrics.loc[idx, 'accuracy_d'] = (accuracy_score(y_val, (y_pred>0.5))*100)\n",
    "#                self._val_metrics.loc[idx, 'recall_d'] = recall_score(y_val, y_pred_round)\n",
    "#\n",
    "#                precision_prc, recall_prc, thresholds = precision_recall_curve(y_val, y_pred)\n",
    "#                precision_prc = np.nan_to_num(precision_prc)\n",
    "#                recall_prc = np.nan_to_num(recall_prc)\n",
    "#\n",
    "#                f1 = f1_score(y_val, y_pred_round) \n",
    "#                k_auc = auc(recall_prc, precision_prc)\n",
    "#  \n",
    "#                fscore_prc = (2 * precision_prc * recall_prc) / (precision_prc + recall_prc)\n",
    "#                fscore_prc = np.nan_to_num(fscore_prc)\n",
    "#\n",
    "#                #print(fscore_prc)\n",
    "#                #print(recall_prc)\n",
    "##\n",
    "#                #print(type(fscore_prc))\n",
    "#                #print(type(recall_prc))\n",
    "##\n",
    "#                #print(len(fscore_prc))\n",
    "#                #print(len(recall_prc))\n",
    "#\n",
    "#                # locate the index of the largest f score\n",
    "#                f1_ix = np.argmax(fscore_prc)\n",
    "#                bt = thresholds[f1_ix]\n",
    "#                fscore_prc_best = fscore_prc[f1_ix]\n",
    "#\n",
    "#                #recall_ix = np.argmax(recall_prc)\n",
    "#                #print(recall_ix)\n",
    "#                #bt_r = thresholds[recall_ix]\n",
    "#                #print(bt_r)\n",
    "#                #recall_prc_best = recall_prc[bt_r]\n",
    "#\n",
    "#                recall_prc_best = recall_prc[f1_ix]\n",
    "#\n",
    "#                self._val_metrics.loc[idx, 'auc'] = k_auc\n",
    "#                self._val_metrics.loc[idx, 'original_f1'] = f1\n",
    "#                self._val_metrics.loc[idx, 'bt_f1'] = thresholds[f1_ix]\n",
    "#                self._val_metrics.loc[idx, 'best_f1'] = fscore_prc_best\n",
    "#                #self._val_metrics.loc[idx, 'bt_r'] = thresholds[recall_ix]\n",
    "#                #self._val_metrics.loc[idx, 'best_r'] = recall_prc_best\n",
    "#                self._val_metrics.loc[idx, 'bt_recall'] = recall_prc[f1_ix]\n",
    "#                self._val_metrics.loc[idx, 'bt_precision'] = precision_prc[f1_ix]\n",
    "#\n",
    "#                p_rule_original = (p_rule(y_pred, z_val['race'], threshold=.50)) / 100\n",
    "#                p_rule_bt = (p_rule(y_pred, z_val['race'], threshold=bt)) / 100\n",
    "#                jackpot_original = ((p_rule(y_pred, z_val['race'], threshold=.50)) / 100) * f1\n",
    "#                jackpot_bt = ((p_rule(y_pred, z_val['race'], threshold=bt)) / 100) * fscore_prc_best\n",
    "#\n",
    "#                print(f'Model= {idx}')\n",
    "#                print('AUC=%.3f, Original F-Score=%.3f, P-rule original=%.3f, JP or=%.4f' % (k_auc, f1, p_rule_original, jackpot_original))\n",
    "#                print('Best Threshold=%f, Best F-Score=%.3f, P-rule revised=%.3f, JP bt=%.4f' % (bt, fscore_prc_best, p_rule_bt, jackpot_bt))\n",
    "#\n",
    "#                pyplot.plot(recall_prc, precision_prc, marker='.', label='NN')\n",
    "#                pyplot.scatter(recall_prc[f1_ix], precision_prc[f1_ix], marker='o', color='black', label='Best f1')\n",
    "#                #pyplot.scatter(recall_prc[recall_ix], precision_prc[recall_ix], marker='o', color='orange', label='Best Recall')\n",
    "#                # axis labels\n",
    "#                pyplot.xlabel('Recall')\n",
    "#                pyplot.ylabel('Precision')\n",
    "#                pyplot.legend()\n",
    "#                # show the plot\n",
    "#                pyplot.show()\n",
    "#\n",
    "#                self._val_metrics.loc[idx, 'p_rule_original'] = (p_rule(y_pred, z_val['race'], threshold=.50)) / 100\n",
    "#                self._val_metrics.loc[idx, 'p_rule_bt_f1'] = (p_rule(y_pred, z_val['race'], threshold=bt)) / 100\n",
    "#\n",
    "#                self._val_metrics.loc[idx, 'jackpot_original'] = ((p_rule(y_pred, z_val['race'], threshold=.50)) / 100) * f1\n",
    "#                self._val_metrics.loc[idx, 'jackpot_bt_ft'] = ((p_rule(y_pred, z_val['race'], threshold=bt)) / 100) * fscore_prc_best\n",
    "#                self._val_metrics.loc[idx, 'jackpot_bt_recall'] = ((p_rule(y_pred, z_val['race'], threshold=bt)) / 100) * recall_prc_best\n",
    "#                \n",
    "#                #assert recall == recall_score(y_val, y_pred_round), 'recall not the same for both approaches'\n",
    "#                #self._val_metrics.loc[idx, 'Sensitivity'] = get_sensitivity(y_val, round_y_pred)\n",
    "#\n",
    "#                #for sensitive_attr in z_val.columns:\n",
    "#                #    self._fairness_metrics.loc[idx, sensitive_attr] = p_rule(y_pred,\n",
    "#                #                                                             z_val[sensitive_attr])\n",
    "#                #display.clear_output(wait=True)\n",
    "#                #plot_distributions(y_pred, z_val, idx+1, self._val_metrics.loc[idx],\n",
    "#                #                   self._fairness_metrics.loc[idx], \n",
    "#                #                   fname=f'output/{idx+1:08d}.png' if save_figs else None)\n",
    "#                #plt.show(plt.gcf())\n",
    "#            \n",
    "#                df = pd.DataFrame(y_pred, columns=['preds'])\n",
    "#                df['preds_bin'] = df['preds'].apply(lambda x: 1 if x >= bt else 0)\n",
    "#                df['preds_bin'].mean()\n",
    "#                self._val_metrics.loc[idx, 'bt_preds_mean'] = df['preds_bin'].mean()\n",
    "#\n",
    "#                self._val_metrics.loc[idx, 'jackpot_bt_recall_mean'] = jackpot_bt * (df['preds_bin'].mean())\n",
    "#\n",
    "#            self._val_metrics = self._val_metrics.sort_values(by=['jackpot_bt_recall_mean', 'jackpot_bt_recall'], ascending=False)\n",
    "#            self._val_metrics.to_csv('output/_val_metrics.csv')\n",
    "#\n",
    "#            # train adverserial\n",
    "#            self._trainable_clf_net(False)\n",
    "#            self._trainable_adv_net(True)\n",
    "#            self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), batch_size=batch_size, \n",
    "#                          class_weight=class_weight_adv, epochs=1, verbose=0)\n",
    "#            \n",
    "#            # train classifier\n",
    "#            self._trainable_clf_net(True)\n",
    "#            self._trainable_adv_net(False)\n",
    "#            indices = np.random.permutation(len(x))[:batch_size]\n",
    "#            self._clf_w_adv.train_on_batch(x.values[indices], \n",
    "#                                           [y.values[indices]]+np.hsplit(z.values[indices], n_sensitive),\n",
    "#                                           class_weight=class_weight_clf_w_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 141;\n                var nbb_unformatted_code = \"model = load_model('output/models/final/model_hp.h5')\\ny_pred = pd.Series(model.predict(X_train).ravel(), index=X_train.index)\\n\\nthresholds = np.arange(0.0, 1.0, 0.001)\\nrscore = np.zeros(shape=(len(thresholds)))\\npscore = np.zeros(shape=(len(thresholds)))\\nsrscore = np.zeros(shape=(len(thresholds)))\\n# Fit the model\\nfor index, elem in enumerate(thresholds):\\n    # Corrected probabilities\\n    y_pred_prob = (y_pred > elem).astype('int')\\n    # Calculate the f-score\\n    rscore[index] = recall_score(y_train, y_pred_prob)\\n    pscore[index] = (p_rule(y_pred_prob, Z_train['race'], threshold=elem)) / 100\\n    srscore[index] = 1 - abs(sr_rule(y_pred_prob, threshold=elem)-.50)\";\n                var nbb_formatted_code = \"model = load_model(\\\"output/models/final/model_hp.h5\\\")\\ny_pred = pd.Series(model.predict(X_train).ravel(), index=X_train.index)\\n\\nthresholds = np.arange(0.0, 1.0, 0.001)\\nrscore = np.zeros(shape=(len(thresholds)))\\npscore = np.zeros(shape=(len(thresholds)))\\nsrscore = np.zeros(shape=(len(thresholds)))\\n# Fit the model\\nfor index, elem in enumerate(thresholds):\\n    # Corrected probabilities\\n    y_pred_prob = (y_pred > elem).astype(\\\"int\\\")\\n    # Calculate the f-score\\n    rscore[index] = recall_score(y_train, y_pred_prob)\\n    pscore[index] = (p_rule(y_pred_prob, Z_train[\\\"race\\\"], threshold=elem)) / 100\\n    srscore[index] = 1 - abs(sr_rule(y_pred_prob, threshold=elem) - 0.50)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "model = load_model('output/models/final/model_hp.h5')\n",
    "y_pred = pd.Series(model.predict(X_train).ravel(), index=X_train.index)\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.001)\n",
    "rscore = np.zeros(shape=(len(thresholds)))\n",
    "pscore = np.zeros(shape=(len(thresholds)))\n",
    "srscore = np.zeros(shape=(len(thresholds)))\n",
    "# Fit the model\n",
    "for index, elem in enumerate(thresholds):\n",
    "    # Corrected probabilities\n",
    "    y_pred_prob = (y_pred > elem).astype('int')\n",
    "    # Calculate the f-score\n",
    "    rscore[index] = recall_score(y_train, y_pred_prob)\n",
    "    pscore[index] = (p_rule(y_pred_prob, Z_train['race'], threshold=elem)) / 100\n",
    "    srscore[index] = 1 - abs(sr_rule(y_pred_prob, threshold=elem)-.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Threshold: 0.077 with Metric: 0.8863\nBest Threshold: 0.077 with Recall: 0.9275\nBest Threshold: 0.077 with Pvalue: 0.956\nBest Threshold: 0.077 with SR: 0.9996\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 142;\n                var nbb_unformatted_code = \"rscore = np.nan_to_num(rscore)\\npscore = np.nan_to_num(pscore)\\nsrscore = np.nan_to_num(srscore)\\n\\nsuper_threshold_indices = srscore < .90\\nsrscore[super_threshold_indices] = 0\\n\\nmetric = rscore * pscore * srscore\\nindex = np.argmax(metric)\\nthresholdOpt = round(thresholds[index], ndigits = 4)\\nmetricOpt = round(metric[index], ndigits = 4)\\nrscoreOpt = round(rscore[index], ndigits = 4)\\npscoreOpt = round(pscore[index], ndigits = 4)\\nsrscoreOpt = round(srscore[index], ndigits = 4)\\nprint('Best Threshold: {} with Metric: {}'.format(thresholdOpt, metricOpt))\\nprint('Best Threshold: {} with Recall: {}'.format(thresholdOpt, rscoreOpt))\\nprint('Best Threshold: {} with Pvalue: {}'.format(thresholdOpt, pscoreOpt))\\nprint('Best Threshold: {} with SR: {}'.format(thresholdOpt, srscoreOpt))\";\n                var nbb_formatted_code = \"rscore = np.nan_to_num(rscore)\\npscore = np.nan_to_num(pscore)\\nsrscore = np.nan_to_num(srscore)\\n\\nsuper_threshold_indices = srscore < 0.90\\nsrscore[super_threshold_indices] = 0\\n\\nmetric = rscore * pscore * srscore\\nindex = np.argmax(metric)\\nthresholdOpt = round(thresholds[index], ndigits=4)\\nmetricOpt = round(metric[index], ndigits=4)\\nrscoreOpt = round(rscore[index], ndigits=4)\\npscoreOpt = round(pscore[index], ndigits=4)\\nsrscoreOpt = round(srscore[index], ndigits=4)\\nprint(\\\"Best Threshold: {} with Metric: {}\\\".format(thresholdOpt, metricOpt))\\nprint(\\\"Best Threshold: {} with Recall: {}\\\".format(thresholdOpt, rscoreOpt))\\nprint(\\\"Best Threshold: {} with Pvalue: {}\\\".format(thresholdOpt, pscoreOpt))\\nprint(\\\"Best Threshold: {} with SR: {}\\\".format(thresholdOpt, srscoreOpt))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "rscore = np.nan_to_num(rscore)\n",
    "pscore = np.nan_to_num(pscore)\n",
    "srscore = np.nan_to_num(srscore)\n",
    "\n",
    "super_threshold_indices = srscore < .90\n",
    "srscore[super_threshold_indices] = 0\n",
    "\n",
    "metric = rscore * pscore * srscore\n",
    "index = np.argmax(metric)\n",
    "thresholdOpt = round(thresholds[index], ndigits = 4)\n",
    "metricOpt = round(metric[index], ndigits = 4)\n",
    "rscoreOpt = round(rscore[index], ndigits = 4)\n",
    "pscoreOpt = round(pscore[index], ndigits = 4)\n",
    "srscoreOpt = round(srscore[index], ndigits = 4)\n",
    "print('Best Threshold: {} with Metric: {}'.format(thresholdOpt, metricOpt))\n",
    "print('Best Threshold: {} with Recall: {}'.format(thresholdOpt, rscoreOpt))\n",
    "print('Best Threshold: {} with Pvalue: {}'.format(thresholdOpt, pscoreOpt))\n",
    "print('Best Threshold: {} with SR: {}'.format(thresholdOpt, srscoreOpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}